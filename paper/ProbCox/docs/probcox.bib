
@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difﬁcult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to ﬁrst posit a family of densities and then to ﬁnd the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-ﬁeld variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	language = {en},
	number = {518},
	urldate = {2020-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, theory},
	pages = {859--877},
}

@article{mittal_high-dimensional_2014,
	title = {High-dimensional, massive sample-size {Cox} proportional hazards regression for survival analysis},
	volume = {15},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxt043},
	doi = {10.1093/biostatistics/kxt043},
	abstract = {Survival analysis endures as an old, yet active research field with applications that spread across many domains. Continuing improvements in data acquisition techniques pose constant challenges in applying existing survival analysis methods to these emerging data sets. In this paper, we present tools for fitting regularized Cox survival analysis models on high-dimensional, massive sample-size (HDMSS) data using a variant of the cyclic coordinate descent optimization technique tailored for the sparsity that HDMSS data often present. Experiments on two real data examples demonstrate that efficient analyses of HDMSS data using these tools result in improved predictive performance and calibration.},
	language = {en},
	number = {2},
	urldate = {2020-11-20},
	journal = {Biostatistics},
	author = {Mittal, S. and Madigan, D. and Burd, R. S. and Suchard, M. A.},
	month = apr,
	year = {2014},
	keywords = {theory},
	pages = {207--221},
}

@article{tarkhan_bigsurvsgd_2020,
	title = {{BigSurvSGD}: {Big} {Survival} {Data} {Analysis} via {Stochastic} {Gradient} {Descent}},
	shorttitle = {{BigSurvSGD}},
	url = {http://arxiv.org/abs/2003.00116},
	abstract = {In many biomedical applications, outcome is measured as a “time-to-event” (eg. disease progression or death). To assess the connection between features of a patient and this outcome, it is common to assume a proportional hazards model, and ﬁt a proportional hazards regression (or Cox regression). To ﬁt this model, a log-concave objective function known as the “partial likelihood” is maximized. For moderate-sized datasets, an eﬃcient Newton-Raphson algorithm that leverages the structure of the objective can be employed. However, in large datasets this approach has two issues: 1) The computational tricks that leverage structure can also lead to computational instability; 2) The objective does not naturally decouple: Thus, if the dataset does not ﬁt in memory, the model can be very computationally expensive to ﬁt. This additionally means that the objective is not directly amenable to stochastic gradient-based optimization methods. To overcome these issues, we propose a simple, new framing of proportional hazards regression: This results in an objective function that is amenable to stochastic gradient descent. We show that this simple modiﬁcation allows us to eﬃciently ﬁt survival models with very large datasets. This also facilitates training complex, eg. neural-network-based, models with survival data.},
	language = {en},
	urldate = {2020-11-20},
	journal = {arXiv:2003.00116 [math, stat]},
	author = {Tarkhan, Aliasghar and Simon, Noah},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.00116},
	keywords = {Mathematics - Statistics Theory, theory},
}

@article{ngwa_generating_2019,
	title = {Generating survival times with time-varying covariates using the {Lambert} {W} {Function}},
	volume = {0},
	issn = {0361-0918},
	url = {https://doi.org/10.1080/03610918.2019.1648822},
	doi = {10.1080/03610918.2019.1648822},
	abstract = {Simulation studies provide an important statistical tool in evaluating survival methods, requiring an appropriate data-generating process to simulate data for an underlying statistical model. Many studies with time-to-event outcomes use the Cox proportional hazard model. While methods for simulating such data with time-invariant predictors have been described, methods for simulating data with time-varying covariates are sorely needed. Here, we describe an approach for generating data for the Cox proportional hazard model with time-varying covariates when event times follow an Exponential or Weibull distribution. For each distribution, we derive a closed-form expression to generate survival times and link the time-varying covariates with the hazard function. We consider a continuous time-varying covariate measured at regular intervals over time, as well as time-invariant covariates, in generating time-to-event data under a number of scenarios. Our results suggest this method can lead to simulation studies with reliable and robust estimation of the association parameter in Cox-Weibull and Cox-Exponential models.},
	number = {0},
	urldate = {2021-03-12},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Ngwa, Julius S. and Cabral, Howard J. and Cheng, Debbie M. and Gagnon, David R. and LaValley, Michael P. and Cupples, L. Adrienne},
	month = aug,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610918.2019.1648822},
	keywords = {Lambert W Function, Linear Mixed effects model, Longitudinal and survival data, simulation, Time-varying covariates, Two step approach},
	pages = {1--19},
}

@article{shih_variate_1993,
	title = {Variate generation for a nonhomogeneous poisson process with time dependent covariates},
	volume = {44},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659308811457},
	doi = {10.1080/00949659308811457},
	abstract = {Algorithms are developed for generating a sequence of event times from a nonhomogeneous Poisson process that is influenced by the values of covariates that vary with time. Closed form expressions for random variate generation are shown for several baseline intensity and link functions. Two specific models linking the baseline process to the general model are considered: the accelerated time model and the proportional intensity model. In the accelerated time model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where z is a covariate vector, ⋀0(t) is the baseline cumulative intensity function and ψ(z) is the link function. In the proportional intensity model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where λ0(t) is the baseline intensity function.},
	number = {3-4},
	urldate = {2021-03-12},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shih, Li-Hsing and Leemis, Lawrence M.},
	month = jan,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659308811457},
	keywords = {simulation, Covariates, Nonhomogeneous Poisson processes, Simulation, Variate generation},
	pages = {165--186},
}

@article{leemis_variate_1990,
	title = {Variate generation for accelerated life and proportional hazards models with time dependent covariates},
	volume = {10},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/0167715290900529},
	doi = {10.1016/0167-7152(90)90052-9},
	abstract = {Variate generation algorithms for lifetimes when survival models incorporate time dependent covariates are presented. These algorithms are closed form for special cases of the function that links the covariate values to the survivor distribution. These algorithms are illustrated by several examples.},
	language = {en},
	number = {4},
	urldate = {2021-03-12},
	journal = {Statistics \& Probability Letters},
	author = {Leemis, Lawrence M. and Shih, Li-Hsing and Reynertson, Kurt},
	month = sep,
	year = {1990},
	keywords = {simulation, Accelerated life model, Monte Carlo simulation, proportional hazards model, time dependent covariates, variate generation},
	pages = {335--339},
}

@article{zhou_understanding_2001,
	title = {Understanding the {Cox} {Regression} {Models} with {Time}-{Change} {Covariates}},
	volume = {55},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2686004},
	abstract = {The Cox regression model is a cornerstone of modern survival analysis and is widely used in many other fields as well. But the Cox models with time-change covariates are not easy to understand or visualize. We therefore offer a simple and easy-to-understand interpretation of the (arbitrary) baseline hazard and time-change covariate. This interpretation also provides a way to simulate variables that follow a Cox model with arbitrary baseline hazard and time-change covariate.},
	number = {2},
	urldate = {2021-03-12},
	journal = {The American Statistician},
	author = {Zhou, Mai},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {simulation},
	pages = {153--155},
}

@article{bender_generating_2005,
	title = {Generating survival times to simulate {Cox} proportional hazards models},
	volume = {24},
	issn = {0277-6715},
	doi = {10.1002/sim.2059},
	abstract = {Simulation studies present an important statistical tool to investigate the performance, properties and adequacy of statistical models in pre-specified situations. One of the most important statistical models in medical research is the proportional hazards model of Cox. In this paper, techniques to generate survival times for simulation studies regarding Cox proportional hazards models are presented. A general formula describing the relation between the hazard and the corresponding survival time of the Cox model is derived, which is useful in simulation studies. It is shown how the exponential, the Weibull and the Gompertz distribution can be applied to generate appropriate survival times for simulation studies. Additionally, the general relation between hazard and survival time can be used to develop own distributions for special situations and to handle flexibly parameterized proportional hazards models. The use of distributions other than the exponential distribution is indispensable to investigate the characteristics of the Cox proportional hazards model, especially in non-standard situations, where the partial likelihood depends on the baseline hazard. A simulation study investigating the effect of measurement errors in the German Uranium Miners Cohort Study is considered to illustrate the proposed simulation techniques and to emphasize the importance of a careful modelling of the baseline hazard in Cox models.},
	language = {eng},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
	month = jun,
	year = {2005},
	pmid = {15724232},
	keywords = {simulation, Cohort Studies, Computer Simulation, Germany, East, Humans, Mining, Neoplasms, Proportional Hazards Models, Radiation, Radon, Survival Analysis},
	pages = {1713--1723},
}

@article{kvamme_time--event_2019,
	title = {Time-to-{Event} {Prediction} with {Neural} {Networks} and {Cox} {Regression}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-424.html},
	number = {129},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kvamme, Håvard and Borgan, Ørnulf and Scheel, Ida},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, theory},
	pages = {1--30},
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	number = {4},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	keywords = {theory},
	pages = {1303--1347},
}

@article{millett_sex_2018,
	title = {Sex differences in risk factors for myocardial infarction: cohort study of {UK} {Biobank} participants},
	issn = {0959-8138, 1756-1833},
	shorttitle = {Sex differences in risk factors for myocardial infarction},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.k4247},
	doi = {10.1136/bmj.k4247},
	abstract = {Objectives To investigate sex differences in risk factors for incident myocardial infarction (MI) and whether they vary with age. Design Prospective population based study. Setting UK Biobank. Participants 471 998 participants (56\% women; mean age 56.2) with no history of cardiovascular disease. Main outcome measure Incident (fatal and non-fatal) MI. Results 5081 participants (1463 (28.8\%) of whom were women) had MI over seven years’ mean follow-up, resulting in an incidence per 10 000 person years of 7.76 (95\% confidence interval 7.37 to 8.16) for women and 24.35 (23.57 to 25.16) for men. Higher blood pressure indices, smoking intensity, body mass index, and the presence of diabetes were associated with an increased risk of MI in men and women, but associations were attenuated with age. In women, systolic blood pressure and hypertension, smoking status and intensity, and diabetes were associated with higher hazard ratios for MI compared with men: ratio of hazard ratios 1.09 (95\% confidence interval 1.02 to 1.16) for systolic blood pressure, 1.55 (1.32 to 1.83) for current smoking, 2.91 (1.56 to 5.45) for type 1 diabetes, and 1.47 (1.16 to 1.87) for type 2 diabetes. There was no evidence that any of these ratios of hazard ratios decreased with age (P{\textgreater}0.2). With the exception of type 1 diabetes, the incidence of MI was higher in men than in women for all risk factors.},
	language = {en},
	urldate = {2021-03-10},
	journal = {BMJ},
	author = {Millett, Elizabeth R C and Peters, Sanne A E and Woodward, Mark},
	month = nov,
	year = {2018},
	keywords = {application},
	pages = {k4247},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2985181},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	keywords = {theory},
	pages = {187--220},
}

@article{cox_partial_1975,
	title = {Partial {Likelihood}},
	volume = {62},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335362},
	doi = {10.2307/2335362},
	abstract = {A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and inference in stochastic processes. It is shown that the usual large-sample properties of maximum likelihood estimates and tests apply when partial likelihood is used.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Cox, D. R.},
	year = {1975},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {269--276},
}

@article{hjort_nonparametric_1990,
	title = {Nonparametric {Bayes} {Estimators} {Based} on {Beta} {Processes} in {Models} for {Life} {History} {Data}},
	volume = {18},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242052},
	abstract = {Several authors have constructed nonparametric Bayes estimators for a cumulative distribution function based on (possibly right-censored) data. The prior distributions have, for example, been Dirichlet processes or, more generally, processes neutral to the right. The present article studies the related problem of finding Bayes estimators for cumulative hazard rates and related quantities, w.r.t. prior distributions that correspond to cumulative hazard rate processes with nonnegative independent increments. A particular class of prior processes, termed beta processes, is introduced and is shown to constitute a conjugate class. To arrive at these, a nonparametric time-discrete framework for survival data, which has some independent interest, is studied first. An important bonus of the approach based on cumulative hazards is that more complicated models for life history data than the simple life table situation can be treated, for example, time-inhomogeneous Markov chains. We find posterior distributions and derive Bayes estimators in such models and also present a semiparametric Bayesian analysis of the Cox regression model. The Bayes estimators are easy to interpret and easy to compute. In the limiting case of a vague prior the Bayes solution for a cumulative hazard is the Nelson-Aalen estimator and the Bayes solution for a survival probability is the Kaplan-Meier estimator.},
	number = {3},
	urldate = {2021-03-11},
	journal = {The Annals of Statistics},
	author = {Hjort, Nils Lid},
	year = {1990},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {theory},
	pages = {1259--1294},
}

@article{yusuf_modifiable_2020,
	title = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE}): a prospective cohort study},
	volume = {395},
	issn = {01406736},
	shorttitle = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673619320082},
	doi = {10.1016/S0140-6736(19)32008-2},
	abstract = {Background Global estimates of the effect of common modifiable risk factors on cardiovascular disease and mortality are largely based on data from separate studies, using different methodologies. The Prospective Urban Rural Epidemiology (PURE) study overcomes these limitations by using similar methods to prospectively measure the effect of modifiable risk factors on cardiovascular disease and mortality across 21 countries (spanning five continents) grouped by different economic levels.},
	language = {en},
	number = {10226},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Yusuf, Salim and Joseph, Philip and Rangarajan, Sumathy and Islam, Shofiqul and Mente, Andrew and Hystad, Perry and Brauer, Michael and Kutty, Vellappillil Raman and Gupta, Rajeev and Wielgosz, Andreas and AlHabib, Khalid F and Dans, Antonio and Lopez-Jaramillo, Patricio and Avezum, Alvaro and Lanas, Fernando and Oguz, Aytekin and Kruger, Iolanthe M and Diaz, Rafael and Yusoff, Khalid and Mony, Prem and Chifamba, Jephat and Yeates, Karen and Kelishadi, Roya and Yusufali, Afzalhussein and Khatib, Rasha and Rahman, Omar and Zatonska, Katarzyna and Iqbal, Romaina and Wei, Li and Bo, Hu and Rosengren, Annika and Kaur, Manmeet and Mohan, Viswanathan and Lear, Scott A and Teo, Koon K and Leong, Darryl and O'Donnell, Martin and McKee, Martin and Dagenais, Gilles},
	month = mar,
	year = {2020},
	keywords = {application},
	pages = {795--808},
}

@article{kalbfleisch_marginal_1973,
	title = {Marginal {Likelihoods} {Based} on {Cox}'s {Regression} and {Life} {Model}},
	volume = {60},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334538},
	doi = {10.2307/2334538},
	abstract = {Marginal likelihoods are obtained for the regression parameters in the model presented by Cox (1972). If no ties occur in the recording of failure time data the results of Cox are given a straightforward justification. If ties occur in the data, results different from those suggested by Cox are obtained. Some Monte-Carlo comparisons of these competing results are made. A discrete model is developed for grouped data from Cox's model and estimates of the survivor function are given for both continuous and grouped data.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Kalbfleisch, J. D. and Prentice, R. L.},
	year = {1973},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {267--278},
}

@article{gustafson_large_1997,
	title = {Large {Hierarchical} {Bayesian} {Analysis} of {Multivariate} {Survival} {Data}},
	volume = {53},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533110},
	doi = {10.2307/2533110},
	abstract = {Failure times that are grouped according to shared environments arise commonly in statistical practice. That is, multiple responses may be observed for each of many units. For instance, the units might be patients or centers in a clinical trial setting. Bayesian hierarchical models are appropriate for data analysis in this context. At the first stage of the model, survival times can be modelled via the Cox partial likelihood, using a justification due to Kalbfleisch (1978, Journal of the Royal Statistical Society, Series B 40, 214-221). Thus, questionable parametric assumptions are avoided. Conventional wisdom dictates that it is comparatively safe to make parametric assumptions at subsequent stages. Thus, unit-specific parameters are modelled parametrically. The posterior distribution of parameters given observed data is examined using Markov chain Monte Carlo methods. Specifically, the hybrid Monte Carlo method, as described by Neal (1993a, in Advances in Neural Information Processing 5, 475-482; 1993b, Probabilistic inference using Markov chain Monte Carlo methods), is utilized.},
	number = {1},
	urldate = {2021-03-11},
	journal = {Biometrics},
	author = {Gustafson, Paul},
	year = {1997},
	note = {Publisher: [Wiley, International Biometric Society]},
	keywords = {theory},
	pages = {230--242},
}

@article{austin_generating_2012,
	title = {Generating survival times to simulate {Cox} proportional hazards models with time-varying covariates},
	volume = {31},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3546387/},
	doi = {10.1002/sim.5452},
	abstract = {Simulations and Monte Carlo methods serve an important role in modern statistical research. They allow for an examination of the performance of statistical procedures in settings in which analytic and mathematical derivations may not be feasible. A key element in any statistical simulation is the existence of an appropriate data-generating process: one must be able to simulate data from a specified statistical model. We describe data-generating processes for the Cox proportional hazards model with time-varying covariates when event times follow an exponential, Weibull, or Gompertz distribution. We consider three types of time-varying covariates: first, a dichotomous time-varying covariate that can change at most once from untreated to treated (e.g., organ transplant); second, a continuous time-varying covariate such as cumulative exposure at a constant dose to radiation or to a pharmaceutical agent used for a chronic condition; third, a dichotomous time-varying covariate with a subject being able to move repeatedly between treatment states (e.g., current compliance or use of a medication). In each setting, we derive closed-form expressions that allow one to simulate survival times so that survival times are related to a vector of fixed or time-invariant covariates and to a single time-varying covariate. We illustrate the utility of our closed-form expressions for simulating event times by using Monte Carlo simulations to estimate the statistical power to detect as statistically significant the effect of different types of binary time-varying covariates. This is compared with the statistical power to detect as statistically significant a binary time-invariant covariate. Copyright © 2012 John Wiley \& Sons, Ltd.},
	number = {29},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C},
	month = dec,
	year = {2012},
	pmid = {22763916},
	pmcid = {PMC3546387},
	keywords = {simulation},
	pages = {3946--3958},
}

@article{dagostino_ralph_b_general_2008,
	title = {General {Cardiovascular} {Risk} {Profile} for {Use} in {Primary} {Care}},
	volume = {117},
	url = {https://www.ahajournals.org/doi/10.1161/circulationaha.107.699579},
	doi = {10.1161/CIRCULATIONAHA.107.699579},
	abstract = {Background— Separate multivariable risk algorithms are commonly used to assess risk of specific atherosclerotic cardiovascular disease (CVD) events, ie, coronary heart disease, cerebrovascular disease, peripheral vascular disease, and heart failure. The present report presents a single multivariable risk function that predicts risk of developing all CVD and of its constituents.Methods and Results— We used Cox proportional-hazards regression to evaluate the risk of developing a first CVD event in 8491 Framingham study participants (mean age, 49 years; 4522 women) who attended a routine examination between 30 and 74 years of age and were free of CVD. Sex-specific multivariable risk functions (“general CVD” algorithms) were derived that incorporated age, total and high-density lipoprotein cholesterol, systolic blood pressure, treatment for hypertension, smoking, and diabetes status. We assessed the performance of the general CVD algorithms for predicting individual CVD events (coronary heart disease, stroke, peripheral artery disease, or heart failure). Over 12 years of follow-up, 1174 participants (456 women) developed a first CVD event. All traditional risk factors evaluated predicted CVD risk (multivariable-adjusted P{\textless}0.0001). The general CVD algorithm demonstrated good discrimination (C statistic, 0.763 [men] and 0.793 [women]) and calibration. Simple adjustments to the general CVD risk algorithms allowed estimation of the risks of each CVD component. Two simple risk scores are presented, 1 based on all traditional risk factors and the other based on non–laboratory-based predictors.Conclusions— A sex-specific multivariable risk factor algorithm can be conveniently used to assess general CVD risk and risk of individual CVD events (coronary, cerebrovascular, and peripheral arterial disease and heart failure). The estimated absolute CVD event rates can be used to quantify risk and to guide preventive care.},
	number = {6},
	urldate = {2021-03-10},
	journal = {Circulation},
	author = {{D’Agostino Ralph B.} and {Vasan Ramachandran S.} and {Pencina Michael J.} and {Wolf Philip A.} and {Cobain Mark} and {Massaro Joseph M.} and {Kannel William B.}},
	month = feb,
	year = {2008},
	note = {Publisher: American Heart Association},
	keywords = {application},
	pages = {743--753},
}

@article{li_fast_2020,
	title = {Fast {Lasso} method for large-scale and ultrahigh-dimensional {Cox} model with applications to {UK} {Biobank}},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxaa038},
	doi = {10.1093/biostatistics/kxaa038},
	abstract = {We develop a scalable and highly efficient algorithm to fit a Cox proportional hazard model by maximizing the \$L{\textasciicircum}1\$-regularized (Lasso) partial likelihood function, based on the Batch Screening Iterative Lasso (BASIL) method developed in Qian and others (2019). Our algorithm is particularly suitable for large-scale and high-dimensional data that do not fit in the memory. The output of our algorithm is the full Lasso path, the parameter estimates at all predefined regularization parameters, as well as their validation accuracy measured using the concordance index (C-index) or the validation deviance. To demonstrate the effectiveness of our algorithm, we analyze a large genotype-survival time dataset across 306 disease outcomes from the UK Biobank (Sudlow and others, 2015). We provide a publicly available implementation of the proposed approach for genetics data on top of the PLINK2 package and name it snpnet-Cox.},
	number = {kxaa038},
	urldate = {2021-03-11},
	journal = {Biostatistics},
	author = {Li, Ruilin and Chang, Christopher and Justesen, Johanne M and Tanigawa, Yosuke and Qiang, Junyang and Hastie, Trevor and Rivas, Manuel A and Tibshirani, Robert},
	month = sep,
	year = {2020},
	keywords = {theory},
}

@article{mortensen_elevated_2020,
	title = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years: a contemporary primary prevention cohort},
	volume = {396},
	issn = {01406736},
	shorttitle = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673620322339},
	doi = {10.1016/S0140-6736(20)32233-9},
	abstract = {Background Findings of historical studies suggest that elevated LDL cholesterol is not associated with increased risk of myocardial infarction and atherosclerotic cardiovascular disease in patients older than 70 years. We aimed to test this hypothesis in a contemporary population of individuals aged 70–100 years.},
	language = {en},
	number = {10263},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Mortensen, Martin Bødtker and Nordestgaard, Børge Grønne},
	month = nov,
	year = {2020},
	keywords = {application},
	pages = {1644--1652},
}

@article{hendry_data_2014,
	title = {Data generation for the {Cox} proportional hazards model with time-dependent covariates: a method for medical researchers},
	volume = {33},
	issn = {02776715},
	shorttitle = {Data generation for the {Cox} proportional hazards model with time-dependent covariates},
	url = {http://doi.wiley.com/10.1002/sim.5945},
	doi = {10.1002/sim.5945},
	abstract = {The proliferation of longitudinal studies has increased the importance of statistical methods for time-to-event data that can incorporate time-dependent covariates. The Cox proportional hazards model is one such method that is widely used. As more extensions of the Cox model with time-dependent covariates are developed, simulations studies will grow in importance as well. An essential starting point for simulation studies of time-to-event models is the ability to produce simulated survival times from a known data generating process. This paper develops a method for the generation of survival times that follow a Cox proportional hazards model with time-dependent covariates. The method presented relies on a simple transformation of random variables generated according to a truncated piecewise exponential distribution, and allows practitioners great ﬂexibility and control over both the number of time-dependent covariates and the number of time periods in the duration of follow-up measurement. Within this framework, an additional argument is suggested that allows researchers to generate time-to-event data in which covariates change at integer-valued steps of the time scale. The purpose of this approach is to produce data for simulation experiments that mimic the types of data structures applied researchers encounter when using longitudinal biomedical data. Validity is assessed in a set of simulation experiments and results indicate that the proposed procedure performs well in producing data that conform to the assumptions of the Cox proportional hazards model.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Hendry, David J.},
	month = feb,
	year = {2014},
	keywords = {simulation},
	pages = {436--454},
}

@article{andersen_coxs_1982,
	title = {Cox's {Regression} {Model} for {Counting} {Processes}: {A} {Large} {Sample} {Study}},
	volume = {10},
	issn = {0090-5364},
	shorttitle = {Cox's {Regression} {Model} for {Counting} {Processes}},
	url = {http://projecteuclid.org/euclid.aos/1176345976},
	doi = {10.1214/aos/1176345976},
	language = {en},
	number = {4},
	urldate = {2021-03-10},
	journal = {The Annals of Statistics},
	author = {Andersen, P. K. and Gill, R. D.},
	month = dec,
	year = {1982},
	keywords = {theory},
	pages = {1100--1120},
}

@article{sylvestre_comparison_2008,
	title = {Comparison of algorithms to generate event times conditional on time-dependent covariates},
	volume = {27},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3092},
	doi = {https://doi.org/10.1002/sim.3092},
	abstract = {The Cox proportional hazards model with time-dependent covariates (TDC) is now a part of the standard statistical analysis toolbox in medical research. As new methods involving more complex modeling of time-dependent variables are developed, simulations could often be used to systematically assess the performance of these models. Yet, generating event times conditional on TDC requires well-designed and efficient algorithms. We compare two classes of such algorithms: permutational algorithms (PAs) and algorithms based on a binomial model. We also propose a modification of the PA to incorporate a rejection sampler. We performed a simulation study to assess the accuracy, stability, and speed of these algorithms in several scenarios. Both classes of algorithms generated data sets that, once analyzed, provided virtually unbiased estimates with comparable variances. In terms of computational efficiency, the PA with the rejection sampler reduced the time necessary to generate data by more than 50 per cent relative to alternative methods. The PAs also allowed more flexibility in the specification of the marginal distributions of event times and required less calibration. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {14},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Sylvestre, Marie-Pierre and Abrahamowicz, Michal},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3092},
	keywords = {simulation, proportional hazards model, algorithms, rejection sampling, simulations, survival analysis, time-dependent covariates},
	pages = {2618--2634},
}

@article{alaa_cardiovascular_2019,
	title = {Cardiovascular disease risk prediction using automated machine learning: {A} prospective study of 423,604 {UK} {Biobank} participants},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Cardiovascular disease risk prediction using automated machine learning},
	url = {https://dx.plos.org/10.1371/journal.pone.0213653},
	doi = {10.1371/journal.pone.0213653},
	language = {en},
	number = {5},
	urldate = {2021-03-10},
	journal = {PLOS ONE},
	author = {Alaa, Ahmed M. and Bolton, Thomas and Di Angelantonio, Emanuele and Rudd, James H. F. and van der Schaar, Mihaela},
	editor = {Aalto-Setala, Katriina},
	month = may,
	year = {2019},
	keywords = {application},
	pages = {e0213653},
}

@article{kucukelbir_automatic_2017,
	title = {Automatic {Differentiation} {Variational} {Inference}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/16-107.html},
	number = {14},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	year = {2017},
	keywords = {theory},
	pages = {1--45},
}

@article{andersen_analysis_2021,
	title = {Analysis of time-to-event for observational studies: {Guidance} to the use of intensity models},
	volume = {40},
	issn = {1097-0258},
	shorttitle = {Analysis of time-to-event for observational studies},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8757},
	doi = {https://doi.org/10.1002/sim.8757},
	abstract = {This paper provides guidance for researchers with some mathematical background on the conduct of time-to-event analysis in observational studies based on intensity (hazard) models. Discussions of basic concepts like time axis, event definition and censoring are given. Hazard models are introduced, with special emphasis on the Cox proportional hazards regression model. We provide check lists that may be useful both when fitting the model and assessing its goodness of fit and when interpreting the results. Special attention is paid to how to avoid problems with immortal time bias by introducing time-dependent covariates. We discuss prediction based on hazard models and difficulties when attempting to draw proper causal conclusions from such models. Finally, we present a series of examples where the methods and check lists are exemplified. Computational details and implementation using the freely available R software are documented in Supplementary Material. The paper was prepared as part of the STRATOS initiative.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Andersen, Per Kragh and Perme, Maja Pohar and Houwelingen, Hans C. van and Cook, Richard J. and Joly, Pierre and Martinussen, Torben and Taylor, Jeremy M. G. and Abrahamowicz, Michal and Therneau, Terry M.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8757},
	keywords = {theory, survival analysis, time-dependent covariates, censoring, Cox regression model, hazard function, immortal time bias, multistate model, prediction, STRATOS initiative},
	pages = {185--211},
}

@article{mackenzie_marginal_2002,
	title = {Marginal and hazard ratio specific random data generation: {Applications} to semi-parametric bootstrapping},
	volume = {12},
	issn = {1573-1375},
	shorttitle = {Marginal and hazard ratio specific random data generation},
	url = {https://doi.org/10.1023/A:1020750810409},
	doi = {10.1023/A:1020750810409},
	abstract = {Cox's partial likelihood for censored time-to-event data can be interpreted as a permutation probability, whereby covariate values are permuted to the observed times-to-event and censoring times. This interpretation facilitates a simple method for jointly generating times-to-event and covariate tuples with considerable flexibility, including time dependence of the hazard ratio and specification of both the marginal time-to-event and covariate distributions. This interpretation also facilitates a method for semi-parametric bootstrapping of hazard ratio estimators.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics and Computing},
	author = {Mackenzie, Todd and Abrahamowicz, Michal},
	month = jul,
	year = {2002},
	keywords = {simulation},
	pages = {245--252},
}

@article{abrahamowicz_time-dependent_1996,
	title = {Time-{Dependent} {Hazard} {Ratio}: {Modeling} and {Hypothesis} {Testing} with {Application} in {Lupus} {Nephritis}},
	volume = {91},
	issn = {0162-1459},
	shorttitle = {Time-{Dependent} {Hazard} {Ratio}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476711},
	doi = {10.1080/01621459.1996.10476711},
	abstract = {We investigate the association between duration of untreated disease and survival in lupus nephritis, a rare rheumatologic disease. In this case, as in many other studies of survival, a priori considerations suggest that the effect of the predictor on hazard may change with increasing follow-up time. To accommodate such situations, we use regression splines to model the hazard ratio as a flexible function of time. We propose model-based tests of the hypotheses of hazards proportionality and of no association. We evaluate the accuracy of estimation and inference in simulations and also present analysis of a larger medical data set.},
	number = {436},
	urldate = {2021-03-11},
	journal = {Journal of the American Statistical Association},
	author = {Abrahamowicz, Michal and Mackenzie, Todd and Esdaile, John M.},
	month = dec,
	year = {1996},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476711},
	keywords = {simulation, Akaike information criterion, Partial likelihood, Proportional hazards, Regression splines, Survival analysis},
	pages = {1432--1439},
}

@article{harden_simulating_2019,
	title = {Simulating {Duration} {Data} for the {Cox} {Model}},
	volume = {7},
	issn = {2049-8470, 2049-8489},
	url = {https://www.cambridge.org/core/journals/political-science-research-and-methods/article/simulating-duration-data-for-the-cox-model/1945D7548766E76FB31C6C833976822E},
	doi = {10.1017/psrm.2018.19},
	abstract = {The Cox proportional hazards model is a popular method for duration analysis that is frequently the subject of simulation studies. However, no standard method exists for simulating durations directly from its data generating process because it does not assume a distributional form for the baseline hazard function. Instead, simulation studies typically rely on parametric survival distributions, which contradicts the primary motivation for employing the Cox model. We propose a method that generates a baseline hazard function at random by fitting a cubic spline to randomly drawn points. Durations drawn from this function match the Cox model’s inherent flexibility and improve the simulation’s generalizability. The method can be extended to include time-varying covariates and non-proportional hazards.},
	language = {en},
	number = {4},
	urldate = {2021-03-16},
	journal = {Political Science Research and Methods},
	author = {Harden, Jeffrey J. and Kropko, Jonathan},
	month = oct,
	year = {2019},
	note = {Publisher: Cambridge University Press},
	pages = {921--928},
}

@article{therneau_package_2015,
	title = {Package ‘survival’},
	volume = {128},
	number = {10},
	journal = {R Top Doc},
	author = {Therneau, Terry M and Lumley, Thomas},
	year = {2015},
	pages = {28--33},
}

@article{quiroz_speeding_2019,
	title = {Speeding {Up} {MCMC} by {Efficient} {Data} {Subsampling}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2018.1448827},
	doi = {10.1080/01621459.2018.1448827},
	abstract = {We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online.},
	number = {526},
	urldate = {2021-03-30},
	journal = {Journal of the American Statistical Association},
	author = {Quiroz, Matias and Kohn, Robert and Villani, Mattias and Tran, Minh-Ngoc},
	month = apr,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2018.1448827},
	keywords = {Bayesian inference, Big Data, Block pseudo-marginal, Correlated pseudo-marginal, Estimated likelihood, Survey sampling},
	pages = {831--843},
}

@inproceedings{bardenet_adaptive_2014,
	title = {An adaptive subsampling approach for mcmc inference in large datasets},
	booktitle = {Proceedings of {The} 31st {International} {Conference} on {Machine} {Learning}},
	author = {Bardenet, Rémi and Doucet, Arnaud and Holmes, Chris},
	year = {2014},
	pages = {405--413},
}

@inproceedings{betancourt_fundamental_2015,
	title = {The fundamental incompatibility of scalable {Hamiltonian} {Monte} {Carlo} and naive data subsampling},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Betancourt, Michael},
	year = {2015},
	pages = {533--540},
}

@article{teh_consistency_2016,
	title = {Consistency and fluctuations for stochastic gradient {Langevin} dynamics},
	volume = {17},
	journal = {Journal of Machine Learning Research},
	author = {Teh, Yee Whye and Thiery, Alexandre H and Vollmer, Sebastian J},
	year = {2016},
	note = {Publisher: Journal of Machine Learning Research},
}

@article{royston_flexible_2002,
	title = {Flexible parametric proportional-hazards and proportional-odds models for censored survival data, with application to prognostic modelling and estimation of treatment effects},
	volume = {21},
	issn = {0277-6715},
	doi = {10.1002/sim.1203},
	abstract = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, non-proportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer.},
	language = {eng},
	number = {15},
	journal = {Statistics in Medicine},
	author = {Royston, Patrick and Parmar, Mahesh K. B.},
	month = aug,
	year = {2002},
	pmid = {12210632},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Antineoplastic Agents, Breast Neoplasms, Carcinoma, Transitional Cell, Female, Models, Biological, Prognosis, Treatment Outcome, Urinary Bladder Neoplasms},
	pages = {2175--2197},
}

@article{nietobarajas_markov_2002,
	title = {Markov {Beta} and {Gamma} {Processes} for {Modelling} {Hazard} {Rates}},
	volume = {29},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00298},
	doi = {https://doi.org/10.1111/1467-9469.00298},
	abstract = {This paper generalizes the discrete time independent increment beta process of Hjort (1990), for modelling discrete failure times, and also generalizes the independent gamma process for modelling piecewise constant hazard rates (Walker and Mallick, 1997). The generalizations are from independent increment to Markov increment prior processes allowing the modelling of smoothness. We derive posterior distributions and undertake a full Bayesian analysis.},
	language = {en},
	number = {3},
	urldate = {2021-03-29},
	journal = {Scandinavian Journal of Statistics},
	author = {Nieto‐Barajas, Luis E. and Walker, Stephen G.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9469.00298},
	keywords = {survival analysis, Bayes non-parametrics, beta process, gamma process, Markov process, stationary process},
	pages = {413--424},
}

@incollection{laud_bayesian_1998,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Nonparametric} and {Covariate} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-4612-1732-9},
	url = {https://doi.org/10.1007/978-1-4612-1732-9_11},
	abstract = {A Bayesian analysis of the semi-parametric regression model of Cox (1972) is given. The cumulative hazard function is modelled as a beta process. The posterior distribution of the regression parameters and the survival function are obtained using a combination of recent Monte Carlo methods. An illustrative analysis within the context of survival time data is given.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {Practical {Nonparametric} and {Semiparametric} {Bayesian} {Statistics}},
	publisher = {Springer},
	author = {Laud, Purushottam W. and Damien, Paul and Smith, Adrian F. M.},
	editor = {Dey, Dipak and Müller, Peter and Sinha, Debajyoti},
	year = {1998},
	doi = {10.1007/978-1-4612-1732-9_11},
	keywords = {Cumulative Hazard, Frailty Model, Hazard Rate, Markov Chain Monte Carlo Method, Posterior Distribution},
	pages = {213--225},
}

@article{sinha_semiparametric_1997,
	title = {Semiparametric {Bayesian} {Analysis} of {Survival} {Data}},
	volume = {92},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2965586},
	doi = {10.2307/2965586},
	abstract = {This review article investigates the potential of Bayes methods for the analysis of survival data using semiparametric models based on either the hazard or the intensity function. The nonparametric part of every model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, including sampling-based algorithms, are used to find posterior estimates of several quantities of interest even when dealing with complex models and unusual data structures. The methodologies developed herein are motivated and aimed at analyzing some common types of survival data from different medical studies; here we focus on univariate survival data in the presence of fixed and time-dependent covariates, multiple event-time data for repeated nonfatal events, and multivariate survival data (subjects are related; e.g., families or litters), each patient with interval-censored infection time and interval-censored disease occurrence time in tandem [e.g., patients with acquired immunodeficiency syndrome (AIDS) and other infectious diseases with long incubation times]. Bayesian exploratory data analysis (EDA) methods and diagnostics for model selection and model assessment are considered for each case. Special attention is given to tests of the parametric modeling assumptions and to censoring.},
	number = {439},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti and Dey, Dipak K.},
	year = {1997},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1195--1212},
}

@article{qiou_multivariate_1999,
	title = {Multivariate survival analysis with positive stable frailties},
	volume = {55},
	issn = {0006-341X},
	doi = {10.1111/j.0006-341x.1999.00637.x},
	abstract = {In this paper, we describe Bayesian modeling of dependent multivariate survival data using positive stable frailty distributions. A flexible baseline hazard formulation using a piecewise exponential model with a correlated prior process is used. The estimation of the stable law parameter together with the parameters of the (conditional) proportional hazards model is facilitated by a modified Gibbs sampling procedure. The methodology is illustrated on kidney infection data (McGilchrist and Aisbett, 1991).},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Qiou, Z. and Ravishanker, N. and Dey, D. K.},
	month = jun,
	year = {1999},
	pmid = {11318227},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Female, Algorithms, Bayes Theorem, Biometry, Kidney Diseases, Male, Multivariate Analysis},
	pages = {637--644},
}

@article{sinha_semiparametric_1993,
	title = {Semiparametric {Bayesian} {Analysis} of {Multiple} {Event} {Time} {Data}},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290789},
	doi = {10.2307/2290789},
	abstract = {Multiple event time data (e.g., carcinogenic growths in different times and locations, multiple attacks of cardiac arrest) arise in various medical studies. A Bayesian analysis of such data based on proportional intensity model of multiple event time data is presented in this paper. The Bayesian structure is somewhat analogous to that used by Kalbfleisch in a proportional hazard model. An unobserved random frailty component is used in the proportional intensity model to take care of heterogeneity among the intensity processes in different subjects. The Monte Carlo method of sampling from multivariate distributions, the so-called Gibbs sampler, is used to sample from the joint posterior distribution of the unknown parameters. The methodology developed here is exemplified with the well-known data set on rat tumors of Gail, Santner, and Brown.},
	number = {423},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {979--983},
}

@article{muller_bayesian_2013,
	title = {Bayesian {Nonparametric} {Inference} – {Why} and {How}},
	volume = {8},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-8/issue-2/Bayesian-Nonparametric-Inference--Why-and-How/10.1214/13-BA811.full},
	doi = {10.1214/13-BA811},
	abstract = {We review inference under models with nonparametric Bayesian (BNP) priors. The discussion follows a set of examples for some common inference problems. The examples are chosen to highlight problems that are challenging for standard parametric inference. We discuss inference for density estimation, clustering, regression and for mixed effects models with random effects distributions. While we focus on arguing for the need for the flexibility of BNP models, we also review some of the more commonly used BNP models, thus hopefully answering a bit of both questions, why and how to use BNP. This review was sponsored by the Bayesian Nonparametrics Section of ISBA (ISBA/BNP). The authors thank the section officers for the support and encouragement.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Müller, Peter and Mitra, Riten},
	month = jun,
	year = {2013},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Dirichlet process, dependent Dirichlet process, nonparametric models, Polya tree},
	pages = {269--302},
}

@article{muller_nonparametric_2004,
	title = {Nonparametric {Bayesian} {Data} {Analysis}},
	volume = {19},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Nonparametric-Bayesian-Data-Analysis/10.1214/088342304000000017.full},
	doi = {10.1214/088342304000000017},
	abstract = {We review the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem we review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models.},
	number = {1},
	urldate = {2021-03-29},
	journal = {Statistical Science},
	author = {Müller, Peter and Quintana, Fernando A.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Survival analysis, Density estimation, Dirichlet process, Pólya tree, random probability model (RPM), regression},
	pages = {95--110},
}

@article{cai_mixed_2002,
	title = {Mixed {Model}-{Based} {Hazard} {Estimation}},
	volume = {11},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/1391161},
	abstract = {This article proposes a new method for estimation of the hazard function from a set of censored failure time data, with a view to extending the general approach to more complicated models. The approach is based on a mixed model representation of penalized spline hazard estimators. One payoff is the automation of the smoothing parameter choice through restricted maximum likelihood. Another is the option to use standard mixed model software for automatic hazard estimation.},
	number = {4},
	urldate = {2021-03-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cai, T. and Hyndman, R. J. and Wand, M. P.},
	year = {2002},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {784--798},
}

@article{hennerfeind_geoadditive_2006,
	title = {Geoadditive {Survival} {Models}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000348},
	doi = {10.1198/016214506000000348},
	abstract = {Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoadditive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting.},
	number = {475},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Hennerfeind, Andrea and Brezger, Andreas and Fahrmeir, Ludwig},
	month = sep,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000000348},
	keywords = {Bayesian hazard rate model, Markov chain Monte Carlo, Markov random field, Penalized spline, Semiparametric modeling, Spatial survival data},
	pages = {1065--1075},
}

@article{alvares_bayesian_2021,
	title = {Bayesian survival analysis with {BUGS}},
	journal = {Statistics in Medicine},
	author = {Alvares, Danilo and Lázaro, Elena and Gómez-Rubio, Virgilio and Armero, Carmen},
	year = {2021},
	note = {Publisher: Wiley Online Library},
}

@article{alsefri_bayesian_2020,
	title = {Bayesian joint modelling of longitudinal and time to event data: a methodological review},
	volume = {20},
	issn = {1471-2288},
	shorttitle = {Bayesian joint modelling of longitudinal and time to event data},
	url = {https://doi.org/10.1186/s12874-020-00976-2},
	doi = {10.1186/s12874-020-00976-2},
	abstract = {In clinical research, there is an increasing interest in joint modelling of longitudinal and time-to-event data, since it reduces bias in parameter estimation and increases the efficiency of statistical inference. Inference and prediction from frequentist approaches of joint models have been extensively reviewed, and due to the recent popularity of data-driven Bayesian approaches, a review on current Bayesian estimation of joint model is useful to draw recommendations for future researches.},
	number = {1},
	urldate = {2021-03-29},
	journal = {BMC Medical Research Methodology},
	author = {Alsefri, Maha and Sudell, Maria and García-Fiñana, Marta and Kolamunnage-Dona, Ruwanthi},
	month = apr,
	year = {2020},
	keywords = {Bayesian estimation, Dynamic prediction, Joint models, Longitudinal outcomes, Time-to-event},
	pages = {94},
}

@article{sharef_bayesian_2010,
	title = {Bayesian adaptive {B}-spline estimation in proportional hazards frailty models},
	volume = {4},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-4/issue-none/Bayesian-adaptive-B-spline-estimation-in-proportional-hazards-frailty-models/10.1214/10-EJS566.full},
	doi = {10.1214/10-EJS566},
	abstract = {Frailty models derived from the proportional hazards regression model are frequently used to analyze clustered right-censored survival data. We propose a semiparametric Bayesian methodology for this purpose, modeling both the unknown baseline hazard and density of the random effects using mixtures of B-splines. The posterior distributions for all regression coefficients and spline parameters are obtained using Markov Chain Monte Carlo (MCMC). The methodology permits the use of weighted mixtures of parametric and nonparametric components in modeling the hazard function and frailty distribution; in addition, the spline knots may also be selected adaptively using reversible-jump MCMC. Simulations indicate that the method produces smooth and accurate posterior hazard and frailty density estimates. The Bayesian approach not only produces point estimators that outperform existing approaches in certain circumstances, but also offers a wealth of information about the parameters of interest in the form of MCMC samples from the joint posterior probability distribution. We illustrate the adaptability of the method with data from a study of congestive heart failure.},
	number = {none},
	urldate = {2021-03-29},
	journal = {Electronic Journal of Statistics},
	author = {Sharef, Emmanuel and Strawderman, Robert L. and Ruppert, David and Cowen, Mark and Halasyamani, Lakshmi},
	month = jan,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Survival analysis, frailty distribution, Hazard regression, heart failure, knot selection, random effect density, re-hospitalization, reversible-jump MCMC},
	pages = {606--642},
}

@article{murray_flexible_2016,
	title = {Flexible {Bayesian} {Survival} {Modeling} with {Semiparametric} {Time}-{Dependent} and {Shape}-{Restricted} {Covariate} {Effects}},
	volume = {11},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-2/Flexible-Bayesian-Survival-Modeling-with-Semiparametric-Time-Dependent-and-Shape/10.1214/15-BA954.full},
	doi = {10.1214/15-BA954},
	abstract = {Presently, there are few options with available software to perform a fully Bayesian analysis of time-to-event data wherein the hazard is estimated semi- or non-parametrically. One option is the piecewise exponential model, which requires an often unrealistic assumption that the hazard is piecewise constant over time. The primary aim of this paper is to construct a tractable semiparametric alternative to the piecewise exponential model that assumes the hazard is continuous, and to provide modifiable, user-friendly software that allows the use of these methods in a variety of settings. To accomplish this aim, we use a novel model formulation for the log-hazard based on a low-rank thin plate linear spline that readily facilitates adjustment for covariates with time-dependent and proportional hazards effects, possibly subject to shape restrictions. We investigate the performance of our model choices via simulation. We then analyze colorectal cancer data from a clinical trial comparing the effectiveness of two novel treatment regimes relative to the standard of care for overall survival. We estimate a time-dependent hazard ratio for each novel regime relative to the standard of care while adjusting for the effect of aspartate transaminase, a biomarker of liver function, that is subject to a non-decreasing shape restriction.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Murray, Thomas A. and Hobbs, Brian P. and Sargent, Daniel J. and Carlin, Bradley P.},
	month = jun,
	year = {2016},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Survival analysis, 62F15, 62F30, 62N86, Bayesian methods, colorectal cancer, penalized splines, semiparametric methods, shape-restricted effects, time-dependent effects},
	pages = {381--402},
}

@article{williamson_factors_2020,
	title = {Factors associated with {COVID}-19-related death using {OpenSAFELY}},
	volume = {584},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2521-4},
	doi = {10.1038/s41586-020-2521-4},
	abstract = {Coronavirus disease 2019 (COVID-19) has rapidly affected mortality worldwide1. There is unprecedented urgency to understand who is most at risk of severe outcomes, and this requires new approaches for the timely analysis of large datasets. Working on behalf of NHS England, we created OpenSAFELY—a secure health analytics platform that covers 40\% of all patients in England and holds patient data within the existing data centre of a major vendor of primary care electronic health records. Here we used OpenSAFELY to examine factors associated with COVID-19-related death. Primary care records of 17,278,392 adults were pseudonymously linked to 10,926 COVID-19-related deaths. COVID-19-related death was associated with: being male (hazard ratio (HR) 1.59 (95\% confidence interval 1.53–1.65)); greater age and deprivation (both with a strong gradient); diabetes; severe asthma; and various other medical conditions. Compared with people of white ethnicity, Black and South Asian people were at higher risk, even after adjustment for other factors (HR 1.48 (1.29–1.69) and 1.45 (1.32–1.58), respectively). We have quantified a range of clinical factors associated with COVID-19-related death in one of the largest cohort studies on this topic so far. More patient records are rapidly being added to OpenSAFELY, we will update and extend our results regularly.},
	language = {en},
	number = {7821},
	urldate = {2021-03-28},
	journal = {Nature},
	author = {Williamson, Elizabeth J. and Walker, Alex J. and Bhaskaran, Krishnan and Bacon, Seb and Bates, Chris and Morton, Caroline E. and Curtis, Helen J. and Mehrkar, Amir and Evans, David and Inglesby, Peter and Cockburn, Jonathan and McDonald, Helen I. and MacKenna, Brian and Tomlinson, Laurie and Douglas, Ian J. and Rentsch, Christopher T. and Mathur, Rohini and Wong, Angel Y. S. and Grieve, Richard and Harrison, David and Forbes, Harriet and Schultze, Anna and Croker, Richard and Parry, John and Hester, Frank and Harper, Sam and Perera, Rafael and Evans, Stephen J. W. and Smeeth, Liam and Goldacre, Ben},
	month = aug,
	year = {2020},
	note = {Number: 7821
Publisher: Nature Publishing Group},
	pages = {430--436},
}

@article{qian_fast_2019,
	title = {A fast and flexible algorithm for solving the lasso in large-scale and ultrahigh-dimensional problems},
	journal = {BioRxiv},
	author = {Qian, Junyang and Du, Wenfei and Tanigawa, Yosuke and Aguirre, Matthew and Tibshirani, Robert and Rivas, Manuel A and Hastie, Trevor},
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {630079},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214508000000337},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and {Its} {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/3085904},
	abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
	number = {456},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1348--1360},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Bootstrap {Methods}},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full},
	doi = {10.1214/aos/1176344552},
	abstract = {We discuss the following problem: given a random sample \${\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ from an unknown probability distribution \$F\$, estimate the sampling distribution of some prespecified random variable \$R({\textbackslash}mathbf\{X\}, F)\$, on the basis of the observed data \${\textbackslash}mathbf\{x\}\$. (Standard jackknife theory gives an approximate mean and variance in the case \$R({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta\$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	number = {1},
	urldate = {2021-03-28},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G05, 62G15, 62H30, 62J05, bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
	pages = {1--26},
}

@article{yang_cocktail_2013,
	title = {A cocktail algorithm for solving the elastic net penalized {Cox}’s regression in high dimensions},
	volume = {6},
	number = {2},
	journal = {Statistics and its Interface},
	author = {Yang, Yi and Zou, Hui},
	year = {2013},
	note = {Publisher: International Press of Boston},
	pages = {167--173},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288},
}

@article{tibshirani_lasso_1997,
	title = {The lasso method for variable selection in the {Cox} model},
	volume = {16},
	issn = {0277-6715},
	doi = {10.1002/(sici)1097-0258(19970228)16:4<385::aid-sim380>3.0.co;2-3},
	abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
	language = {eng},
	number = {4},
	journal = {Statistics in Medicine},
	author = {Tibshirani, R.},
	month = feb,
	year = {1997},
	pmid = {9044528},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Karnofsky Performance Status, Likelihood Functions, Liver Cirrhosis, Lung Neoplasms, Randomized Controlled Trials as Topic},
	pages = {385--395},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2021-03-28},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
}

@article{simon_regularization_2011,
	title = {Regularization {Paths} for {Cox}'s {Proportional} {Hazards} {Model} via {Coordinate} {Descent}},
	volume = {39},
	issn = {1548-7660},
	doi = {10.18637/jss.v039.i05},
	abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
	language = {eng},
	number = {5},
	journal = {Journal of Statistical Software},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = mar,
	year = {2011},
	pmid = {27065756},
	pmcid = {PMC4824408},
	keywords = {Cox model, elastic net, lasso, survival},
	pages = {1--13},
}

@article{witten_survival_2010,
	title = {Survival analysis with high-dimensional covariates},
	volume = {19},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806549/},
	doi = {10.1177/0962280209105024},
	abstract = {In recent years, breakthroughs in biomedical technology have led to a wealth of data in which the number of features (for instance, genes on which expression measurements are available) exceeds the number of observations (e.g. patients). Sometimes survival outcomes are also available for those same observations. In this case, one might be interested in (a) identifying features that are associated with survival (in a univariate sense), and (b) developing a multivariate model for the relationship between the features and survival that can be used to predict survival in a new observation. Due to the high dimensionality of this data, most classical statistical methods for survival analysis cannot be applied directly. Here, we review a number of methods from the literature that address these two problems.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Statistical methods in medical research},
	author = {Witten, Daniela M and Tibshirani, Robert},
	month = feb,
	year = {2010},
	pmid = {19654171},
	pmcid = {PMC4806549},
	pages = {29--51},
}

@book{andersen_statistical_1993,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Statistical {Models} {Based} on {Counting} {Processes}},
	isbn = {978-0-387-94519-4},
	url = {https://www.springer.com/gp/book/9780387945194},
	abstract = {Modern survival analysis and more general event history analysis may be effectively handled in the mathematical framework of counting processes, stochastic integration, martingale central limit theory and product integration. This book presents this theory, which has been the subject of an intense research activity during the past one-and-a- half decades. The exposition of the theory is integrated with careful presentation of many practical examples, almost exclusively from the authors' own experience, with detailed numerical and graphical illustrations. Statistical Models Based on Counting Processes may be viewed as a research monograph for mathematical statisticians and biostatisticians, although almost all methods are given in concrete detail to be used in practice by other mathematically oriented researchers studying event histories (demographers, econometricians, epidemiologists, actuarial mathematicians, reliabilty engineers and biologists). Much of the material has so far only been available in the journal literature (if at all), and so a wide variety of researchers will find this an invaluable survey of the subject."This book is a masterful account of the counting process approach...is certain to be the standard reference for the area, and should be on the bookshelf of anyone interested in event-history analysis." International Statistical Institute Short Book Reviews "...this impressive reference, which contains a a wealth of powerful mathematics, practical examples, and analytic insights, as well as a complete integration of historical developments and recent advances in event history analysis." Journal of the American Statistical Association},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Andersen, Per Kragh and Borgan, Ornulf and Gill, Richard D. and Keiding, Niels},
	year = {1993},
	doi = {10.1007/978-1-4612-4348-9},
}

@book{kalbfleisch_statistical_2011,
	title = {The {Statistical} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-118-03123-0},
	abstract = {* Contains additional discussion and examples on left truncation as well as material on more general censoring and truncation patterns. * Introduces the martingale and counting process formulation swil lbe in a new chapter. * Develops multivariate failure time data in a separate chapter and extends the material on Markov and semi Markov formulations. * Presents new examples and applications of data analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Kalbfleisch, John D. and Prentice, Ross L.},
	month = jan,
	year = {2011},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{nan_asymptotic_2009,
	title = {Asymptotic {Theory} for the {Semiparametric} {Accelerated} {Failure} {Time} {Model} with {Missing} {Data}},
	volume = {37},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/30243708},
	abstract = {We consider a class of doubly weighted rank-based estimating methods for the transformation (or accelerated failure time) model with missing data as arise, for example, in case-cohort studies. The weights considered may not be predictable as required in a martingale stochastic process formulation. We treat the general problem as a semiparametric estimating equation problem and provide proofs of asymptotic properties for the weighted estimators, with either true weights or estimated weights, by using empirical process theory where martingale theory may fail. Simulations show that the outcome-dependent weighted method works well for finite samples in case-cohort studies and improves efficiency compared to methods based on predictable weights. Further, it is seen that the method is even more efficient when estimated weights are used, as is commonly the case in the missing data literature. The Gehan censored data Wilcoxon weights are found to be surprisingly efficient in a wide class of problems.},
	number = {5A},
	urldate = {2021-03-26},
	journal = {The Annals of Statistics},
	author = {Nan, Bin and Kalbfleisch, John D. and Yu, Menggang},
	year = {2009},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2351--2376},
}

@article{lee_threshold_2006,
	title = {Threshold {Regression} for {Survival} {Analysis}: {Modeling} {Event} {Times} by a {Stochastic} {Process} {Reaching} a {Boundary}},
	volume = {21},
	issn = {0883-4237},
	shorttitle = {Threshold {Regression} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/0708.0346},
	doi = {10.1214/088342306000000330},
	abstract = {Many researchers have investigated ﬁrst hitting times as models for survival data. First hitting times arise naturally in many types of stochastic processes, ranging from Wiener processes to Markov chains. In a survival context, the state of the underlying process represents the strength of an item or the health of an individual. The item fails or the individual experiences a clinical endpoint when the process reaches an adverse threshold state for the ﬁrst time. The time scale can be calendar time or some other operational measure of degradation or disease progression. In many applications, the process is latent (i.e., unobservable). Threshold regression refers to ﬁrst-hitting-time models with regression structures that accommodate covariate data. The parameters of the process, threshold state and time scale may depend on the covariates. This paper reviews aspects of this topic and discusses fruitful avenues for future research.},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Statistical Science},
	author = {Lee, Mei-Ling Ting and Whitmore, G. A.},
	month = nov,
	year = {2006},
	note = {arXiv: 0708.0346},
	keywords = {Statistics - Methodology},
}

@book{ibrahim_bayesian_2001,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Bayesian {Survival} {Analysis}},
	isbn = {978-0-387-95277-2},
	url = {https://www.springer.com/gp/book/9780387952772},
	abstract = {Survival analysis arises in many fields of study including medicine, biology, engineering, public health, epidemiology, and economics. This book provides a comprehensive treatment of Bayesian survival analysis.Several topics are addressed, including parametric models, semiparametric models based on prior processes, proportional and non-proportional hazards models, frailty models, cure rate models, model selection and comparison, joint models for longitudinal and survival data, models with time varying covariates, missing covariate data, design and monitoring of clinical trials, accelerated failure time models, models for mulitivariate survival data, and special types of hierarchial survival models. Also various censoring schemes are examined including right and interval censored data. Several additional topics are discussed, including noninformative and informative prior specificiations, computing posterior qualities of interest, Bayesian hypothesis testing, variable selection, model selection with nonnested models, model checking techniques using Bayesian diagnostic methods, and Markov chain Monte Carlo (MCMC) algorithms for sampling from the posteiror and predictive distributions.The book presents a balance between theory and applications, and for each class of models discussed, detailed examples and analyses from case studies are presented whenever possible. The applications are all essentially from the health sciences, including cancer, AIDS, and the environment. The book is intended as a graduate textbook or a reference book for a one semester course at the advanced masters or Ph.D. level. This book would be most suitable for second or third year graduate students in statistics or biostatistics. It would also serve as a useful reference book for applied or theoretical researchers as well as practitioners.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Sinha, Debajyoti},
	year = {2001},
	doi = {10.1007/978-1-4757-3447-8},
}

@book{aalen_survival_2008,
	address = {New York},
	series = {Statistics for {Biology} and {Health}},
	title = {Survival and {Event} {History} {Analysis}: {A} {Process} {Point} of {View}},
	isbn = {978-0-387-20287-7},
	shorttitle = {Survival and {Event} {History} {Analysis}},
	url = {https://www.springer.com/gp/book/9780387202877},
	abstract = {Time-to-event data are ubiquitous in fields such as medicine, biology, demography, sociology, economics and reliability theory. Recently, a need to analyze more complex event histories has emerged. Examples are individuals that move among several states, frailty that makes some units fail before others, internal time-dependent covariates, and the estimation of causal effects from observational data. The aim of this book is to bridge the gap between standard textbook models and a range of models where the dynamic structure of the data manifests itself fully. The common denominator of such models is stochastic processes. The authors show how counting processes, martingales, and stochastic integrals fit very nicely with censored data. Beginning with standard analyses such as Kaplan-Meier plots and Cox regression, the presentation progresses to the additive hazard model and recurrent event data. Stochastic processes are also used as natural models for individual frailty; they allow sensible interpretations of a number of surprising artifacts seen in population data. The stochastic process framework is naturally connected to causality. The authors show how dynamic path analyses can incorporate many modern causality ideas in a framework that takes the time aspect seriously. To make the material accessible to the reader, a large number of practical examples, mainly from medicine, are developed in detail. Stochastic processes are introduced in an intuitive and non-technical manner. The book is aimed at investigators who use event history methods and want a better understanding of the statistical concepts. It is suitable as a textbook for graduate courses in statistics and biostatistics. Odd O. Aalen is professor of medical statistics at the University of Oslo, Norway. His Ph.D. from the University of California, Berkeley in 1975 introduced counting processes and martingales in event history analysis. He has also contributed to numerous other areas of event history analysis, such as additive hazards regression, frailty, and causality through dynamic modeling. Ørnulf Borgan is professor of statistics at the University of Oslo, Norway. Since his Ph.D. in 1984 he has contributed extensively to event history analysis. He is co-author of the monograph Statistical Models Based on Counting Processes, and is editor of Scandinavian Journal of Statistics. Håkon K. Gjessing is professor of medical statistics at the Norwegian Institute of Public Health and the University of Bergen, Norway. Since his Ph.D. in probability in 1995, he has worked on a broad range of theoretical and applied problems in biostatistics.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Aalen, Odd and Borgan, Ornulf and Gjessing, Hakon},
	year = {2008},
	doi = {10.1007/978-0-387-68560-1},
}

@article{crowley_note_1974,
	title = {A {Note} on {Some} {Recent} {Likelihoods} {Leading} to the {Log} {Rank} {Test}},
	volume = {61},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334736},
	doi = {10.2307/2334736},
	abstract = {A discussion is given of the derivation by Peto (1972) of the log rank test as the locally most powerful rank invariant test against certain Lehmann-type alternatives in the two-sample problem with censored data. The random censorship model is used to show that the test with this local optimality property may depend on the `censoring mechanism, even if censoring is applied equally to both groups. An explanation is offered of Peto's likelihood and of a special case of the conditional likelihood of Cox (1972). Reference is made to asymptotic relative efficiency results which demonstrate that the log rank test may not be fully efficient when censoring is not equal.},
	number = {3},
	urldate = {2021-03-26},
	journal = {Biometrika},
	author = {Crowley, John},
	year = {1974},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {533--538},
}

@article{breslow_covariance_1974,
	title = {Covariance analysis of censored survival data},
	volume = {30},
	issn = {0006-341X},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Breslow, N.},
	month = mar,
	year = {1974},
	pmid = {4813387},
	keywords = {Humans, Models, Biological, Prognosis, Age Factors, Child, Preschool, Dactinomycin, Leukemia, Lymphoid, Leukocyte Count, Mercaptopurine, Methotrexate, Nitrogen Mustard Compounds, Regression Analysis, Remission, Spontaneous, Statistics as Topic, Time Factors},
	pages = {89--99},
}

@article{kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2281868},
	doi = {10.2307/2281868},
	abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 ≤ t$_{\textrm{1}}$' ≤ t$_{\textrm{2}}$' ≤ ⋯ ≤ t$_{\textrm{N}}$'. Then {\textless}tex-math{\textgreater}\${\textbackslash}hat\{P\}(t) = {\textbackslash}prod\_r {\textbackslash}lbrack(N - r)/(N - r + 1){\textbackslash}rbrack\${\textless}/tex-math{\textgreater}, where r assumes those values for which t$_{\textrm{r}}$' ≤ t and for which t$_{\textrm{r}}$' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
	number = {282},
	urldate = {2021-03-26},
	journal = {Journal of the American Statistical Association},
	author = {Kaplan, E. L. and Meier, Paul},
	year = {1958},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {457--481},
}

@article{kalbfleisch_non-parametric_1978,
	title = {Non-{Parametric} {Bayesian} {Analysis} of {Survival} {Time} {Data}},
	volume = {40},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984758},
	abstract = {A Bayesian analysis of the semi-parametric regression and life model of Cox (1972) is given. The cumulative hazard function is modelled as a gamma process. Both estimation of the regression parameters and of the underlying survival distribution are considered. The results are compared to the results obtained by other approaches.},
	number = {2},
	urldate = {2021-03-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Kalbfleisch, John D.},
	year = {1978},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {214--221},
}

@article{aalen_nonparametric_1978,
	title = {Nonparametric inference for a family of counting processes},
	journal = {The Annals of Statistics},
	author = {Aalen, Odd},
	year = {1978},
	note = {Publisher: JSTOR},
	pages = {701--726},
}

@article{nelson_theory_1972,
	title = {Theory and applications of hazard plotting for censored failure data},
	volume = {14},
	number = {4},
	journal = {Technometrics},
	author = {Nelson, Wayne},
	year = {1972},
	note = {Publisher: Taylor \& Francis},
	pages = {945--966},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki and {others}},
	year = {2017},
	note = {Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	pages = {5018--5051},
}

@article{arjas_marked_1984,
	title = {A marked point process approach to censored failure data with complicated covariates},
	journal = {Scandinavian Journal of Statistics},
	author = {Arjas, Elja and Haara, Pentti},
	year = {1984},
	note = {Publisher: JSTOR},
	pages = {193--209},
}

@article{arjas_filtering_1992,
	title = {Filtering the histories of a partially observed marked point process},
	volume = {40},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/030441499290013G},
	doi = {10.1016/0304-4149(92)90013-G},
	abstract = {We consider a situation in which the evolution of an ‘underlying’ marked point process is of interest, but where this process is not directly observable. Instead, we assume that another marked point process, which is fully determined by the underlying process, can be observed. The problem is then the estimation, at any given time t, of the underlying development so far, given the corresponding observations. The solution, in the sense of a conditional distribution of the underlying pre-t history, is shown to satisfy a recursive filter formula. Sufficient conditions for the uniqueness of the solution are given. Two non-trivial examples are considered in detail.},
	language = {en},
	number = {2},
	urldate = {2021-04-04},
	journal = {Stochastic Processes and their Applications},
	author = {Arjas, Elja and Haara, Pentti and Norros, Ikka},
	month = mar,
	year = {1992},
	keywords = {alternating renewal process, compensator, disruption problem, filtering, history set, marked point process},
	pages = {225--250},
}

@article{segall_heart_2014,
	title = {Heart failure in patients with chronic kidney disease: a systematic integrative review},
	volume = {2014},
	journal = {BioMed research international},
	author = {Segall, Liviu and Nistor, Ionut and Covic, Adrian},
	year = {2014},
	note = {Publisher: Hindawi},
}

@article{fine_proportional_1999,
	title = {A proportional hazards model for the subdistribution of a competing risk},
	volume = {94},
	number = {446},
	journal = {Journal of the American statistical association},
	author = {Fine, Jason P and Gray, Robert J},
	year = {1999},
	note = {Publisher: Taylor \& Francis},
	pages = {496--509},
}

@article{bramer_international_1988,
	title = {International statistical classification of diseases and related health problems. {Tenth} revision.},
	volume = {41},
	number = {1},
	journal = {World health statistics quarterly. Rapport trimestriel de statistiques sanitaires mondiales},
	author = {Brämer, Gerlind R},
	year = {1988},
	pages = {32--36},
}

@article{carey_prevention_2018,
	title = {Prevention, detection, evaluation, and management of high blood pressure in adults: synopsis of the 2017 {American} {College} of {Cardiology}/{American} {Heart} {Association} {Hypertension} {Guideline}},
	volume = {168},
	number = {5},
	journal = {Annals of internal medicine},
	author = {Carey, Robert M and Whelton, Paul K},
	year = {2018},
	note = {Publisher: American College of Physicians},
	pages = {351--358},
}

@article{sudlow_uk_2015,
	title = {{UK} biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age},
	volume = {12},
	number = {3},
	journal = {Plos med},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and {others}},
	year = {2015},
	note = {Publisher: Public Library of Science},
	pages = {e1001779},
}

@article{mahmood_framingham_2014,
	title = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease: a historical perspective},
	volume = {383},
	number = {9921},
	journal = {The lancet},
	author = {Mahmood, Syed S and Levy, Daniel and Vasan, Ramachandran S and Wang, Thomas J},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {999--1008},
}

@article{dawber_epidemiological_1951,
	title = {Epidemiological approaches to heart disease: the {Framingham} {Study}},
	volume = {41},
	number = {3},
	journal = {American Journal of Public Health and the Nations Health},
	author = {Dawber, Thomas R and Meadors, Gilcin F and Moore Jr, Felix E},
	year = {1951},
	note = {Publisher: American Public Health Association},
	pages = {279--286},
}

@article{kannel_lessons_1976,
	title = {Some lessons in cardiovascular epidemiology from {Framingham}},
	volume = {37},
	number = {2},
	journal = {The American journal of cardiology},
	author = {Kannel, William B},
	year = {1976},
	note = {Publisher: Elsevier},
	pages = {269--282},
}

@article{virani_heart_2021,
	title = {Heart disease and stroke statistics—2021 update: a report from the {American} {Heart} {Association}},
	volume = {143},
	number = {8},
	journal = {Circulation},
	author = {Virani, Salim S and Alonso, Alvaro and Aparicio, Hugo J and Benjamin, Emelia J and Bittencourt, Marcio S and Callaway, Clifton W and Carson, April P and Chamberlain, Alanna M and Cheng, Susan and Delling, Francesca N and {others}},
	year = {2021},
	note = {Publisher: Am Heart Assoc},
	pages = {e254--e743},
}

@inproceedings{yao_yes_2018,
	title = {Yes, but did it work?: {Evaluating} variational inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	year = {2018},
	pages = {5581--5590},
}

@article{gelman_bayesian_2020,
	title = {Bayesian workflow},
	journal = {arXiv preprint arXiv:2011.01808},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	year = {2020},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Publisher: Wiley Online Library},
	pages = {389--402},
}

@article{muhammad_epic-survival_2021,
	title = {{EPIC}-{Survival}: {End}-to-end {Part} {Inferred} {Clustering} for {Survival} {Analysis}, {Featuring} {Prognostic} {Stratification} {Boosting}},
	shorttitle = {{EPIC}-{Survival}},
	url = {http://arxiv.org/abs/2101.11085},
	abstract = {Histopathology-based survival modelling has two major hurdles. Firstly, a well-performing survival model has minimal clinical application if it does not contribute to the stratiﬁcation of a cancer patient cohort into different risk groups, preferably driven by histologic morphologies. In the clinical setting, individuals are not given speciﬁc prognostic predictions, but are rather predicted to lie within a risk group which has a general survival trend. Thus, It is imperative that a survival model produces well-stratiﬁed risk groups. Secondly, until now, survival modelling was done in a two-stage approach (encoding and aggregation). The massive amount of pixels in digitized whole slide images were never utilized to their fullest extent due to technological constraints on data processing, forcing decoupled learning. EPIC-Survival bridges encoding and aggregation into an end-to-end survival modelling approach, while introducing stratiﬁcation boosting to encourage the model to not only optimize ranking, but also to discriminate between risk groups. In this study we show that EPIC-Survival performs better than other approaches in modelling intrahepatic cholangiocarcinoma, a historically difﬁcult cancer to model. Further, we show that stratiﬁcation boosting improves further improves model performance, resulting in a concordance-index of 0.880 on a held-out test set. Finally, we were able to identify speciﬁc histologic differences, not commonly sought out in ICC, between low and high risk groups.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2101.11085 [cs]},
	author = {Muhammad, Hassan and Xie, Chensu and Sigel, Carlie S. and Doukas, Michael and Alpert, Lindsay and Fuchs, Thomas J.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.11085},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{allen_nonalcoholic_2018,
	title = {Nonalcoholic fatty liver disease incidence and impact on metabolic burden and death: a 20 year-community study},
	volume = {67},
	number = {5},
	journal = {Hepatology},
	author = {Allen, Alina M and Therneau, Terry M and Larson, Joseph J and Coward, Alexandra and Somers, Virend K and Kamath, Patrick S},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1726--1736},
}

@article{crowley_covariance_1977,
	title = {Covariance analysis of heart transplant survival data},
	volume = {72},
	number = {357},
	journal = {Journal of the American Statistical Association},
	author = {Crowley, John and Hu, Marie},
	year = {1977},
	note = {Publisher: Taylor \& Francis},
	pages = {27--36},
}

@article{loprinzi_prospective_1994,
	title = {Prospective evaluation of prognostic variables from patient-completed questionnaires. {North} {Central} {Cancer} {Treatment} {Group}.},
	volume = {12},
	number = {3},
	journal = {Journal of Clinical Oncology},
	author = {Loprinzi, Charles Lawrence and Laurie, John A and Wieand, H Sam and Krook, James E and Novotny, Paul J and Kugler, John W and Bartel, Joan and Law, Marlys and Bateman, Marilyn and Klatt, Nancy E},
	year = {1994},
	pages = {601--607},
}

@article{laurie_surgical_1989,
	title = {Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. {The} {North} {Central} {Cancer} {Treatment} {Group} and the {Mayo} {Clinic}.},
	volume = {7},
	number = {10},
	journal = {Journal of Clinical Oncology},
	author = {Laurie, John A and Moertel, Charles G and Fleming, Thomas R and Wieand, Harry S and Leigh, John E and Rubin, Jebal and McCormack, Greg W and Gerstner, James B and Krook, James E and Malliard, James},
	year = {1989},
	pages = {1447--1456},
}

@article{mohamed_monte_2019,
	title = {Monte carlo gradient estimation in machine learning},
	journal = {arXiv preprint arXiv:1906.10652},
	author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
	year = {2019},
}

@book{wainwright_graphical_2008,
	title = {Graphical models, exponential families, and variational inference},
	publisher = {Now Publishers Inc},
	author = {Wainwright, Martin J and Jordan, Michael Irwin},
	year = {2008},
}

@inproceedings{ranganath_black_2014,
	title = {Black box variational inference},
	booktitle = {Artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
	year = {2014},
	pages = {814--822},
}

@book{klein_survival_2006,
	title = {Survival analysis: techniques for censored and truncated data},
	publisher = {Springer Science \& Business Media},
	author = {Klein, John P and Moeschberger, Melvin L},
	year = {2006},
}

@inproceedings{ranganath_deep_2016,
	title = {Deep survival analysis},
	booktitle = {Machine {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Perotte, Adler and Elhadad, Noémie and Blei, David},
	year = {2016},
	pages = {101--114},
}

@article{brilleman_bayesian_2020,
	title = {Bayesian {Survival} {Analysis} {Using} the rstanarm {R} {Package}},
	journal = {arXiv preprint arXiv:2002.09633},
	author = {Brilleman, Samuel L and Elci, Eren M and Novik, Jacqueline Buros and Wolfe, Rory},
	year = {2020},
}

@article{schoenfeld_partial_1982,
	title = {Partial residuals for the proportional hazards regression model},
	volume = {69},
	number = {1},
	journal = {Biometrika},
	author = {Schoenfeld, David},
	year = {1982},
	note = {Publisher: Oxford University Press},
	pages = {239--241},
}

@article{egeberg_assessment_2016,
	title = {Assessment of the risk of cardiovascular disease in patients with rosacea},
	volume = {75},
	number = {2},
	journal = {Journal of the American Academy of Dermatology},
	author = {Egeberg, Alexander and Hansen, Peter R and Gislason, Gunnar H and Thyssen, Jacob P},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {336--339},
}

@misc{sas_sas_nodate,
	title = {{SAS} {Help} {Center}: {Specifics} for {Bayesian} {Analysis}},
	url = {https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_phreg_details82.htm#statug_phreg020296},
	urldate = {2021-05-14},
	author = {SAS},
}

@misc{tay_regularized_2021,
	title = {Regularized {Cox} {Regression}},
	url = {https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf},
	language = {en},
	urldate = {2021-05-01},
	author = {Tay, Kenneth and Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob and Narasimhan, Balasubramanian},
	year = {2021},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	url = {https://www.jstatsoft.org/v33/i01/},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	year = {2010},
	pages = {1--22},
}

@incollection{therneau_cox_2000,
	title = {The cox model},
	booktitle = {Modeling survival data: extending the {Cox} model},
	publisher = {Springer},
	author = {Therneau, Terry M and Grambsch, Patricia M},
	year = {2000},
	pages = {39--77},
}

@article{shin_scalable_2018,
	title = {Scalable {Bayesian} variable selection using nonlocal prior densities in ultrahigh-dimensional settings},
	volume = {28},
	number = {2},
	journal = {Statistica Sinica},
	author = {Shin, Minsuk and Bhattacharya, Anirban and Johnson, Valen E},
	year = {2018},
	note = {Publisher: NIH Public Access},
	pages = {1053},
}

@article{ibrahim_bayesian_1999,
	title = {Bayesian variable selection for proportional hazards models},
	volume = {27},
	number = {4},
	journal = {Canadian Journal of Statistics},
	author = {Ibrahim, Joseph G and Chen, Ming-Hui and MacEachern, Steven N},
	year = {1999},
	note = {Publisher: Wiley Online Library},
	pages = {701--717},
}

@article{nikooienejad_bayesian_2020,
	title = {Bayesian variable selection for survival data using inverse moment priors},
	volume = {14},
	number = {2},
	journal = {The annals of applied statistics},
	author = {Nikooienejad, Amir and Wang, Wenyi and Johnson, Valen E},
	year = {2020},
	note = {Publisher: NIH Public Access},
	pages = {809},
}

@article{hippisley-cox_predicting_2021,
	title = {Predicting the risk of prostate cancer in asymptomatic men: a cohort study to develop and validate a novel algorithm},
	volume = {71},
	number = {706},
	journal = {British Journal of General Practice},
	author = {Hippisley-Cox, Julia and Coupland, Carol},
	year = {2021},
	note = {Publisher: British Journal of General Practice},
	pages = {e364--e371},
}

@article{hippisley-cox_qresearch_2004,
	title = {{QRESEARCH}: a new general practice database for research},
	volume = {12},
	number = {1},
	journal = {Journal of Innovation in Health Informatics},
	author = {Hippisley-Cox, Julia and Stables, David and Pringle, Mike},
	year = {2004},
	pages = {49--50},
}

@article{hippisley-cox_development_2017,
	title = {Development and validation of {QRISK3} risk prediction algorithms to estimate future risk of cardiovascular disease: prospective cohort study},
	volume = {357},
	journal = {bmj},
	author = {Hippisley-Cox, Julia and Coupland, Carol and Brindle, Peter},
	year = {2017},
	note = {Publisher: British Medical Journal Publishing Group},
}

@article{clift_living_2020,
	title = {Living risk prediction algorithm ({QCOVID}) for risk of hospital admission and mortality from coronavirus 19 in adults: national derivation and validation cohort study},
	volume = {371},
	journal = {bmj},
	author = {Clift, Ash K and Coupland, Carol AC and Keogh, Ruth H and Diaz-Ordaz, Karla and Williamson, Elizabeth and Harrison, Ewen M and Hayward, Andrew and Hemingway, Harry and Horby, Peter and Mehta, Nisha and {others}},
	year = {2020},
	note = {Publisher: British Medical Journal Publishing Group},
}

@article{sund_quality_2012,
	title = {Quality of the {Finnish} {Hospital} {Discharge} {Register}: a systematic review},
	volume = {40},
	number = {6},
	journal = {Scandinavian journal of public health},
	author = {Sund, Reijo},
	year = {2012},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {505--515},
}

@article{schmidt_danish_2015,
	title = {The {Danish} {National} {Patient} {Registry}: a review of content, data quality, and research potential},
	volume = {7},
	journal = {Clinical epidemiology},
	author = {Schmidt, Morten and Schmidt, Sigrun Alba Johannesdottir and Sandegaard, Jakob Lynge and Ehrenstein, Vera and Pedersen, Lars and Sørensen, Henrik Toft},
	year = {2015},
	note = {Publisher: Dove Press},
	pages = {449},
}

@article{leitsalu_cohort_2015,
	title = {Cohort profile: {Estonian} biobank of the {Estonian} genome center, university of {Tartu}},
	volume = {44},
	number = {4},
	journal = {International journal of epidemiology},
	author = {Leitsalu, Liis and Haller, Toomas and Esko, Tõnu and Tammesoo, Mari-Liis and Alavere, Helene and Snieder, Harold and Perola, Markus and Ng, Pauline C and Mägi, Reedik and Milani, Lili and {others}},
	year = {2015},
	note = {Publisher: Oxford University Press},
	pages = {1137--1147},
}

@article{chen_china_2011,
	title = {China {Kadoorie} {Biobank} of 0.5 million people: survey methods, baseline characteristics and long-term follow-up},
	volume = {40},
	number = {6},
	journal = {International journal of epidemiology},
	author = {Chen, Zhengming and Chen, Junshi and Collins, Rory and Guo, Yu and Peto, Richard and Wu, Fan and Li, Liming},
	year = {2011},
	note = {Publisher: Oxford University Press},
	pages = {1652--1666},
}

@article{austin_review_2020,
	title = {A review of the use of time-varying covariates in the {Fine}-{Gray} subdistribution hazard competing risk regression model},
	volume = {39},
	number = {2},
	journal = {Statistics in medicine},
	author = {Austin, Peter C and Latouche, Aurélien and Fine, Jason P},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {103--113},
}

@article{royston_use_1999,
	title = {The use of fractional polynomials to model continuous risk variables in epidemiology.},
	volume = {28},
	number = {5},
	journal = {International journal of epidemiology},
	author = {Royston, Patrick and Ambler, Gareth and Sauerbrei, Willi},
	year = {1999},
	pages = {964--974},
}

@article{sleeper_regression_1990,
	title = {Regression splines in the {Cox} model with application to covariate effects in liver disease},
	volume = {85},
	number = {412},
	journal = {Journal of the American Statistical Association},
	author = {Sleeper, Lynn A and Harrington, David P},
	year = {1990},
	note = {Publisher: Taylor \& Francis},
	pages = {941--949},
}

@article{cox_general_1968,
	title = {A general definition of residuals},
	volume = {30},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Cox, David R and Snell, E Joyce},
	year = {1968},
	note = {Publisher: Wiley Online Library},
	pages = {248--265},
}

@article{halabi_score_2020,
	title = {Score and deviance residuals based on the full likelihood approach in survival analysis},
	volume = {19},
	number = {6},
	journal = {Pharmaceutical statistics},
	author = {Halabi, Susan and Dutta, Sandipan and Wu, Yuan and Liu, Aiyi},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {940--954},
}

@article{davison_deviance_1989,
	title = {Deviance residuals and normal scores plots},
	volume = {76},
	number = {2},
	journal = {Biometrika},
	author = {Davison, AC and Gigli and {A}},
	year = {1989},
	note = {Publisher: Oxford University Press},
	pages = {211--221},
}

@article{therneau_martingale-based_1990,
	title = {Martingale-based residuals for survival models},
	volume = {77},
	number = {1},
	journal = {Biometrika},
	author = {Therneau, Terry M and Grambsch, Patricia M and Fleming, Thomas R},
	year = {1990},
	note = {Publisher: Oxford University Press},
	pages = {147--160},
}

@article{lin_exponential-family_2021,
	title = {Exponential-{Family} {Embedding} {With} {Application} to {Cell} {Developmental} {Trajectories} for {Single}-{Cell} {RNA}-{Seq} {Data}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1886106},
	doi = {10.1080/01621459.2021.1886106},
	abstract = {Scientists often embed cells into a lower-dimensional space when studying single-cell RNA-seq data for improved downstream analyses such as developmental trajectory analyses, but the statistical properties of such nonlinear embedding methods are often not well understood. In this article, we develop the exponential-family SVD (eSVD), a nonlinear embedding method for both cells and genes jointly with respect to a random dot product model using exponential-family distributions. Our estimator uses alternating minimization, which enables us to have a computationally efficient method, prove the identifiability conditions and consistency of our method, and provide statistically principled procedures to tune our method. All these qualities help advance the single-cell embedding literature, and we provide extensive simulations to demonstrate that the eSVD is competitive compared to other embedding methods. We apply the eSVD via Gaussian distributions where the standard deviations are proportional to the means to analyze a single-cell dataset of oligodendrocytes in mouse brains. Using the eSVD estimated embedding, we then investigate the cell developmental trajectories of the oligodendrocytes. While previous results are not able to distinguish the trajectories among the mature oligodendrocyte cell types, our diagnostics and results demonstrate there are two major developmental trajectories that diverge at mature oligodendrocytes. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplementary materials.},
	language = {en},
	number = {534},
	urldate = {2021-06-12},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Kevin Z. and Lei, Jing and Roeder, Kathryn},
	month = apr,
	year = {2021},
	pages = {457--470},
	file = {Lin et al. - 2021 - Exponential-Family Embedding With Application to C.pdf:/Users/alexwjung/Documents/library/storage/HF5HZ5HC/Lin et al. - 2021 - Exponential-Family Embedding With Application to C.pdf:application/pdf},
}

@article{sinha_bayesian_2003,
	title = {A {Bayesian} {Justification} of {Cox}'s {Partial} {Likelihood}},
	volume = {90},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/30042071},
	abstract = {In this paper, we establish both naive and formal Bayesian justifications of Cox's (1975) partial likelihood and its various modifications. We extend the original work of Kalbfieisch (1978), who showed that the partial likelihood is a limiting marginal posterior under noninformative priors for baseline hazards. We extend the result to scenarios with time-dependent covariates and time-varying regression parameters. We establish results for continuous time as well as grouped survival data. In addition, we present a Bayesian justification of a modified partial likelihood for handling ties. We also present tools for simplification of the Gibbs sampling algorithm for implementing partial likelihood based Bayesian inference in various practical applications.},
	number = {3},
	urldate = {2021-10-25},
	journal = {Biometrika},
	author = {Sinha, Debajyoti and Ibrahim, Joseph G. and Chen, Ming-Hui},
	year = {2003},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {629--641},
}

@article{us_preventive_services_task_force_screening_2019,
	title = {Screening for {Pancreatic} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Reaffirmation} {Recommendation} {Statement}},
	volume = {322},
	issn = {0098-7484},
	shorttitle = {Screening for {Pancreatic} {Cancer}},
	url = {https://doi.org/10.1001/jama.2019.10232},
	doi = {10.1001/jama.2019.10232},
	abstract = {Pancreatic cancer is an uncommon cancer with an age-adjusted annual incidence of 12.9 cases per 100 000 person-years. However, the death rate is 11.0 deaths per 100 000 person-years because the prognosis of pancreatic cancer is poor. Although its incidence is low, pancreatic cancer is the third most common cause of cancer death in the United States. Because of the increasing incidence of pancreatic cancer, along with improvements in early detection and treatment of other types of cancer, it is estimated that pancreatic cancer may soon become the second-leading cause of cancer death in the United States.To update the 2004 US Preventive Services Task Force (USPSTF) recommendation on screening for pancreatic cancer.The USPSTF reviewed the evidence on the benefits and harms of screening for pancreatic cancer, the diagnostic accuracy of screening tests for pancreatic cancer, and the benefits and harms of treatment of screen-detected or asymptomatic pancreatic cancer.The USPSTF found no evidence that screening for pancreatic cancer or treatment of screen-detected pancreatic cancer improves disease-specific morbidity or mortality, or all-cause mortality. The USPSTF found adequate evidence that the magnitude of the benefits of screening for pancreatic cancer in asymptomatic adults can be bounded as no greater than small. The USPSTF found adequate evidence that the magnitude of the harms of screening for pancreatic cancer and treatment of screen-detected pancreatic cancer can be bounded as at least moderate. The USPSTF reaffirms its previous conclusion that the potential benefits of screening for pancreatic cancer in asymptomatic adults do not outweigh the potential harms.The USPSTF recommends against screening for pancreatic cancer in asymptomatic adults. (D recommendation)},
	number = {5},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = aug,
	year = {2019},
	pages = {438--444},
}

@article{us_preventive_services_task_force_screening_2021,
	title = {Screening for {Lung} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {325},
	issn = {0098-7484},
	shorttitle = {Screening for {Lung} {Cancer}},
	url = {https://doi.org/10.1001/jama.2021.1117},
	doi = {10.1001/jama.2021.1117},
	abstract = {Lung cancer is the second most common cancer and the leading cause of cancer death in the US. In 2020, an estimated 228 820 persons were diagnosed with lung cancer, and 135 720 persons died of the disease. The most important risk factor for lung cancer is smoking. Increasing age is also a risk factor for lung cancer. Lung cancer has a generally poor prognosis, with an overall 5-year survival rate of 20.5\%. However, early-stage lung cancer has a better prognosis and is more amenable to treatment.To update its 2013 recommendation, the US Preventive Services Task Force (USPSTF) commissioned a systematic review on the accuracy of screening for lung cancer with low-dose computed tomography (LDCT) and on the benefits and harms of screening for lung cancer and commissioned a collaborative modeling study to provide information about the optimum age at which to begin and end screening, the optimal screening interval, and the relative benefits and harms of different screening strategies compared with modified versions of multivariate risk prediction models.This recommendation statement applies to adults aged 50 to 80 years who have a 20 pack-year smoking history and currently smoke or have quit within the past 15 years.The USPSTF concludes with moderate certainty that annual screening for lung cancer with LDCT has a moderate net benefit in persons at high risk of lung cancer based on age, total cumulative exposure to tobacco smoke, and years since quitting smoking.The USPSTF recommends annual screening for lung cancer with LDCT in adults aged 50 to 80 years who have a 20 pack-year smoking history and currently smoke or have quit within the past 15 years. Screening should be discontinued once a person has not smoked for 15 years or develops a health problem that substantially limits life expectancy or the ability or willingness to have curative lung surgery. (B recommendation) This recommendation replaces the 2013 USPSTF statement that recommended annual screening for lung cancer with LDCT in adults aged 55 to 80 years who have a 30 pack-year smoking history and currently smoke or have quit within the past 15 years.},
	number = {10},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = mar,
	year = {2021},
	pages = {962--970},
}

@article{us_preventive_services_task_force_screening_2009,
	title = {Screening for breast cancer: {U}.{S}. {Preventive} {Services} {Task} {Force} recommendation statement},
	volume = {151},
	issn = {1539-3704},
	shorttitle = {Screening for breast cancer},
	doi = {10.7326/0003-4819-151-10-200911170-00008},
	abstract = {DESCRIPTION: Update of the 2002 U.S. Preventive Services Task Force (USPSTF) recommendation statement on screening for breast cancer in the general population.
METHODS: The USPSTF examined the evidence on the efficacy of 5 screening modalities in reducing mortality from breast cancer: film mammography, clinical breast examination, breast self-examination, digital mammography, and magnetic resonance imaging in order to update the 2002 recommendation. To accomplish this update, the USPSTF commissioned 2 studies: 1) a targeted systematic evidence review of 6 selected questions relating to benefits and harms of screening, and 2) a decision analysis that used population modeling techniques to compare the expected health outcomes and resource requirements of starting and ending mammography screening at different ages and using annual versus biennial screening intervals.
RECOMMENDATIONS: The USPSTF recommends against routine screening mammography in women aged 40 to 49 years. The decision to start regular, biennial screening mammography before the age of 50 years should be an individual one and take into account patient context, including the patient's values regarding specific benefits and harms. (Grade C recommendation) The USPSTF recommends biennial screening mammography for women between the ages of 50 and 74 years. (Grade B recommendation) The USPSTF concludes that the current evidence is insufficient to assess the additional benefits and harms of screening mammography in women 75 years or older. (I statement) The USPSTF concludes that the current evidence is insufficient to assess the additional benefits and harms of clinical breast examination beyond screening mammography in women 40 years or older. (I statement) The USPSTF recommends against clinicians teaching women how to perform breast self-examination. (Grade D recommendation) The USPSTF concludes that the current evidence is insufficient to assess additional benefits and harms of either digital mammography or magnetic resonance imaging instead of film mammography as screening modalities for breast cancer. (I statement).},
	language = {eng},
	number = {10},
	journal = {Annals of Internal Medicine},
	author = {{US Preventive Services Task Force}},
	month = nov,
	year = {2009},
	pmid = {19920272},
	keywords = {Humans, Breast Neoplasms, Female, Age Factors, Adult, Aged, Anxiety, Breast Self-Examination, Early Detection of Cancer, False Positive Reactions, Health Care Costs, Magnetic Resonance Imaging, Mammography, Mass Screening, Middle Aged, Palpation, Risk Factors},
	pages = {716--726, W--236},
}

@article{us_preventive_services_task_force_screening_2021-1,
	title = {Screening for {Colorectal} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {325},
	issn = {0098-7484},
	shorttitle = {Screening for {Colorectal} {Cancer}},
	url = {https://doi.org/10.1001/jama.2021.6238},
	doi = {10.1001/jama.2021.6238},
	abstract = {Colorectal cancer is the third leading cause of cancer death for both men and women, with an estimated 52 980 persons in the US projected to die of colorectal cancer in 2021. Colorectal cancer is most frequently diagnosed among persons aged 65 to 74 years. It is estimated that 10.5\% of new colorectal cancer cases occur in persons younger than 50 years. Incidence of colorectal cancer (specifically adenocarcinoma) in adults aged 40 to 49 years has increased by almost 15\% from 2000-2002 to 2014-2016. In 2016, 26\% of eligible adults in the US had never been screened for colorectal cancer and in 2018, 31\% were not up to date with screening.To update its 2016 recommendation, the US Preventive Services Task Force (USPSTF) commissioned a systematic review to evaluate the benefits and harms of screening for colorectal cancer in adults 40 years or older. The review also examined whether these findings varied by age, sex, or race/ethnicity. In addition, as in 2016, the USPSTF commissioned a report from the Cancer Intervention and Surveillance Modeling Network Colorectal Cancer Working Group to provide information from comparative modeling on how estimated life-years gained, colorectal cancer cases averted, and colorectal cancer deaths averted vary by different starting and stopping ages for various screening strategies.Asymptomatic adults 45 years or older at average risk of colorectal cancer (ie, no prior diagnosis of colorectal cancer, adenomatous polyps, or inflammatory bowel disease; no personal diagnosis or family history of known genetic disorders that predispose them to a high lifetime risk of colorectal cancer [such as Lynch syndrome or familial adenomatous polyposis]).The USPSTF concludes with high certainty that screening for colorectal cancer in adults aged 50 to 75 years has substantial net benefit. The USPSTF concludes with moderate certainty that screening for colorectal cancer in adults aged 45 to 49 years has moderate net benefit. The USPSTF concludes with moderate certainty that screening for colorectal cancer in adults aged 76 to 85 years who have been previously screened has small net benefit. Adults who have never been screened for colorectal cancer are more likely to benefit.The USPSTF recommends screening for colorectal cancer in all adults aged 50 to 75 years. (A recommendation) The USPSTF recommends screening for colorectal cancer in adults aged 45 to 49 years. (B recommendation) The USPSTF recommends that clinicians selectively offer screening for colorectal cancer in adults aged 76 to 85 years. Evidence indicates that the net benefit of screening all persons in this age group is small. In determining whether this service is appropriate in individual cases, patients and clinicians should consider the patient’s overall health, prior screening history, and preferences. (C recommendation)},
	number = {19},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = may,
	year = {2021},
	pages = {1965--1977},
}

@article{us_preventive_services_task_force_screening_2018,
	title = {Screening for {Cervical} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {320},
	issn = {0098-7484},
	shorttitle = {Screening for {Cervical} {Cancer}},
	url = {https://doi.org/10.1001/jama.2018.10897},
	doi = {10.1001/jama.2018.10897},
	abstract = {The number of deaths from cervical cancer in the United States has decreased substantially since the implementation of widespread cervical cancer screening and has declined from 2.8 to 2.3 deaths per 100 000 women from 2000 to 2015.To update the US Preventive Services Task Force (USPSTF) 2012 recommendation on screening for cervical cancer.The USPSTF reviewed the evidence on screening for cervical cancer, with a focus on clinical trials and cohort studies that evaluated screening with high-risk human papillomavirus (hrHPV) testing alone or hrHPV and cytology together (cotesting) compared with cervical cytology alone. The USPSTF also commissioned a decision analysis model to evaluate the age at which to begin and end screening, the optimal interval for screening, the effectiveness of different screening strategies, and related benefits and harms of different screening strategies.Screening with cervical cytology alone, primary hrHPV testing alone, or cotesting can detect high-grade precancerous cervical lesions and cervical cancer. Screening women aged 21 to 65 years substantially reduces cervical cancer incidence and mortality. The harms of screening for cervical cancer in women aged 30 to 65 years are moderate. The USPSTF concludes with high certainty that the benefits of screening every 3 years with cytology alone in women aged 21 to 29 years substantially outweigh the harms. The USPSTF concludes with high certainty that the benefits of screening every 3 years with cytology alone, every 5 years with hrHPV testing alone, or every 5 years with both tests (cotesting) in women aged 30 to 65 years outweigh the harms. Screening women older than 65 years who have had adequate prior screening and women younger than 21 years does not provide significant benefit. Screening women who have had a hysterectomy with removal of the cervix for indications other than a high-grade precancerous lesion or cervical cancer provides no benefit. The USPSTF concludes with moderate to high certainty that screening women older than 65 years who have had adequate prior screening and are not otherwise at high risk for cervical cancer, screening women younger than 21 years, and screening women who have had a hysterectomy with removal of the cervix for indications other than a high-grade precancerous lesion or cervical cancer does not result in a positive net benefit.The USPSTF recommends screening for cervical cancer every 3 years with cervical cytology alone in women aged 21 to 29 years. (A recommendation) The USPSTF recommends screening every 3 years with cervical cytology alone, every 5 years with hrHPV testing alone, or every 5 years with hrHPV testing in combination with cytology (cotesting) in women aged 30 to 65 years. (A recommendation) The USPSTF recommends against screening for cervical cancer in women younger than 21 years. (D recommendation) The USPSTF recommends against screening for cervical cancer in women older than 65 years who have had adequate prior screening and are not otherwise at high risk for cervical cancer. (D recommendation) The USPSTF recommends against screening for cervical cancer in women who have had a hysterectomy with removal of the cervix and do not have a history of a high-grade precancerous lesion or cervical cancer. (D recommendation)},
	number = {7},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = aug,
	year = {2018},
	pages = {674--686},
}

@article{us_preventive_services_task_force_screening_2018-1,
	title = {Screening for {Ovarian} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {319},
	issn = {0098-7484},
	shorttitle = {Screening for {Ovarian} {Cancer}},
	url = {https://doi.org/10.1001/jama.2017.21926},
	doi = {10.1001/jama.2017.21926},
	abstract = {With approximately 14 000 deaths per year, ovarian cancer is the fifth most common cause of cancer death among US women and the leading cause of death from gynecologic cancer. More than 95\% of ovarian cancer deaths occur among women 45 years and older.To update the 2012 US Preventive Services Task Force (USPSTF) recommendation on screening for ovarian cancer.The USPSTF reviewed the evidence on the benefits and harms of screening for ovarian cancer in asymptomatic women not known to be at high risk for ovarian cancer (ie, high risk includes women with certain hereditary cancer syndromes that increase their risk for ovarian cancer). Outcomes of interest included ovarian cancer mortality, quality of life, false-positive rate, surgery and surgical complication rates, and psychological effects of screening.The USPSTF found adequate evidence that screening for ovarian cancer does not reduce ovarian cancer mortality. The USPSTF found adequate evidence that the harms from screening for ovarian cancer are at least moderate and may be substantial in some cases, and include unnecessary surgery for women who do not have cancer. Given the lack of mortality benefit of screening, and the moderate to substantial harms that could result from false-positive screening test results and subsequent surgery, the USPSTF concludes with moderate certainty that the harms of screening for ovarian cancer outweigh the benefit, and the net balance of the benefit and harms of screening is negative.The USPSTF recommends against screening for ovarian cancer in asymptomatic women. (D recommendation) This recommendation applies to asymptomatic women who are not known to have a high-risk hereditary cancer syndrome.},
	number = {6},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = feb,
	year = {2018},
	pages = {588--594},
}

@article{us_preventive_services_task_force_screening_2016,
	title = {Screening for {Skin} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {316},
	issn = {0098-7484},
	shorttitle = {Screening for {Skin} {Cancer}},
	url = {https://doi.org/10.1001/jama.2016.8465},
	doi = {10.1001/jama.2016.8465},
	abstract = {Basal and squamous cell carcinoma are the most common types of cancer in the United States and represent the vast majority of all cases of skin cancer; however, they rarely result in death or substantial morbidity, whereas melanoma skin cancer has notably higher mortality rates. In 2016, an estimated 76 400 US men and women will develop melanoma and 10 100 will die from the disease.To update the 2009 US Preventive Services Task Force (USPSTF) recommendation on screening for skin cancer.The USPSTF reviewed the evidence on the effectiveness of screening for skin cancer with a clinical visual skin examination in reducing skin cancer morbidity and mortality and death from any cause; its potential harms, including any harms resulting from associated diagnostic follow-up; its test characteristics when performed by a primary care clinician vs a dermatologist; and whether its use leads to earlier detection of skin cancer compared with usual care.Evidence to assess the net benefit of screening for skin cancer with a clinical visual skin examination is limited. Direct evidence on the effectiveness of screening in reducing melanoma morbidity and mortality is limited to a single fair-quality ecologic study with important methodological limitations. Information on harms is similarly sparse. The potential for harm clearly exists, including a high rate of unnecessary biopsies, possibly resulting in cosmetic or, more rarely, functional adverse effects, and the risk of overdiagnosis and overtreatment.The USPSTF concludes that the current evidence is insufficient to assess the balance of benefits and harms of visual skin examination by a clinician to screen for skin cancer in adults (I statement).},
	number = {4},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = jul,
	year = {2016},
	pages = {429--435},
}

@article{us_preventive_services_task_force_screening_2018-2,
	title = {Screening for {Prostate} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {319},
	issn = {0098-7484},
	shorttitle = {Screening for {Prostate} {Cancer}},
	url = {https://doi.org/10.1001/jama.2018.3710},
	doi = {10.1001/jama.2018.3710},
	abstract = {In the United States, the lifetime risk of being diagnosed with prostate cancer is approximately 11\%, and the lifetime risk of dying of prostate cancer is 2.5\%. The median age of death from prostate cancer is 80 years. Many men with prostate cancer never experience symptoms and, without screening, would never know they have the disease. African American men and men with a family history of prostate cancer have an increased risk of prostate cancer compared with other men.To update the 2012 US Preventive Services Task Force (USPSTF) recommendation on prostate-specific antigen (PSA)–based screening for prostate cancer.The USPSTF reviewed the evidence on the benefits and harms of PSA-based screening for prostate cancer and subsequent treatment of screen-detected prostate cancer. The USPSTF also commissioned a review of existing decision analysis models and the overdiagnosis rate of PSA-based screening. The reviews also examined the benefits and harms of PSA-based screening in patient subpopulations at higher risk of prostate cancer, including older men, African American men, and men with a family history of prostate cancer.Adequate evidence from randomized clinical trials shows that PSA-based screening programs in men aged 55 to 69 years may prevent approximately 1.3 deaths from prostate cancer over approximately 13 years per 1000 men screened. Screening programs may also prevent approximately 3 cases of metastatic prostate cancer per 1000 men screened. Potential harms of screening include frequent false-positive results and psychological harms. Harms of prostate cancer treatment include erectile dysfunction, urinary incontinence, and bowel symptoms. About 1 in 5 men who undergo radical prostatectomy develop long-term urinary incontinence, and 2 in 3 men will experience long-term erectile dysfunction. Adequate evidence shows that the harms of screening in men older than 70 years are at least moderate and greater than in younger men because of increased risk of false-positive results, diagnostic harms from biopsies, and harms from treatment. The USPSTF concludes with moderate certainty that the net benefit of PSA-based screening for prostate cancer in men aged 55 to 69 years is small for some men. How each man weighs specific benefits and harms will determine whether the overall net benefit is small. The USPSTF concludes with moderate certainty that the potential benefits of PSA-based screening for prostate cancer in men 70 years and older do not outweigh the expected harms.For men aged 55 to 69 years, the decision to undergo periodic PSA-based screening for prostate cancer should be an individual one and should include discussion of the potential benefits and harms of screening with their clinician. Screening offers a small potential benefit of reducing the chance of death from prostate cancer in some men. However, many men will experience potential harms of screening, including false-positive results that require additional testing and possible prostate biopsy; overdiagnosis and overtreatment; and treatment complications, such as incontinence and erectile dysfunction. In determining whether this service is appropriate in individual cases, patients and clinicians should consider the balance of benefits and harms on the basis of family history, race/ethnicity, comorbid medical conditions, patient values about the benefits and harms of screening and treatment-specific outcomes, and other health needs. Clinicians should not screen men who do not express a preference for screening. (C recommendation) The USPSTF recommends against PSA-based screening for prostate cancer in men 70 years and older. (D recommendation)},
	number = {18},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = may,
	year = {2018},
	pages = {1901--1913},
}

@article{us_preventive_services_task_force_screening_2017,
	title = {Screening for {Thyroid} {Cancer}: {US} {Preventive} {Services} {Task} {Force} {Recommendation} {Statement}},
	volume = {317},
	issn = {0098-7484},
	shorttitle = {Screening for {Thyroid} {Cancer}},
	url = {https://doi.org/10.1001/jama.2017.4011},
	doi = {10.1001/jama.2017.4011},
	abstract = {The incidence of thyroid cancer detection has increased by 4.5\% per year over the last 10 years, faster than for any other cancer, but without a corresponding change in the mortality rate. In 2013, the incidence rate of thyroid cancer in the United States was 15.3 cases per 100 000 persons. Most cases of thyroid cancer have a good prognosis; the 5-year survival rate for thyroid cancer overall is 98.1\%.To update the US Preventive Services Task Force (USPSTF) recommendation on screening for thyroid cancer.The USPSTF reviewed the evidence on the benefits and harms of screening for thyroid cancer in asymptomatic adults, the diagnostic accuracy of screening (including neck palpation and ultrasound), and the benefits and harms of treatment of screen-detected thyroid cancer.The USPSTF found inadequate direct evidence on the benefits of screening but determined that the magnitude of the overall benefits of screening and treatment can be bounded as no greater than small, given the relative rarity of thyroid cancer, the apparent lack of difference in outcomes between patients who are treated vs monitored (for the most common tumor types), and observational evidence showing no change in mortality over time after introduction of a mass screening program. The USPSTF found inadequate direct evidence on the harms of screening but determined that the overall magnitude of the harms of screening and treatment can be bounded as at least moderate, given adequate evidence of harms of treatment and indirect evidence that overdiagnosis and overtreatment are likely to be substantial with population-based screening. The USPSTF therefore determined that the net benefit of screening for thyroid cancer is negative.The USPSTF recommends against screening for thyroid cancer in asymptomatic adults. (D recommendation)},
	number = {18},
	urldate = {2021-11-04},
	journal = {JAMA},
	author = {{US Preventive Services Task Force}},
	month = may,
	year = {2017},
	pages = {1882--1887},
}

@article{chou_screening_2010,
	title = {Screening adults for bladder cancer: a review of the evidence for the {U}.{S}. preventive services task force},
	volume = {153},
	issn = {1539-3704},
	shorttitle = {Screening adults for bladder cancer},
	doi = {10.7326/0003-4819-153-7-201010050-00009},
	abstract = {BACKGROUND: Bladder cancer is 1 of the 10 most frequently diagnosed types of cancer. Screening could identify high-grade bladder cancer at earlier stages, when it may be more easily and effectively treated.
PURPOSE: To update the 2004 U.S. Preventive Services Task Force evidence review on screening for bladder cancer in adults in primary care settings.
DATA SOURCES: MEDLINE (2002 to December 2009), the Cochrane Database of Systematic Reviews, the Cochrane Central Register of Controlled Trials (through the fourth quarter of 2009), and the CancerLit subsection of PubMed (through March 2010) were searched for studies published in English.
STUDY SELECTION: Randomized trials and controlled observational studies that directly evaluated screening for bladder cancer in adults, studies on the diagnostic accuracy of screening tests for bladder cancer, and randomized trials and controlled observational studies on clinical outcomes associated with treatment compared with no treatment of screen-detected or superficial bladder cancer.
DATA EXTRACTION: Details were abstracted about the patient sample, study design, data analysis, follow-up, and results. Quality was assessed by using methods developed by the U.S. Preventive Services Task Force.
DATA SYNTHESIS: No randomized trials or high-quality controlled observational studies evaluated clinical outcomes associated with screening compared with no screening or treatment of screen-detected bladder cancer compared with no treatment. No study evaluated the sensitivity or specificity of tests for hematuria, urinary cytology, or other urinary biomarkers for bladder cancer in asymptomatic persons without a history of bladder cancer. The positive predictive value of screening is less than 10\% in asymptomatic persons, including higher-risk populations. No study evaluated harms associated with treatment of screen-detected bladder cancer compared with no treatment.
LIMITATION: High-quality evidence was not available for any of the key questions.
CONCLUSION: Additional research is needed to determine whether screening of adults for bladder cancer leads to better outcomes compared with no screening.
PRIMARY FUNDING SOURCE: Agency for Healthcare Research and Quality.},
	language = {eng},
	number = {7},
	journal = {Annals of Internal Medicine},
	author = {Chou, Roger and Dana, Tracy},
	month = oct,
	year = {2010},
	pmid = {20921545},
	keywords = {Humans, Urinary Bladder Neoplasms, Adult, Mass Screening, Biomarkers, Tumor, Evidence-Based Medicine, Hematuria, Reproducibility of Results, Risk Assessment, Urinalysis, Urine},
	pages = {461--468},
}

@book{olson_screening_2013,
	address = {Rockville (MD)},
	series = {U.{S}. {Preventive} {Services} {Task} {Force} {Evidence} {Syntheses}, formerly {Systematic} {Evidence} {Reviews}},
	title = {Screening for {Oral} {Cancer}: {A} {Targeted} {Evidence} {Update} for the {U}.{S}. {Preventive} {Services} {Task} {Force}},
	shorttitle = {Screening for {Oral} {Cancer}},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK132472/},
	abstract = {To assess whether screening for oral cancer reduces morbidity or mortality and to determine the performance characteristics of the oral screening examination for cancer or potentially malignant disorders (PMDs)., Building on previous searches, we searched Medline from January 2008 through July 2011. We supplemented searches with bibliographies from retrieved articles and from previous U.S. Preventive Services Task Force (USPSTF) reviews., One investigator reviewed citations at the title and abstract level; two investigators independently reviewed potentially relevant citations at the full-text level using predefined inclusion and exclusion criteria. A single investigator extracted study characteristics and results; a second investigator confirmed data. Two investigators rated the studies for internal validity using USPSTF criteria. Evidence was described in text and tables and summarized by qualitative analysis., Evidence for the effect of oral screening on morbidity and mortality came from a single, large randomized, controlled trial (n=191,873) conducted in a population with high disease prevalence using home-based screening by advanced health workers. Screened subjects had no significant difference in incidence or mortality rates from oral cancer compared with subjects who were not screened. However, screened subjects had oral cancer diagnosed at lower stages and with greater 5-year survival. Within the subgroup who used tobacco or alcohol (n=84,600), screened subjects had a lower mortality rate from oral cancer than subjects who were not screened. Evidence for the performance characteristics of the screening examination came from seven primary studies (n=49,120), most conducted in settings with much higher incidence and mortality from oral cancer than the United States. Studies also had considerable heterogeneity in design and showed wide variation in performance characteristics. Screening examinations by general dentists in the United Kingdom among 2,336 presumably higher-risk patients age 40 years and older showed sensitivity for oral cancer or PMD of 71 to 74 percent, with positive predictive value of 67 to 86 percent and specificity of 98 to 99 percent. Adding toluidine blue dye to a screening examination did not significantly change its performance, as measured by the malignant transformation rate or incidence of oral cancer., We found no evidence on screening either a general or a selected high-risk population for oral cancer in the United States. Screening subjects in a high-prevalence population outside the United States lowered the stage of oral cancer at diagnosis and improved 5-year survival. However, survival differences could represent length or lead-time bias. Screening subjects in the subgroup who used tobacco or alcohol reduced the mortality rate from oral cancer. Subgroup analyses, however, were post-hoc and should be viewed as exploratory. The performance characteristics of the screening examination varied widely, with applicable results only from dentists addressing higher-risk patients in the United Kingdom. However, sensitivity and specificity estimates were for PMDs as well as cancers, and do not represent a clear screening strategy that is applicable to U.S. practice. We found no evidence that any adjunctive device affects the performance of the screening examination.},
	language = {eng},
	urldate = {2021-11-04},
	publisher = {Agency for Healthcare Research and Quality (US)},
	author = {Olson, Carin M. and Burda, Brittany U. and Beil, Tracy and Whitlock, Evelyn P.},
	year = {2013},
	pmid = {23617014},
}

@article{lin_screening_2010,
	title = {Screening for testicular cancer: an evidence review for the {U}.{S}. {Preventive} {Services} {Task} {Force}},
	volume = {153},
	issn = {1539-3704},
	shorttitle = {Screening for testicular cancer},
	doi = {10.7326/0003-4819-153-6-201009210-00007},
	abstract = {BACKGROUND: Testicular cancer is the most common type of cancer in men aged 15 to 34 years. Because treatment produces favorable outcomes even in advanced stages, the U.S. Preventive Services Task Force (USPSTF) concluded in 2004 that screening asymptomatic men for testicular cancer is unlikely to produce additional benefits over clinical detection.
PURPOSE: To search for new evidence on the benefits and harms of screening for testicular cancer to assist the USPSTF in updating its 2004 recommendation.
DATA SOURCES: English-language articles indexed in PubMed and the Cochrane Library and published between 1 January 2001 and 11 November 2009.
STUDY SELECTION: Randomized, controlled trials; meta-analyses; systematic reviews; cohort studies; and case-control studies were selected to determine the benefits of screening for testicular cancer. Randomized, controlled trials; meta-analyses; systematic reviews; cohort studies; case-control studies; and case series of large, multisite databases were selected to determine the harms of screening. Each author independently reviewed titles, abstracts, and full-text articles for possible inclusion.
DATA EXTRACTION: One author abstracted information on the benefits and harms of screening for testicular cancer.
DATA SYNTHESIS: No studies met the inclusion criteria. Three studies were considered for inclusion at the full-text stage of review. These inconclusive studies addressed testicular microlithiasis, XIST gene testing, and testis-sparing surgery.
LIMITATION: The focused search strategy may have missed some smaller studies or studies published in languages other than English on the benefits or harms of testicular cancer screening.
CONCLUSION: No new evidence was found on the benefits or harms of screening for testicular cancer that would affect the USPSTF's previous recommendation against screening.},
	language = {eng},
	number = {6},
	journal = {Annals of Internal Medicine},
	author = {Lin, Kenneth and Sharangpani, Ruta},
	month = sep,
	year = {2010},
	pmid = {20855803},
	keywords = {Humans, Male, Adult, Mass Screening, Evidence-Based Medicine, Adolescent, Testicular Neoplasms, Young Adult},
	pages = {396--399},
}

@article{richards_independent_2019,
	title = {Independent review of national cancer screening programmes in {England}: interim report of emerging findings},
	urldate = {2021-11-04},
	author = {Richards, M},
	year = {2019},
}

@misc{danish_health_authority_national_2021,
	title = {National screening programme},
	url = {https://www.sst.dk/en/english/responsibilities-and-tasks/health-promotion/national-screening-programme},
	abstract = {In Denmark, we offer free national screening programmes for cervical cancer, breast cancer, and bowel and rectal cancer, as well as for a number of conditions and diseases in pregnant women and newborns.},
	language = {en},
	urldate = {2021-11-04},
	journal = {Danish Health Authority},
	author = {Danish Health Authority},
	year = {2021},
}

@article{njor_three_2018,
	title = {Three years of colorectal cancer screening in {Denmark}},
	volume = {57},
	issn = {1877-783X},
	doi = {10.1016/j.canep.2018.09.003},
	abstract = {BACKGROUND: The Danish National Colorectal Cancer Screening Programme was implemented in March 2014 and is offered free of charge to all residents aged 50-74 years. The aim of this study is to compare performance indicators from the Danish National Colorectal Cancer Screening Programme to the recommendations from European Guidelines in order to assure the quality of the programme and to provide findings relevant to other population-based colorectal cancer screening programmes.
METHODS: Based on data from the Danish Colorectal Cancer Screening Database, we evaluated all performance indicators for which the European Guidelines provided acceptable level, desirable level or the level from first screening rounds in population-based studies using FIT.
RESULTS: All performance indicators were above the acceptable level and/or in line with the level from the first screening round in population-based studies using FIT. Whenever the European Guidelines provided a desirable level for a performance indicator, the Danish National Colorectal Cancer Screening Programme was close to or above this desirable level.
CONCLUSIONS: Compared to the European Guidelines, all performance indicators were above the acceptable level and close to the desirable level. Based on these findings, the implementation of the National Danish Colorectal Cancer Screening Programme is considered a success and the programme is hopefully in the process of reducing colorectal cancer morbidity and mortality in Denmark. This study provides relevant information for comparisons to other population-based public service colorectal cancer screening programmes as well as for future revisions of guidelines.},
	language = {eng},
	journal = {Cancer Epidemiology},
	author = {Njor, Sisse Helle and Friis-Hansen, Lennart and Andersen, Berit and Søndergaard, Bo and Linnemann, Dorte and Jørgensen, Jens Christian Riis and Roikjær, Ole and Rasmussen, Morten},
	month = dec,
	year = {2018},
	pmid = {30292899},
	keywords = {Humans, Female, Aged, Early Detection of Cancer, Mass Screening, Middle Aged, Colorectal cancer, Colorectal Neoplasms, Databases, Factual, Denmark, Epidemiology, Indicators, Prevention, Screening},
	pages = {39--44},
}

@article{lynge_outcome_2017,
	title = {Outcome of breast cancer screening in {Denmark}},
	volume = {17},
	issn = {1471-2407},
	url = {https://doi.org/10.1186/s12885-017-3929-6},
	doi = {10.1186/s12885-017-3929-6},
	abstract = {In Denmark, national roll-out of a population-based, screening mammography program took place in 2007–2010. We report on outcome of the first four biennial invitation rounds.},
	language = {en},
	number = {1},
	urldate = {2021-11-04},
	journal = {BMC Cancer},
	author = {Lynge, Elsebeth and Bak, Martin and von Euler-Chelpin, My and Kroman, Niels and Lernevall, Anders and Mogensen, Nikolaj Borg and Schwartz, Walter and Wronecki, Adam Jan and Vejborg, Ilse},
	month = dec,
	year = {2017},
	pages = {897},
}

@article{bigaard_cervical_2000,
	title = {Cervical cancer screening in {Denmark}},
	volume = {36},
	issn = {0959-8049},
	doi = {10.1016/s0959-8049(00)00309-9},
	abstract = {Denmark is divided into 15 counties and it is up to regional politicians and the health authority in each county to organise the cervical screening programmes. The National Board of Health issued national guidelines and recommendations for the cervical cancer screening back in 1986, and these guidelines are now, in 1998, almost fully implemented. In this study, a literature review on cervical cancer screening in Denmark, review of local guidelines, personal interviews with pathologists and collection of information about the education of cytotechnologists in Denmark was carried out. In Denmark in 1997 90\% of women aged 23-59 years and 46\% of women aged 60-74 years were covered by organised screening. A total of 650000 smears were taken annually. This figure corresponds to screening of all Danish women aged 25-59 years on average, every second year. The national recommendation is screening every third year. Thus, as the incidence of cervical cancer in Denmark is decreasing, we could probably move towards a longer screening interval. However, before the Danish recommendations are changed, more detailed data on the actual performances of screening programmes are warranted.},
	language = {eng},
	number = {17},
	journal = {European Journal of Cancer (Oxford, England: 1990)},
	author = {Bigaard, J. and Hariri, J. and Lynge, E.},
	month = nov,
	year = {2000},
	pmid = {11072204},
	keywords = {Humans, Female, Algorithms, Adult, Aged, Mass Screening, Middle Aged, Denmark, Family Practice, Follow-Up Studies, Quality Assurance, Health Care, Referral and Consultation, Uterine Cervical Neoplasms, Vaginal Smears, Women's Health Services},
	pages = {2198--2204},
}

@misc{nhs_england_nhs_2021,
	title = {{NHS} {England} » {Screening} and earlier diagnosis},
	url = {https://www.england.nhs.uk/cancer/early-diagnosis/screening-and-earlier-diagnosis/},
	urldate = {2021-11-04},
	author = {NHS England},
	year = {2021},
}

@misc{cdc_cancer_2021,
	title = {Cancer {Screening} {Tests} {\textbar} {CDC}},
	url = {https://www.cdc.gov/cancer/dcpc/prevention/screening.htm},
	abstract = {Tests can find some cancers early, when treatment works best.},
	language = {en-us},
	urldate = {2021-11-04},
	author = {CDC},
	month = aug,
	year = {2021},
}

@misc{john_cancer_2019,
	title = {Cancer survival in {England} - {Office} for {National} {Statistics}},
	url = {https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/cancersurvivalinengland/stageatdiagnosisandchildhoodpatientsfollowedupto2018},
	urldate = {2021-11-04},
	author = {John, Sophie and Broggio, John},
	year = {2019},
}

@article{lee_boadicea_2019,
	title = {{BOADICEA}: a comprehensive breast cancer risk prediction model incorporating genetic and nongenetic risk factors},
	volume = {21},
	copyright = {2019 The Author(s)},
	issn = {1530-0366},
	shorttitle = {{BOADICEA}},
	url = {https://www.nature.com/articles/s41436-018-0406-9},
	doi = {10.1038/s41436-018-0406-9},
	abstract = {Breast cancer (BC) risk prediction allows systematic identificationof individuals at highest and lowest risk. We extend the Breast and OvarianAnalysis of Disease Incidence and Carrier Estimation Algorithm (BOADICEA) riskmodel to incorporate the effects of polygenic risk scores (PRS) and other riskfactors (RFs).},
	language = {en},
	number = {8},
	urldate = {2021-11-04},
	journal = {Genetics in Medicine},
	author = {Lee, Andrew and Mavaddat, Nasim and Wilcox, Amber N. and Cunningham, Alex P. and Carver, Tim and Hartley, Simon and Babb de Villiers, Chantal and Izquierdo, Angel and Simard, Jacques and Schmidt, Marjanka K. and Walter, Fiona M. and Chatterjee, Nilanjan and Garcia-Closas, Montserrat and Tischkowitz, Marc and Pharoah, Paul and Easton, Douglas F. and Antoniou, Antonis C.},
	month = aug,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_y
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group},
	keywords = {Biomedicine, general, Human Genetics, Laboratory Medicine},
	pages = {1708--1718},
}

@article{carver_canrisk_2021,
	title = {{CanRisk} {Tool}—{A} {Web} {Interface} for the {Prediction} of {Breast} and {Ovarian} {Cancer} {Risk} and the {Likelihood} of {Carrying} {Genetic} {Pathogenic} {Variants}},
	volume = {30},
	copyright = {©2020 American Association for Cancer Research.},
	issn = {1055-9965, 1538-7755},
	url = {https://cebp.aacrjournals.org/content/30/3/469},
	doi = {10.1158/1055-9965.EPI-20-1319},
	abstract = {Background: The CanRisk Tool (https://canrisk.org) is the next-generation web interface for the latest version of the BOADICEA (Breast and Ovarian Analysis of Disease Incidence and Carrier Estimation Algorithm) state-of-the-art risk model and a forthcoming ovarian cancer risk model.
Methods: The tool captures information on family history, rare pathogenic variants in cancer susceptibility genes, polygenic risk scores, lifestyle/hormonal/clinical features, and imaging risk factors to predict breast and ovarian cancer risks and estimate the probabilities of carrying pathogenic variants in certain genes. It was implemented using modern web frameworks, technologies, and web services to make it extensible and increase accessibility to researchers and third-party applications. The design of the graphical user interface was informed by feedback from health care professionals and a formal evaluation.
Results: This freely accessible tool was designed to be user friendly for clinicians and to boost acceptability in clinical settings. The tool incorporates a novel graphical pedigree builder to facilitate collection of the family history data required by risk calculations.
Conclusions: The CanRisk Tool provides health care professionals and researchers with a user-friendly interface to carry out multifactorial breast and ovarian cancer risk predictions. It is the first freely accessible cancer risk prediction program to carry the CE marking.
Impact: There have been over 3,100 account registrations, and 98,000 breast and ovarian cancer risk calculations have been run within the first 9 months of the CanRisk Tool launch.},
	language = {en},
	number = {3},
	urldate = {2021-11-05},
	journal = {Cancer Epidemiology and Prevention Biomarkers},
	author = {Carver, Tim and Hartley, Simon and Lee, Andrew and Cunningham, Alex P. and Archer, Stephanie and Villiers, Chantal Babb de and Roberts, Jonathan and Ruston, Rod and Walter, Fiona M. and Tischkowitz, Marc and Easton, Douglas F. and Antoniou, Antonis C.},
	month = mar,
	year = {2021},
	pmid = {33335023},
	note = {Publisher: American Association for Cancer Research
Section: Research Articles},
	pages = {469--473},
}

@article{gail_projecting_1989,
	title = {Projecting individualized probabilities of developing breast cancer for white females who are being examined annually},
	volume = {81},
	issn = {0027-8874},
	doi = {10.1093/jnci/81.24.1879},
	abstract = {To assist in medical counseling, we present a method to estimate the chance that a woman with given age and risk factors will develop breast cancer over a specified interval. The risk factors used were age at menarche, age at first live birth, number of previous biopsies, and number of first-degree relatives with breast cancer. A model of relative risks for various combinations of these factors was developed from case-control data from the Breast Cancer Detection Demonstration Project (BCDDP). The model allowed for the fact that relative risks associated with previous breast biopsies were smaller for women aged 50 or more than for younger women. Thus, the proportional hazards models for those under age 50 and for those of age 50 or more. The baseline age-specific hazard rate, which is the rate for a patient without identified risk factors, is computed as the product of the observed age-specific composite hazard rate times the quantity 1 minus the attributable risk. We calculated individualized breast cancer probabilities from information on relative risks and the baseline hazard rate. These calculations take competing risks and the interval of risk into account. Our data were derived from women who participated in the BCDDP and who tended to return for periodic examinations. For this reason, the risk projections given are probably most reliable for counseling women who plan to be examined about once a year.},
	language = {eng},
	number = {24},
	journal = {Journal of the National Cancer Institute},
	author = {Gail, M. H. and Brinton, L. A. and Byar, D. P. and Corle, D. K. and Green, S. B. and Schairer, C. and Mulvihill, J. J.},
	month = dec,
	year = {1989},
	pmid = {2593165},
	keywords = {Humans, Breast Neoplasms, Female, Age Factors, Adult, Aged, Middle Aged, European Continental Ancestry Group, Probability, Risk},
	pages = {1879--1886},
}

@article{louro_systematic_2019,
	title = {A systematic review and quality assessment of individualised breast cancer risk prediction models},
	volume = {121},
	copyright = {2019 The Author(s)},
	issn = {1532-1827},
	url = {https://www.nature.com/articles/s41416-019-0476-8},
	doi = {10.1038/s41416-019-0476-8},
	abstract = {Individualised breast cancer risk prediction models may be key for planning risk-based screening approaches. Our aim was to conduct a systematic review and quality assessment of these models addressed to women in the general population.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {British Journal of Cancer},
	author = {Louro, Javier and Posso, Margarita and Hilton Boon, Michele and Román, Marta and Domingo, Laia and Castells, Xavier and Sala, María},
	month = jul,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Breast cancer;Epidemiology
Subject\_term\_id: breast-cancer;epidemiology},
	keywords = {Epidemiology, Breast cancer},
	pages = {76--85},
}

@article{zheng_new_2020,
	title = {A {New} {Comprehensive} {Colorectal} {Cancer} {Risk} {Prediction} {Model} {Incorporating} {Family} {History}, {Personal} {Characteristics}, and {Environmental} {Factors}},
	volume = {29},
	copyright = {©2020 American Association for Cancer Research.},
	issn = {1055-9965, 1538-7755},
	url = {https://cebp.aacrjournals.org/content/29/3/549},
	doi = {10.1158/1055-9965.EPI-19-0929},
	abstract = {Purpose: Reducing colorectal cancer incidence and mortality through early detection would improve efficacy if targeted. We developed a colorectal cancer risk prediction model incorporating personal, family, genetic, and environmental risk factors to enhance prevention.
Methods: A familial risk profile (FRP) was calculated to summarize individuals' risk based on detailed cancer family history (FH), family structure, probabilities of mutation in major colorectal cancer susceptibility genes, and a polygenic component. We developed risk models, including individuals' FRP or binary colorectal cancer FH, and colorectal cancer risk factors collected at enrollment using population-based colorectal cancer cases (N = 4,445) and controls (N = 3,967) recruited by the Colon Cancer Family Registry Cohort (CCFRC). Model validation used CCFRC follow-up data for population-based (N = 12,052) and clinic-based (N = 5,584) relatives with no cancer history at recruitment to assess model calibration [expected/observed rate ratio (E/O)] and discrimination [area under the receiver-operating-characteristic curve (AUC)].
Results: The E/O [95\% confidence interval (CI)] for FRP models for population-based relatives were 1.04 (0.74–1.45) for men and 0.86 (0.64–1.20) for women, and for clinic-based relatives were 1.15 (0.87–1.58) for men and 1.04 (0.76–1.45) for women. The age-adjusted AUCs (95\% CI) for FRP models for population-based relatives were 0.69 (0.60–0.78) for men and 0.70 (0.62–0.77) for women, and for clinic-based relatives were 0.77 (0.69–0.84) for men and 0.68 (0.60–0.76) for women. The incremental values of AUC for FRP over FH models for population-based relatives were 0.08 (0.01–0.15) for men and 0.10 (0.04–0.16) for women, and for clinic-based relatives were 0.11 (0.05–0.17) for men and 0.11 (0.06–0.17) for women.
Conclusions: Both models calibrated well. The FRP-based model provided better risk stratification and risk discrimination than the FH-based model.
Impact: Our findings suggest detailed FH may be useful for targeted risk-based screening and clinical management.},
	language = {en},
	number = {3},
	urldate = {2021-11-05},
	journal = {Cancer Epidemiology and Prevention Biomarkers},
	author = {Zheng, Yingye and Hua, Xinwei and Win, Aung K. and MacInnis, Robert J. and Gallinger, Steven and Marchand, Loic Le and Lindor, Noralane M. and Baron, John A. and Hopper, John L. and Dowty, James G. and Antoniou, Antonis C. and Zheng, Jiayin and Jenkins, Mark A. and Newcomb, Polly A.},
	month = mar,
	year = {2020},
	pmid = {31932410},
	note = {Publisher: American Association for Cancer Research
Section: Research Articles},
	pages = {549--557},
}

@article{freedman_colorectal_2009,
	title = {Colorectal cancer risk prediction tool for white men and women without known susceptibility},
	volume = {27},
	issn = {1527-7755},
	doi = {10.1200/JCO.2008.17.4797},
	abstract = {PURPOSE: Given the high incidence of colorectal cancer (CRC), and the availability of procedures that can detect disease and remove precancerous lesions, there is a need for a model that estimates the probability of developing CRC across various age intervals and risk factor profiles.
METHODS: The development of separate CRC absolute risk models for men and women included estimating relative risks and attributable risk parameters from population-based case-control data separately for proximal, distal, and rectal cancer and combining these estimates with baseline age-specific cancer hazard rates based on Surveillance, Epidemiology, and End Results (SEER) incidence rates and competing mortality risks.
RESULTS: For men, the model included a cancer-negative sigmoidoscopy/colonoscopy in the last 10 years, polyp history in the last 10 years, history of CRC in first-degree relatives, aspirin and nonsteroidal anti-inflammatory drug (NSAID) use, cigarette smoking, body mass index (BMI), current leisure-time vigorous activity, and vegetable consumption. For women, the model included sigmoidoscopy/colonoscopy, polyp history, history of CRC in first-degree relatives, aspirin and NSAID use, BMI, leisure-time vigorous activity, vegetable consumption, hormone-replacement therapy (HRT), and estrogen exposure on the basis of menopausal status. For men and women, relative risks differed slightly by tumor site. A validation study in independent data indicates that the models for men and women are well calibrated.
CONCLUSION: We developed absolute risk prediction models for CRC from population-based data, and a simple questionnaire suitable for self-administration. This model is potentially useful for counseling, for designing research intervention studies, and for other applications.},
	language = {eng},
	number = {5},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Freedman, Andrew N. and Slattery, Martha L. and Ballard-Barbash, Rachel and Willis, Gordon and Cann, Bette J. and Pee, David and Gail, Mitchell H. and Pfeiffer, Ruth M.},
	month = feb,
	year = {2009},
	pmid = {19114701},
	pmcid = {PMC2645090},
	keywords = {Humans, Proportional Hazards Models, Female, Male, Aged, Middle Aged, Risk Factors, Colorectal Neoplasms, European Continental Ancestry Group, Anti-Inflammatory Agents, Non-Steroidal, Body Mass Index, Colonoscopy, Diet, Intestinal Polyps, Leisure Activities, Models, Theoretical, Sigmoidoscopy},
	pages = {686--693},
}

@article{smith_comparison_2019,
	title = {Comparison of prognostic models to predict the occurrence of colorectal cancer in asymptomatic individuals: a systematic literature review and external validation in the {EPIC} and {UK} {Biobank} prospective cohort studies},
	volume = {68},
	copyright = {© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2019. All rights reserved. No commercial use is permitted unless otherwise expressly granted.. This is an open access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/},
	issn = {0017-5749, 1468-3288},
	shorttitle = {Comparison of prognostic models to predict the occurrence of colorectal cancer in asymptomatic individuals},
	url = {https://gut.bmj.com/content/68/4/672},
	doi = {10.1136/gutjnl-2017-315730},
	abstract = {Objective To systematically identify and validate published colorectal cancer risk prediction models that do not require invasive testing in two large population-based prospective cohorts.
Design Models were identified through an update of a published systematic review and validated in the European Prospective Investigation into Cancer and Nutrition (EPIC) and the UK Biobank. The performance of the models to predict the occurrence of colorectal cancer within 5 or 10 years after study enrolment was assessed by discrimination (C-statistic) and calibration (plots of observed vs predicted probability).
Results The systematic review and its update identified 16 models from 8 publications (8 colorectal, 5 colon and 3 rectal). The number of participants included in each model validation ranged from 41 587 to 396 515, and the number of cases ranged from 115 to 1781. Eligible and ineligible participants across the models were largely comparable. Calibration of the models, where assessable, was very good and further improved by recalibration. The C-statistics of the models were largely similar between validation cohorts with the highest values achieved being 0.70 (95\% CI 0.68 to 0.72) in the UK Biobank and 0.71 (95\% CI 0.67 to 0.74) in EPIC.
Conclusion Several of these non-invasive models exhibited good calibration and discrimination within both external validation populations and are therefore potentially suitable candidates for the facilitation of risk stratification in population-based colorectal screening programmes. Future work should both evaluate this potential, through modelling and impact studies, and ascertain if further enhancement in their performance can be obtained.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Gut},
	author = {Smith, Todd and Muller, David C. and Moons, Karel G. M. and Cross, Amanda J. and Johansson, Mattias and Ferrari, Pietro and Fagherazzi, Guy and Peeters, Petra H. M. and Severi, Gianluca and Hüsing, Anika and Kaaks, Rudolf and Tjonneland, Anne and Olsen, Anja and Overvad, Kim and Bonet, Catalina and Rodriguez-Barranco, Miguel and Huerta, Jose Maria and Gurrea, Aurelio Barricarte and Bradbury, Kathryn E. and Trichopoulou, Antonia and Bamia, Christina and Orfanos, Philippos and Palli, Domenico and Pala, Valeria and Vineis, Paolo and Bueno-de-Mesquita, Bas and Ohlsson, Bodil and Harlid, Sophia and Guelpen, Bethany Van and Skeie, Guri and Weiderpass, Elisabete and Jenab, Mazda and Murphy, Neil and Riboli, Elio and Gunter, Marc J. and Aleksandrova, Krasimira Jekova and Tzoulaki, Ioanna},
	month = apr,
	year = {2019},
	pmid = {29615487},
	note = {Publisher: BMJ Publishing Group
Section: Colon},
	keywords = {colorectal cancer, cancer prevention, colorectal cancer screening, epidemiology, medical statistics},
	pages = {672--683},
}

@article{gaba_population_2020,
	title = {Population {Study} of {Ovarian} {Cancer} {Risk} {Prediction} for {Targeted} {Screening} and {Prevention}},
	volume = {12},
	issn = {2072-6694},
	doi = {10.3390/cancers12051241},
	abstract = {Unselected population-based personalised ovarian cancer (OC) risk assessment combining genetic/epidemiology/hormonal data has not previously been undertaken. We aimed to perform a feasibility study of OC risk stratification of general population women using a personalised OC risk tool followed by risk management. Volunteers were recruited through London primary care networks.
INCLUSION CRITERIA: women ≥18 years.
EXCLUSION CRITERIA: prior ovarian/tubal/peritoneal cancer, previous genetic testing for OC genes. Participants accessed an online/web-based decision aid along with optional telephone helpline use. Consenting individuals completed risk assessment and underwent genetic testing (BRCA1/BRCA2/RAD51C/RAD51D/BRIP1, OC susceptibility single-nucleotide polymorphisms). A validated OC risk prediction algorithm provided a personalised OC risk estimate using genetic/lifestyle/hormonal OC risk factors. Population genetic testing (PGT)/OC risk stratification uptake/acceptability, satisfaction, decision aid/telephone helpline use, psychological health and quality of life were assessed using validated/customised questionnaires over six months. Linear-mixed models/contrast tests analysed impact on study outcomes.
MAIN OUTCOMES: feasibility/acceptability, uptake, decision aid/telephone helpline use, satisfaction/regret, and impact on psychological health/quality of life. In total, 123 volunteers (mean age = 48.5 (SD = 15.4) years) used the decision aid, 105 (85\%) consented. None fulfilled NHS genetic testing clinical criteria. OC risk stratification revealed 1/103 at ≥10\% (high), 0/103 at ≥5\%-{\textless}10\% (intermediate), and 100/103 at {\textless}5\% (low) lifetime OC risk. Decision aid satisfaction was 92.2\%. The telephone helpline use rate was 13\% and the questionnaire response rate at six months was 75\%. Contrast tests indicated that overall depression (p = 0.30), anxiety (p = 0.10), quality-of-life (p = 0.99), and distress (p = 0.25) levels did not jointly change, while OC worry (p = 0.021) and general cancer risk perception (p = 0.015) decreased over six months. In total, 85.5-98.7\% were satisfied with their decision. Findings suggest population-based personalised OC risk stratification is feasible and acceptable, has high satisfaction, reduces cancer worry/risk perception, and does not negatively impact psychological health/quality of life.},
	language = {eng},
	number = {5},
	journal = {Cancers},
	author = {Gaba, Faiza and Blyuss, Oleg and Liu, Xinting and Goyal, Shivam and Lahoti, Nishant and Chandrasekaran, Dhivya and Kurzer, Margarida and Kalsi, Jatinderpal and Sanderson, Saskia and Lanceley, Anne and Ahmed, Munaza and Side, Lucy and Gentry-Maharaj, Aleksandra and Wallis, Yvonne and Wallace, Andrew and Waller, Jo and Luccarini, Craig and Yang, Xin and Dennis, Joe and Dunning, Alison and Lee, Andrew and Antoniou, Antonis C. and Legood, Rosa and Menon, Usha and Jacobs, Ian and Manchanda, Ranjit},
	month = may,
	year = {2020},
	pmid = {32429029},
	pmcid = {PMC7281662},
	keywords = {BRCA1, BRCA2, BRIP1, ovarian cancer risk, population genetic testing, RAD51C, RAD51D, risk modelling, risk stratification, SNP},
	pages = {E1241},
}

@article{jervis_risk_2015,
	title = {A risk prediction algorithm for ovarian cancer incorporating {BRCA1}, {BRCA2}, common alleles and other familial effects},
	volume = {52},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/},
	issn = {0022-2593, 1468-6244},
	url = {https://jmg.bmj.com/content/52/7/465},
	doi = {10.1136/jmedgenet-2015-103077},
	abstract = {Background Although BRCA1 and BRCA2 mutations account for only ∼27\% of the familial aggregation of ovarian cancer (OvC), no OvC risk prediction model currently exists that considers the effects of BRCA1, BRCA2 and other familial factors. Therefore, a currently unresolved problem in clinical genetics is how to counsel women with family history of OvC but no identifiable BRCA1/2 mutations.
Methods We used data from 1548 patients with OvC and their relatives from a population-based study, with known BRCA1/2 mutation status, to investigate OvC genetic susceptibility models, using segregation analysis methods.
Results The most parsimonious model included the effects of BRCA1/2 mutations, and the residual familial aggregation was accounted for by a polygenic component (SD 1.43, 95\% CI 1.10 to 1.86), reflecting the multiplicative effects of a large number of genes with small contributions to the familial risk. We estimated that 1 in 630 individuals carries a BRCA1 mutation and 1 in 195 carries a BRCA2 mutation. We extended this model to incorporate the explicit effects of 17 common alleles that are associated with OvC risk. Based on our models, assuming all of the susceptibility genes could be identified we estimate that the half of the female population at highest genetic risk will account for 92\% of all OvCs.
Conclusions The resulting model can be used to obtain the risk of developing OvC on the basis of BRCA1/2, explicit family history and common alleles. This is the first model that accounts for all OvC familial aggregation and would be useful in the OvC genetic counselling process.},
	language = {en},
	number = {7},
	urldate = {2021-11-05},
	journal = {Journal of Medical Genetics},
	author = {Jervis, Sarah and Song, Honglin and Lee, Andrew and Dicks, Ed and Harrington, Patricia and Baynes, Caroline and Manchanda, Ranjit and Easton, Douglas F. and Jacobs, Ian and Pharoah, Paul P. D. and Antoniou, Antonis C.},
	month = jul,
	year = {2015},
	pmid = {26025000},
	note = {Publisher: BMJ Publishing Group Ltd
Section: Cancer genetics},
	keywords = {Genetic epidemiology, Genetic screening/counselling, Genome-wide, Ovarian Cancer, Risk prediction},
	pages = {465--475},
}

@article{muller_lung_2017,
	title = {Lung {Cancer} {Risk} {Prediction} {Model} {Incorporating} {Lung} {Function}: {Development} and {Validation} in the {UK} {Biobank} {Prospective} {Cohort} {Study}},
	volume = {35},
	issn = {1527-7755},
	shorttitle = {Lung {Cancer} {Risk} {Prediction} {Model} {Incorporating} {Lung} {Function}},
	doi = {10.1200/JCO.2016.69.2467},
	abstract = {Purpose Several lung cancer risk prediction models have been developed, but none to date have assessed the predictive ability of lung function in a population-based cohort. We sought to develop and internally validate a model incorporating lung function using data from the UK Biobank prospective cohort study. Methods This analysis included 502,321 participants without a previous diagnosis of lung cancer, predominantly between 40 and 70 years of age. We used flexible parametric survival models to estimate the 2-year probability of lung cancer, accounting for the competing risk of death. Models included predictors previously shown to be associated with lung cancer risk, including sex, variables related to smoking history and nicotine addiction, medical history, family history of lung cancer, and lung function (forced expiratory volume in 1 second [FEV1]). Results During accumulated follow-up of 1,469,518 person-years, there were 738 lung cancer diagnoses. A model incorporating all predictors had excellent discrimination (concordance (c)-statistic [95\% CI] = 0.85 [0.82 to 0.87]). Internal validation suggested that the model will discriminate well when applied to new data (optimism-corrected c-statistic = 0.84). The full model, including FEV1, also had modestly superior discriminatory power than one that was designed solely on the basis of questionnaire variables (c-statistic = 0.84 [0.82 to 0.86]; optimism-corrected c-statistic = 0.83; pFEV1 = 3.4 × 10-13). The full model had better discrimination than standard lung cancer screening eligibility criteria (c-statistic = 0.66 [0.64 to 0.69]). Conclusion A risk prediction model that includes lung function has strong predictive ability, which could improve eligibility criteria for lung cancer screening programs.},
	language = {eng},
	number = {8},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Muller, David C. and Johansson, Mattias and Brennan, Paul},
	month = mar,
	year = {2017},
	pmid = {28095156},
	keywords = {Cohort Studies, Humans, Proportional Hazards Models, Female, Male, Lung Neoplasms, Adult, Aged, Middle Aged, Risk Assessment, Risk, Forced Expiratory Volume, Kaplan-Meier Estimate, Lung, Models, Statistical, Predictive Value of Tests, Prospective Studies},
	pages = {861--869},
}

@article{tammemagi_development_2019,
	title = {Development and {Validation} of a {Multivariable} {Lung} {Cancer} {Risk} {Prediction} {Model} {That} {Includes} {Low}-{Dose} {Computed} {Tomography} {Screening} {Results}: {A} {Secondary} {Analysis} of {Data} {From} the {National} {Lung} {Screening} {Trial}},
	volume = {2},
	issn = {2574-3805},
	shorttitle = {Development and {Validation} of a {Multivariable} {Lung} {Cancer} {Risk} {Prediction} {Model} {That} {Includes} {Low}-{Dose} {Computed} {Tomography} {Screening} {Results}},
	url = {https://doi.org/10.1001/jamanetworkopen.2019.0204},
	doi = {10.1001/jamanetworkopen.2019.0204},
	abstract = {Low-dose computed tomography lung cancer screening is most effective when applied to high-risk individuals.To develop and validate a risk prediction model that incorporates low-dose computed tomography screening results.A logistic regression risk model was developed in National Lung Screening Trial (NLST) Lung Screening Study (LSS) data and was validated in NLST American College of Radiology Imaging Network (ACRIN) data. The NLST was a randomized clinical trial that recruited participants between August 2002 and April 2004, with follow-up to December 31, 2009. This secondary analysis of data from the NLST took place between August 10, 2013, and November 1, 2018. Included were LSS (n = 14 576) and ACRIN (n = 7653) participants who had 3 screens, adequate follow-up, and complete predictor information.Incident lung cancers occurring 1 to 4 years after the third screen (202 LSS and 96 ACRIN). Predictors included scores from the validated PLCOm2012 risk model and Lung CT Screening Reporting \&amp; Data System (Lung-RADS) screening results.Overall, the mean (SD) age of 22 229 participants was 61.3 (5.0) years, 59.3\% were male, and 90.9\% were of non-Hispanic white race/ethnicity. During follow-up, 298 lung cancers were diagnosed in 22 229 individuals (1.3\%). Eight result combinations were pooled into 4 groups based on similar associations. Adjusted for PLCOm2012 risks, compared with participants with 3 negative screens, participants with 1 positive screen and last negative had an odds ratio (OR) of 1.93 (95\% CI, 1.34-2.76), and participants with 2 positive screens with last negative or 2 negative screens with last positive had an OR of 2.66 (95\% CI, 1.60-4.43); when 2 or more screens were positive with last positive, the OR was 8.97 (95\% CI, 5.76-13.97). In ACRIN validation data, the model that included PLCOm2012 scores and screening results (PLCO2012results) demonstrated significantly greater discrimination (area under the curve, 0.761; 95\% CI, 0.716-0.799) than when screening results were excluded (PLCOm2012) (area under the curve, 0.687; 95\% CI, 0.645-0.728) (P \&lt; .001). In ACRIN validation data, PLCO2012results demonstrated good calibration. Individuals who had initial negative scans but elevated PLCOm2012 six-year risks of at least 2.6\% did not have risks decline below the 1.5\% screening eligibility criterion when subsequent screens were negative.According to this analysis, some individuals with elevated risk scores who have negative initial screens remain at elevated risks, warranting annual screening. Positive screens seem to increase baseline risk scores and may identify high-risk individuals for continued screening and enrollment into clinical trials.ClinicalTrials.gov Identifier: NCT00047385},
	number = {3},
	urldate = {2021-11-05},
	journal = {JAMA Network Open},
	author = {Tammemägi, Martin C. and ten Haaf, Kevin and Toumazis, Iakovos and Kong, Chung Yin and Han, Summer S. and Jeon, Jihyoun and Commins, John and Riley, Thomas and Meza, Rafael},
	month = mar,
	year = {2019},
	pages = {e190204},
}

@article{robbins_comparative_2021,
	title = {Comparative performance of lung cancer risk models to define lung screening eligibility in the {United} {Kingdom}},
	volume = {124},
	copyright = {2021 World Health Organization},
	issn = {1532-1827},
	url = {https://www.nature.com/articles/s41416-021-01278-0},
	doi = {10.1038/s41416-021-01278-0},
	abstract = {The National Health Service England (NHS) classifies individuals as eligible for lung cancer screening using two risk prediction models, PLCOm2012 and Liverpool Lung Project-v2 (LLPv2). However, no study has compared the performance of lung cancer risk models in the UK.},
	language = {en},
	number = {12},
	urldate = {2021-11-05},
	journal = {British Journal of Cancer},
	author = {Robbins, Hilary A. and Alcala, Karine and Swerdlow, Anthony J. and Schoemaker, Minouk J. and Wareham, Nick and Travis, Ruth C. and Crosbie, Philip A. J. and Callister, Matthew and Baldwin, David R. and Landy, Rebecca and Johansson, Mattias},
	month = jun,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cancer epidemiology;Cancer screening;Epidemiology;Lung cancer
Subject\_term\_id: cancer-epidemiology;cancer-screening;epidemiology;lung-cancer},
	keywords = {Epidemiology, Cancer epidemiology, Cancer screening, Lung cancer},
	pages = {2026--2034},
}

@article{nordstrom_prostate_2021,
	title = {Prostate cancer screening using a combination of risk-prediction, {MRI}, and targeted prostate biopsies ({STHLM3}-{MRI}): a prospective, population-based, randomised, open-label, non-inferiority trial},
	volume = {22},
	issn = {1470-2045, 1474-5488},
	shorttitle = {Prostate cancer screening using a combination of risk-prediction, {MRI}, and targeted prostate biopsies ({STHLM3}-{MRI})},
	url = {https://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(21)00348-X/fulltext},
	doi = {10.1016/S1470-2045(21)00348-X},
	language = {English},
	number = {9},
	urldate = {2021-11-05},
	journal = {The Lancet Oncology},
	author = {Nordström, Tobias and Discacciati, Andrea and Bergman, Martin and Clements, Mark and Aly, Markus and Annerstedt, Magnus and Glaessgen, Axel and Carlsson, Stefan and Jäderling, Fredrik and Eklund, Martin and Grönberg, Henrik and Cavalli-Björkman, Carin and Björklund, Astrid and Hune, Britt-Marie and Clements, Mark and Hao, Shuang and Discacciati, Andrea and Grönberg, Henrik and Eklund, Martin and Nordström, Tobias and Carlsson, Stefan and Aly, Markus and Walldén, Mats and Steinberg, Ola and Andersson, Karl and Jäderling, Fredrik and Wimmercranz, Fredrik and Meurling, Edward and Gleassgen, Axel and Majeed, Nada and Awadelkarim, Rihab and Fyhr, Ing-Marie and Sandström, Dag and Waage, Linda and Imamov, Otabek and Lantz, Rafael and Thorstensson, Andreas and Stiernstedt, Carl and Wande, Dushaid and Trygg, Gunnar and Söderbäck, Harald and Michajlowski, Jerzy and Leykamm, Lars and Svedberg, Nils-Erik and Bergman, Tommy and Sabockis, Raimundas and Akrawi, Sirvan and Bergman, Martin and Annerstedt, Magnus},
	month = sep,
	year = {2021},
	pmid = {34391509},
	note = {Publisher: Elsevier},
	pages = {1240--1249},
}

@article{thompson_assessing_2006,
	title = {Assessing {Prostate} {Cancer} {Risk}: {Results} from the {Prostate} {Cancer} {Prevention} {Trial}},
	volume = {98},
	issn = {0027-8874},
	shorttitle = {Assessing {Prostate} {Cancer} {Risk}},
	url = {https://doi.org/10.1093/jnci/djj131},
	doi = {10.1093/jnci/djj131},
	abstract = {Background: Prostate-specific antigen (PSA) testing is the primary method used to diagnose prostate cancer in the United States. Methods to integrate other risk factors associated with prostate cancer into individualized risk prediction are needed. We used prostate biopsy data from men who participated in the Prostate Cancer Prevention Trial (PCPT) to develop a predictive model of prostate cancer. Methods: We included 5519 men from the placebo group of the PCPT who underwent prostate biopsy, had at least one PSA measurement and a digital rectal examination (DRE) performed during the year before the biopsy, and had at least two PSA measurements performed during the 3 years before the prostate biopsy. Logistic regression was used to model the risk of prostate cancer and high-grade disease associated with age at biopsy, race, family history of prostate cancer, PSA level, PSA velocity, DRE result, and previous prostate biopsy. Risk equations were created from the estimated logistic regression models. All statistical tests were two-sided. Results: A total of 1211 (21.9\%) men were diagnosed with prostate cancer by prostate biopsy. Variables that predicted prostate cancer included higher PSA level, positive family history of prostate cancer, and abnormal DRE result, whereas a previous negative prostate biopsy was associated with reduced risk. Neither age at biopsy nor PSA velocity contributed independent prognostic information. Higher PSA level, abnormal DRE result, older age at biopsy, and African American race were predictive for high-grade disease (Gleason score ≥7) whereas a previous negative prostate biopsy reduced this risk. Conclusions: This predictive model allows an individualized assessment of prostate cancer risk and risk of high-grade disease for men who undergo a prostate biopsy.},
	number = {8},
	urldate = {2021-11-05},
	journal = {JNCI: Journal of the National Cancer Institute},
	author = {Thompson, Ian M. and Ankerst, Donna Pauler and Chi, Chen and Goodman, Phyllis J. and Tangen, Catherine M. and Lucia, M. Scott and Feng, Ziding and Parnes, Howard L. and Coltman, Jr., Charles A.},
	month = apr,
	year = {2006},
	pages = {529--534},
}

@article{kachuri_pan-cancer_2020,
	title = {Pan-cancer analysis demonstrates that integrating polygenic risk scores with modifiable risk factors improves risk prediction},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-19600-4},
	doi = {10.1038/s41467-020-19600-4},
	abstract = {Cancer risk is determined by a complex interplay of environmental and heritable factors. Polygenic risk scores (PRS) provide a personalized genetic susceptibility profile that may be leveraged for disease prediction. Using data from the UK Biobank (413,753 individuals; 22,755 incident cancer cases), we quantify the added predictive value of integrating cancer-specific PRS with family history and modifiable risk factors for 16 cancers. We show that incorporating PRS measurably improves prediction accuracy for most cancers, but the magnitude of this improvement varies substantially. We also demonstrate that stratifying on levels of PRS identifies significantly divergent 5-year risk trajectories after accounting for family history and modifiable risk factors. At the population level, the top 20\% of the PRS distribution accounts for 4.0\% to 30.3\% of incident cancer cases, exceeding the impact of many lifestyle-related factors. In summary, this study illustrates the potential for improving cancer risk assessment by integrating genetic risk scores.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Nature Communications},
	author = {Kachuri, Linda and Graff, Rebecca E. and Smith-Byrne, Karl and Meyers, Travis J. and Rashkin, Sara R. and Ziv, Elad and Witte, John S. and Johansson, Mattias},
	month = nov,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cancer epidemiology;Cancer genetics
Subject\_term\_id: cancer-epidemiology;cancer-genetics},
	keywords = {Cancer epidemiology, Cancer genetics},
	pages = {6084},
}

@article{hippisley-cox_development_2015,
	title = {Development and validation of risk prediction algorithms to estimate future risk of common cancers in men and women: prospective cohort study},
	volume = {5},
	issn = {2044-6055},
	shorttitle = {Development and validation of risk prediction algorithms to estimate future risk of common cancers in men and women},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4368998/},
	doi = {10.1136/bmjopen-2015-007825},
	abstract = {Objective
To derive and validate a set of clinical risk prediction algorithm to estimate the 10-year risk of 11 common cancers.

Design
Prospective open cohort study using routinely collected data from 753 QResearch general practices in England. We used 565 practices to develop the scores and 188 for validation.

Subjects
4.96 million patients aged 25–84 years in the derivation cohort; 1.64 million in the validation cohort. Patients were free of the relevant cancer at baseline.

Methods
Cox proportional hazards models in the derivation cohort to derive 10-year risk algorithms. Risk factors considered included age, ethnicity, deprivation, body mass index, smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities and medication. Measures of calibration and discrimination in the validation cohort.

Outcomes
Incident cases of blood, breast, bowel, gastro-oesophageal, lung, oral, ovarian, pancreas, prostate, renal tract and uterine cancers. Cancers were recorded on any one of four linked data sources (general practitioner (GP), mortality, hospital or cancer records).

Results
We identified 228 241 incident cases during follow-up of the 11 types of cancer. Of these 25 444 were blood; 41 315 breast; 32 626 bowel, 12 808 gastro-oesophageal; 32 187 lung; 4811 oral; 6635 ovarian; 7119 pancreatic; 35 256 prostate; 23 091 renal tract; 6949 uterine cancers. The lung cancer algorithm had the best performance with an R2 of 64.2\%; D statistic of 2.74; receiver operating characteristic curve statistic of 0.91 in women. The sensitivity for the top 10\% of women at highest risk of lung cancer was 67\%. Performance of the algorithms in men was very similar to that for women.

Conclusions
We have developed and validated a prediction models to quantify absolute risk of 11 common cancers. They can be used to identify patients at high risk of cancers for prevention or further assessment. The algorithms could be integrated into clinical computer systems and used to identify high-risk patients.

Web calculator:
There is a simple web calculator to implement the Qcancer 10 year risk algorithm together with the open source software for download (available at http://qcancer.org/10yr/).},
	number = {3},
	urldate = {2021-11-05},
	journal = {BMJ Open},
	author = {Hippisley-Cox, Julia and Coupland, Carol},
	month = mar,
	year = {2015},
	pmid = {25783428},
	pmcid = {PMC4368998},
	pages = {e007825},
}

@article{hu_large-cohort_2019,
	title = {A {Large}-{Cohort}, {Longitudinal} {Study} {Determines} {Precancer} {Disease} {Routes} across {Different} {Cancer} {Types}},
	volume = {79},
	copyright = {©2018 American Association for Cancer Research.},
	issn = {0008-5472, 1538-7445},
	url = {https://cancerres.aacrjournals.org/content/79/4/864},
	doi = {10.1158/0008-5472.CAN-18-1677},
	abstract = {Although many diseases are associated with cancer, the full spectrum of temporal disease correlations across cancer types has not yet been characterized. A population-wide study of longitudinal disease trajectories is needed to interrogate the general medical histories of patients with cancer. Here we performed a retrospective study covering a 20-year period, using 6.9 million patients from the Danish National Patient Registry linked to 0.7 million patients with cancer from the Danish Cancer Registry. Statistical analysis identified all significant disease associations occurring prior to cancer diagnoses. These associations were used to build frequently occurring, longitudinal disease trajectories. Across 17 cancer types, a total of 648 significant diagnoses correlated directly with a cancer, while 168 diagnosis trajectories of time-ordered steps were identified for seven cancer types. The most common diseases across cancer types involved cardiovascular, obesity, and genitourinary diseases. A comprehensive, publicly available web tool of interactive illustrations for all cancer disease associations is provided. By exploring the precancer landscape using this large dataset, we identify disease associations that can be used to derive mechanistic hypotheses for future cancer research.
Significance: This study offers an innovative approach to examine prediagnostic disease and cancer development in a large national population-based setting and provides a publicly available tool to foster additional cancer surveillance research.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Cancer Research},
	author = {Hu, Jessica X. and Helleberg, Marie and Jensen, Anders B. and Brunak, Søren and Lundgren, Jens},
	month = feb,
	year = {2019},
	pmid = {30591553},
	note = {Publisher: American Association for Cancer Research
Section: Population and Prevention Science},
	pages = {864--872},
}

@article{jensen_temporal_2014,
	title = {Temporal disease trajectories condensed from population-wide registry data covering 6.2 million patients},
	volume = {5},
	copyright = {2014 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms5022},
	doi = {10.1038/ncomms5022},
	abstract = {A key prerequisite for precision medicine is the estimation of disease progression from the current patient state. Disease correlations and temporal disease progression (trajectories) have mainly been analysed with focus on a small number of diseases or using large-scale approaches without time consideration, exceeding a few years. So far, no large-scale studies have focused on defining a comprehensive set of disease trajectories. Here we present a discovery-driven analysis of temporal disease progression patterns using data from an electronic health registry covering the whole population of Denmark. We use the entire spectrum of diseases and convert 14.9 years of registry data on 6.2 million patients into 1,171 significant trajectories. We group these into patterns centred on a small number of key diagnoses such as chronic obstructive pulmonary disease (COPD) and gout, which are central to disease progression and hence important to diagnose early to mitigate the risk of adverse outcomes. We suggest such trajectory analyses may be useful for predicting and preventing future diseases of individual patients.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Nature Communications},
	author = {Jensen, Anders Boeck and Moseley, Pope L. and Oprea, Tudor I. and Ellesøe, Sabrina Gade and Eriksson, Robert and Schmock, Henriette and Jensen, Peter Bjødstrup and Jensen, Lars Juhl and Brunak, Søren},
	month = jun,
	year = {2014},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Health care;Medical research
Subject\_term\_id: health-care;medical-research},
	keywords = {Health care, Medical research},
	pages = {4022},
}

@techreport{placido_pancreatic_2021,
	type = {preprint},
	title = {Pancreatic cancer risk predicted from disease trajectories using deep learning},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.06.27.449937},
	abstract = {Pancreatic cancer is an aggressive disease that typically presents late with poor patient outcomes. There is a pronounced medical need for early detection of pancreatic cancer, which can be facilitated by identifying high-risk populations. Here we apply artificial intelligence (AI) methods to a large corpus of more than 6 million patient records spanning 40 years with 24,000 pancreatic cancer cases in the Danish National Patient Registry. In contrast to existing methods that do not use temporal information, we explicitly train machine learning models on the time sequence of diseases in patient clinical histories. In addition, the models predict the risk of cancer occurrence in time intervals of 3 to 60 months duration after risk assessment. For cancer occurrence within 12 months, the performance of the best model trained on full trajectories (AUROC=0.91) substantially exceeds that of a model without time information (AUROC=0.81). For the best model, lower performance (AUROC=0.86) results when disease events within a 3 month window before cancer diagnosis are excluded from training, reflecting the decreasing information value of earlier disease events. These results raise the state-of-the-art level of performance of cancer risk prediction on real-world data sets and provide support for the design of real-world population-wide clinical screening trials, in which high risk patients are assigned to serial imaging and measurement of blood-based markers to facilitate earlier cancer detection. AI on real-world clinical records has the potential to shift focus from treatment of late- to early-stage cancer, benefiting patients by improving lifespan and quality of life.},
	language = {en},
	urldate = {2021-11-05},
	institution = {Bioinformatics},
	author = {Placido, Davide and Yuan, Bo and Hjaltelin, Jessica X. and Haue, Amalie D. and Yuan, Chen and Kim, Jihye and Umeton, Renato and Antell, Gregory and Chowdhury, Alexander and Franz, Alexandra and Brais, Lauren and Andrews, Elizabeth and Regev, Aviv and Kraft, Peter and Wolpin, Brian M. and Rosenthal, Michael and Brunak, Søren and Sander, Chris},
	month = jun,
	year = {2021},
	doi = {10.1101/2021.06.27.449937},
	file = {Placido et al. - 2021 - Pancreatic cancer risk predicted from disease traj.pdf:/Users/alexwjung/Documents/library/storage/P9EV8JMN/Placido et al. - 2021 - Pancreatic cancer risk predicted from disease traj.pdf:application/pdf},
}

@article{chen_clinical_2021,
	title = {Clinical {Data} {Prediction} {Model} to {Identify} {Patients} {With} {Early}-{Stage} {Pancreatic} {Cancer}},
	volume = {5},
	issn = {2473-4276},
	doi = {10.1200/CCI.20.00137},
	abstract = {PURPOSE: Pancreatic cancer is an aggressive malignancy with patients often experiencing nonspecific symptoms before diagnosis. This study evaluates a machine learning approach to help identify patients with early-stage pancreatic cancer from clinical data within electronic health records (EHRs).
MATERIALS AND METHODS: From the Optum deidentified EHR data set, we identified early-stage (n = 3,322) and late-stage (n = 25,908) pancreatic cancer cases over 40 years of age diagnosed between 2009 and 2017. Patients with early-stage pancreatic cancer were matched to noncancer controls (1:16 match). We constructed a prediction model using eXtreme Gradient Boosting (XGBoost) to identify early-stage patients on the basis of 18,220 features within the EHR including diagnoses, procedures, information within clinical notes, and medications. Model accuracy was assessed with sensitivity, specificity, positive predictive value, and the area under the curve.
RESULTS: The final predictive model included 582 predictive features from the EHR, including 248 (42.5\%) physician note elements, 146 (25.0\%) procedure codes, 91 (15.6\%) diagnosis codes, 89 (15.3\%) medications, and 9 (1.5\%) demographic features. The final model area under the curve was 0.84. Choosing a model cut point with a sensitivity of 60\% and specificity of 90\% would enable early detection of 58\% late-stage patients with a median of 24 months before their actual diagnosis.
CONCLUSION: Prediction models using EHR data show promise in the early detection of pancreatic cancer. Although widespread use of this approach on an unselected population would produce high rates of false-positive tests, this technique may be rapidly impactful if deployed among high-risk patients or paired with other imaging or biomarker screening tools.},
	language = {eng},
	journal = {JCO clinical cancer informatics},
	author = {Chen, Qinyu and Cherry, Daniel R. and Nalawade, Vinit and Qiao, Edmund M. and Kumar, Abhishek and Lowy, Andrew M. and Simpson, Daniel R. and Murphy, James D.},
	month = mar,
	year = {2021},
	pmid = {33739856},
	pmcid = {PMC8462624},
	keywords = {Humans, Early Detection of Cancer, Predictive Value of Tests, Electronic Health Records, Machine Learning, Pancreatic Neoplasms},
	pages = {279--287},
}

@article{appelbaum_development_2021,
	title = {Development of a pancreatic cancer prediction model using a multinational medical records database.},
	volume = {39},
	issn = {0732-183X},
	url = {https://ascopubs.org/doi/abs/10.1200/JCO.2021.39.3_suppl.394},
	doi = {10.1200/JCO.2021.39.3_suppl.394},
	abstract = {394

Background: Previous work by our group has demonstrated that leveraging Machine Learning on diagnostic codes from Electronic Health Records (EHRs), can identify individuals at high-risk for Pancreatic Duct Adenocarcinoma (PDAC), as early as 1 year before current cancer diagnosis. We aim to improve the performance of our existing PDAC risk stratification model, by using an independent, multi-center dataset, and adding lab test features. Methods: EHR data from TriNetX, a federated global health research network, was utilized to develop Logistic Regression (LR) models. Diagnoses and lab test data from 32 different Health Care Organizations in the United States from 2015-2020 was used. PDAC patients ages 60-80 years, were identified using ICD codes, and cross-checked with tumor registry and pathology data to decrease false positives. Only patients with one or more clinical encounter/s, at least 6 months prior to cancer diagnosis, were included. Prediction time cutoffs of 180, 270, and 360 days before PDAC diagnosis were used. Preliminary basic data analysis was initially performed to explore potential lab test features that could be used to improve model performance. The discriminatory capabilities of the LR models were compared using Area Under the Receiver Operating Characteristic Curve (AUC) and 95\% Confidence Interval using empirical bootstrap over test data were computed. We used L2-regularized LR, and performed evaluation using cross-validation. We report cross-validation performance. In contrast to prior published work that used predefined feature sets for model development, we incorporated a wide range of indicators, and relied on regularization to address potential overfitting risk. Results: The LR models were trained and evaluated on diagnoses and labs for 25,644 patients (cases= 1352; age-sex paired controls). Lab test administration per patient (i.e., for a given patient, what lab tests were administered and how frequently), was found to be the most valuable feature for improving discrimination. For almost every type of lab test, the average number of administrations per patient was higher for PDAC patients than controls. The top lab tests with highest discriminatory coefficients included glucose, potassium, hematocrit, hemoglobin, sodium, chloride and creatinine. With a 365-day lead time, the diagnoses-based LR obtained a test AUC of 0.58, the lab-test based LR obtained a test AUC of 0.72. The combined diagnoses and lab-test model (“concatenated LR model”) outperformed both of these models, obtaining a test AUC of 0.73. Conclusions: Our findings demonstrate that LR models based on concatenated lab test and diagnoses feature sets (“concatenated LR models”), can outperform both diagnoses-based LR models and lab-test-based LR models, and can be utilized in early prediction of PDAC development.},
	number = {3\_suppl},
	urldate = {2021-11-05},
	journal = {Journal of Clinical Oncology},
	author = {Appelbaum, Limor and Berg, Alexandra and Cambronero, Jose Pablo and Dang, Thurston Hou Yeen and Jin, Charles Chuan and Zhang, Lori and Kundrot, Steven and Palchuk, Matvey and Evans, Laura A. and Kaplan, Irving D. and Rinard, Martin},
	month = jan,
	year = {2021},
	note = {Publisher: Wolters Kluwer},
	keywords = {227-149-1069, 283-237-267-6887-6888, 3, 5, 613-213-3369, 8},
	pages = {394--394},
}

@misc{who_international_2021,
	title = {International {Classification} of {Diseases} ({ICD})},
	url = {https://www.who.int/standards/classifications/classification-of-diseases},
	abstract = {International Classification of Diseases (ICD) Revision},
	language = {en},
	urldate = {2021-11-05},
	author = {WHO},
	year = {2021},
}

@article{abelson_prediction_2018,
	title = {Prediction of acute myeloid leukaemia risk in healthy individuals},
	volume = {559},
	copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0317-6},
	doi = {10.1038/s41586-018-0317-6},
	abstract = {The incidence of acute myeloid leukaemia (AML) increases with age and mortality exceeds 90\% when diagnosed after age 65. Most cases arise without any detectable early symptoms and patients usually present with the acute complications of bone marrow failure1. The onset of such de novo AML cases is typically preceded by the accumulation of somatic mutations in preleukaemic haematopoietic stem and progenitor cells (HSPCs) that undergo clonal expansion2,3. However, recurrent AML mutations also accumulate in HSPCs during ageing of healthy individuals who do not develop AML, a phenomenon referred to as age-related clonal haematopoiesis (ARCH)4–8. Here we use deep sequencing to analyse genes that are recurrently mutated in AML to distinguish between individuals who have a high risk of developing AML and those with benign ARCH. We analysed peripheral blood cells from 95 individuals that were obtained on average 6.3 years before AML diagnosis (pre-AML group), together with 414 unselected age- and gender-matched individuals (control group). Pre-AML cases were distinct from controls and had more mutations per sample, higher variant allele frequencies, indicating greater clonal expansion, and showed enrichment of mutations in specific genes. Genetic parameters were used to derive a model that accurately predicted AML-free survival; this model was validated in an independent cohort of 29 pre-AML cases and 262 controls. Because AML is rare, we also developed an AML predictive model using a large electronic health record database that identified individuals at greater risk. Collectively our findings provide proof-of-concept that it is possible to discriminate ARCH from pre-AML many years before malignant transformation. This could in future enable earlier detection and monitoring, and may help to inform intervention.},
	language = {en},
	number = {7714},
	urldate = {2021-11-05},
	journal = {Nature},
	author = {Abelson, Sagi and Collord, Grace and Ng, Stanley W. K. and Weissbrod, Omer and Mendelson Cohen, Netta and Niemeyer, Elisabeth and Barda, Noam and Zuzarte, Philip C. and Heisler, Lawrence and Sundaravadanam, Yogi and Luben, Robert and Hayat, Shabina and Wang, Ting Ting and Zhao, Zhen and Cirlan, Iulia and Pugh, Trevor J. and Soave, David and Ng, Karen and Latimer, Calli and Hardy, Claire and Raine, Keiran and Jones, David and Hoult, Diana and Britten, Abigail and McPherson, John D. and Johansson, Mattias and Mbabaali, Faridah and Eagles, Jenna and Miller, Jessica K. and Pasternack, Danielle and Timms, Lee and Krzyzanowski, Paul and Awadalla, Philip and Costa, Rui and Segal, Eran and Bratman, Scott V. and Beer, Philip and Behjati, Sam and Martincorena, Inigo and Wang, Jean C. Y. and Bowles, Kristian M. and Quirós, J. Ramón and Karakatsani, Anna and La Vecchia, Carlo and Trichopoulou, Antonia and Salamanca-Fernández, Elena and Huerta, José M. and Barricarte, Aurelio and Travis, Ruth C. and Tumino, Rosario and Masala, Giovanna and Boeing, Heiner and Panico, Salvatore and Kaaks, Rudolf and Krämer, Alwin and Sieri, Sabina and Riboli, Elio and Vineis, Paolo and Foll, Matthieu and McKay, James and Polidoro, Silvia and Sala, Núria and Khaw, Kay-Tee and Vermeulen, Roel and Campbell, Peter J. and Papaemmanuil, Elli and Minden, Mark D. and Tanay, Amos and Balicer, Ran D. and Wareham, Nicholas J. and Gerstung, Moritz and Dick, John E. and Brennan, Paul and Vassiliou, George S. and Shlush, Liran I.},
	month = jul,
	year = {2018},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7714
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Acute myeloid leukaemia;Cancer genetics;Cancer stem cells;Predictive markers;Risk factors
Subject\_term\_id: acute-myeloid-leukaemia;cancer-genetics;cancer-stem-cells;predictive-markers;risk-factors},
	keywords = {Cancer genetics, Acute myeloid leukaemia, Cancer stem cells, Predictive markers, Risk factors},
	pages = {400--404},
}

@article{schmidt_danish_2015-1,
	title = {The {Danish} {National} {Patient} {Registry}: a review of content, data quality, and research potential},
	volume = {7},
	issn = {1179-1349},
	shorttitle = {The {Danish} {National} {Patient} {Registry}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655913/},
	doi = {10.2147/CLEP.S91125},
	abstract = {Background
The Danish National Patient Registry (DNPR) is one of the world’s oldest nationwide hospital registries and is used extensively for research. Many studies have validated algorithms for identifying health events in the DNPR, but the reports are fragmented and no overview exists.

Objectives
To review the content, data quality, and research potential of the DNPR.

Methods
We examined the setting, history, aims, content, and classification systems of the DNPR. We searched PubMed and the Danish Medical Journal to create a bibliography of validation studies. We included also studies that were referenced in retrieved papers or known to us beforehand. Methodological considerations related to DNPR data were reviewed.

Results
During 1977–2012, the DNPR registered 8,085,603 persons, accounting for 7,268,857 inpatient, 5,953,405 outpatient, and 5,097,300 emergency department contacts. The DNPR provides nationwide longitudinal registration of detailed administrative and clinical data. It has recorded information on all patients discharged from Danish nonpsychiatric hospitals since 1977 and on psychiatric inpatients and emergency department and outpatient specialty clinic contacts since 1995. For each patient contact, one primary and optional secondary diagnoses are recorded according to the International Classification of Diseases. The DNPR provides a data source to identify diseases, examinations, certain in-hospital medical treatments, and surgical procedures. Long-term temporal trends in hospitalization and treatment rates can be studied. The positive predictive values of diseases and treatments vary widely ({\textless}15\%–100\%). The DNPR data are linkable at the patient level with data from other Danish administrative registries, clinical registries, randomized controlled trials, population surveys, and epidemiologic field studies – enabling researchers to reconstruct individual life and health trajectories for an entire population.

Conclusion
The DNPR is a valuable tool for epidemiological research. However, both its strengths and limitations must be considered when interpreting research results, and continuous validation of its clinical data is essential.},
	urldate = {2021-11-07},
	journal = {Clinical Epidemiology},
	author = {Schmidt, Morten and Schmidt, Sigrun Alba Johannesdottir and Sandegaard, Jakob Lynge and Ehrenstein, Vera and Pedersen, Lars and Sørensen, Henrik Toft},
	month = nov,
	year = {2015},
	pmid = {26604824},
	pmcid = {PMC4655913},
	pages = {449--490},
}

@article{gjerstorff_danish_2011,
	title = {The {Danish} {Cancer} {Registry}},
	volume = {39},
	issn = {1651-1905},
	doi = {10.1177/1403494810393562},
	abstract = {INTRODUCTION: The Danish Cancer Registry was founded in 1942.
CONTENT: The Cancer Registry contains data on the incidence of cancer in the Danish population since 1943.
VALIDITY AND COVERAGE: Validity of the Cancer Registry is secured by the application of manual quality control routines in the daily production of the Cancer Registry, the application of the automated cancer logic, and the use of multiple notifications from different data sources, which also secures a high degree of completeness.
CONCLUSION: In 2008 the Cancer Registry finished a process of modernisation where reporting became electronic through integration with the patient administrative systems and manual coding was partly replaced by an automatic coding logic.},
	language = {eng},
	number = {7 Suppl},
	journal = {Scandinavian Journal of Public Health},
	author = {Gjerstorff, Marianne Lundkjær},
	month = jul,
	year = {2011},
	pmid = {21775350},
	keywords = {Humans, Neoplasms, Female, Male, Denmark, Clinical Coding, Incidence, Prevalence, Registries},
	pages = {42--45},
}

@article{helweg-larsen_danish_2011,
	title = {The {Danish} {Register} of {Causes} of {Death}},
	volume = {39},
	issn = {1651-1905},
	doi = {10.1177/1403494811399958},
	abstract = {INTRODUCTION: Cause-specific mortality statistics is a valuable source for the identification of risk factors for poor public health.
CONTENT: Since 1875, the National Board of Health has maintained the register covering all deaths among citizens dying in Denmark, and since 1970 has computerised individual records.
VALIDITY AND COVERAGE: Classification of cause(s) of deaths is done in accordance to WHO's rules, since 1994 by ICD-10 codes. A change in coding practices and a low autopsy rate might influence the continuity and validity in cause-specific mortality.
CONCLUSION: The longstanding national registration of causes of death is essential for much research. The quality of the register on causes of death relies mainly upon the correctness of the physicians' notification and the coding in the National Board of Health.},
	language = {eng},
	number = {7 Suppl},
	journal = {Scandinavian Journal of Public Health},
	author = {Helweg-Larsen, Karin},
	month = jul,
	year = {2011},
	pmid = {21775346},
	keywords = {Humans, Denmark, Clinical Coding, Registries, Cause of Death, Death Certificates, World Health Organization},
	pages = {26--29},
}

@article{collins_transparent_2015,
	title = {Transparent {Reporting} of a multivariable prediction model for {Individual} {Prognosis} or {Diagnosis} ({TRIPOD}): the {TRIPOD} statement},
	volume = {162},
	issn = {1539-3704},
	shorttitle = {Transparent {Reporting} of a multivariable prediction model for {Individual} {Prognosis} or {Diagnosis} ({TRIPOD})},
	doi = {10.7326/M14-0697},
	abstract = {Prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. This article describes how the TRIPOD Statement was developed. An extensive list of items based on a review of the literature was created, which was reduced after a Web-based survey and revised during a 3-day meeting in June 2011 with methodologists, health care professionals, and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors. The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. To aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org).},
	language = {eng},
	number = {1},
	journal = {Annals of Internal Medicine},
	author = {Collins, Gary S. and Reitsma, Johannes B. and Altman, Douglas G. and Moons, Karel G. M.},
	month = jan,
	year = {2015},
	pmid = {25560714},
	keywords = {Humans, Prognosis, Multivariate Analysis, Models, Statistical, Checklist, Decision Support Techniques, Diagnosis, Publishing},
	pages = {55--63},
}

@article{engholm_nordcan_2010,
	title = {{NORDCAN} – a {Nordic} tool for cancer information, planning, quality control and research},
	volume = {49},
	issn = {0284-186X},
	url = {https://doi.org/10.3109/02841861003782017},
	doi = {10.3109/02841861003782017},
	abstract = {The NORDCAN database and program (www.ancr.nu) include detailed information and results on cancer incidence, mortality and prevalence in each of the Nordic countries over five decades and has lately been supplemented with predictions of cancer incidence and mortality; future extensions include the incorporation of cancer survival estimates. Material and methods. The data originates from the national cancer registries and causes of death registries in Denmark, Finland, Iceland, Norway, Sweden, and Faroe Islands and is regularly updated. Presently 41 cancer entities are included in the common dataset, and conversions of the original national data according to international rules ensure comparability. Results. With 25 million inhabitants in the Nordic countries, 130 000 incident cancers are reported yearly, alongside nearly 60 000 cancer deaths, with almost a million persons living with a cancer diagnosis. This web-based application is available in English and in each of the five Nordic national languages. It includes comprehensive and easy-to-use descriptive epidemiology tools that provide tabulations and graphs, with further user-specified options available. Discussion. The NORDCAN database aims to provide comparable and timely data to serve the varying needs of policy makers, cancer societies, the public, and journalists, as well as the clinical and research community.},
	number = {5},
	urldate = {2021-11-08},
	journal = {Acta Oncologica},
	author = {Engholm, Gerda and Ferlay, Jacques and Christensen, Niels and Bray, Freddie and Gjerstorff, Marianne L. and Klint, Åsa and Køtlum, Jóanis E. and Ólafsdóttir, Elínborg and Pukkala, Eero and Storm, Hans H.},
	month = jan,
	year = {2010},
	pmid = {20491528},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.3109/02841861003782017},
	pages = {725--736},
}

@article{povl_munk-jorgensen_implementation_1999,
	title = {Implementation of {ICD}-10 in the {Nordic} countries},
	volume = {53},
	issn = {0803-9488},
	url = {https://doi.org/10.1080/080394899426648},
	doi = {10.1080/080394899426648},
	abstract = {ICD-10 was published in 1992. On 1 January 1994 Denmark, as the first of the Nordic countries, changed to ICD-10 as the official classification system in the psychiatric health service system. Finland followed on 1 January 1996, and the last three of the five countries - Sweden, Norway, and Iceland - did so on 1 January 1997. Preparations for publication of guidelines and teaching material, responsibility for introduction, and implementation of teaching programmes are described. On the basis of acquired experiences it is recommended that the five Nordic psychiatric societies establish and finance a forum for collaboration of key persons in psychopathology, diagnosing, classification, and registration. Such a forum should deal with the development of quality, mutual empirical interchange, and support and should contribute to sustaining the high quality level of classification in the five countries.},
	number = {1},
	urldate = {2021-11-08},
	journal = {Nordic Journal of Psychiatry},
	author = {Povl Munk-Jørgensen, Alv A. Dahl, Klaus Lehtinen, Eva Lindström, Kristinn Tomasson, Aksel Bertelsen},
	month = jan,
	year = {1999},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/080394899426648},
	pages = {5--9},
}

@article{jung_bayesian_2021,
	title = {Bayesian {Cox} {Regression} for {Population}-scale {Inference} in {Electronic} {Health} {Records}},
	url = {http://arxiv.org/abs/2106.10057},
	abstract = {The Cox model is an indispensable tool for time-to-event analysis, particularly in biomedical research. However, medicine is undergoing a profound transformation, generating data at an unprecedented scale, which opens new frontiers to study and understand diseases. With the wealth of data collected, new challenges for statistical inference arise, as datasets are often high dimensional, exhibit an increasing number of measurements at irregularly spaced time points, and are simply too large to ﬁt in memory. Many current implementations for time-to-event analysis are ill-suited for these problems as inference is computationally demanding and requires access to the full data at once. Here we propose a Bayesian version for the counting process representation of Cox’s partial likelihood for eﬃcient inference on large-scale datasets with millions of data points and thousands of time-dependent covariates. Through the combination of stochastic variational inference and a reweighting of the log-likelihood, we obtain an approximation for the posterior distribution that factorizes over subsamples of the data, enabling the analysis in big data settings. Crucially, the method produces viable uncertainty estimates for large-scale and high-dimensional datasets. We show the utility of our method through a simulation study and an application to myocardial infarction in the UK Biobank.},
	language = {en},
	urldate = {2021-11-09},
	journal = {arXiv:2106.10057 [stat]},
	author = {Jung, Alexander W. and Gerstung, Moritz},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.10057},
	keywords = {Statistics - Methodology, Statistics - Applications},
	file = {Jung and Gerstung - 2021 - Bayesian Cox Regression for Population-scale Infer.pdf:/Users/alexwjung/Documents/library/storage/ET7ETQTC/Jung and Gerstung - 2021 - Bayesian Cox Regression for Population-scale Infer.pdf:application/pdf},
}

@article{austin_introduction_2016,
	title = {Introduction to the {Analysis} of {Survival} {Data} in the {Presence} of {Competing} {Risks}},
	volume = {133},
	url = {https://www.ahajournals.org/doi/10.1161/circulationaha.115.017719},
	doi = {10.1161/CIRCULATIONAHA.115.017719},
	abstract = {Competing risks occur frequently in the analysis of survival data. A competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. In a study examining time to death attributable to cardiovascular causes, death attributable to noncardiovascular causes is a competing risk. When estimating the crude incidence of outcomes, analysts should use the cumulative incidence function, rather than the complement of the Kaplan-Meier survival function. The use of the Kaplan-Meier survival function results in estimates of incidence that are biased upward, regardless of whether the competing events are independent of one another. When fitting regression models in the presence of competing risks, researchers can choose from 2 different families of models: modeling the effect of covariates on the cause-specific hazard of the outcome or modeling the effect of covariates on the cumulative incidence function. The former allows one to estimate the effect of the covariates on the rate of occurrence of the outcome in those subjects who are currently event free. The latter allows one to estimate the effect of covariates on the absolute risk of the outcome over time. The former family of models may be better suited for addressing etiologic questions, whereas the latter model may be better suited for estimating a patient’s clinical prognosis. We illustrate the application of these methods by examining cause-specific mortality in patients hospitalized with heart failure. Statistical software code in both R and SAS is provided.},
	number = {6},
	urldate = {2021-11-09},
	journal = {Circulation},
	author = {Austin, Peter C. and Lee, Douglas S. and Fine, Jason P.},
	month = feb,
	year = {2016},
	note = {Publisher: American Heart Association},
	keywords = {survival analysis, cumulative incidence function, data interpretation, statistical, incidence, models, statistical, proportional hazards models, risk assessment},
	pages = {601--609},
}

@article{noauthor_discussion_1972,
	title = {Discussion on {Professor} {Cox}'s {Paper}},
	volume = {34},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1972.tb00900.x},
	doi = {10.1111/j.2517-6161.1972.tb00900.x},
	language = {en},
	number = {2},
	urldate = {2021-11-09},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	year = {1972},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1972.tb00900.x},
	pages = {202--220},
}

@article{lin_breslow_2007,
	title = {On the {Breslow} estimator},
	volume = {13},
	issn = {1380-7870},
	doi = {10.1007/s10985-007-9048-y},
	abstract = {In his discussion of Cox's (1972) paper on proportional hazards regression, Breslow (1972) provided the maximum likelihood estimator for the cumulative baseline hazard function. This estimator is commonly used in practice. The estimator has also been highly valuable in the further development of Cox regression and semiparametric inference with censored data. The present paper describes the Breslow estimator and its tremendous impact on the theory and practice of survival analysis.},
	language = {eng},
	number = {4},
	journal = {Lifetime Data Analysis},
	author = {Lin, D. Y.},
	month = dec,
	year = {2007},
	pmid = {17768681},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Likelihood Functions, Models, Statistical},
	pages = {471--480},
}

@article{gayvert_data-driven_2016,
	title = {A {Data}-{Driven} {Approach} to {Predicting} {Successes} and {Failures} of {Clinical} {Trials}},
	volume = {23},
	issn = {24519456},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2451945616302914},
	doi = {10.1016/j.chembiol.2016.07.023},
	abstract = {Over the past decade, the rate of drug attrition due to clinical trial failures has risen substantially. Unfortunately it is difﬁcult to identify compounds that have unfavorable toxicity properties before conducting clinical trials. Inspired by the effective use of sabermetrics in predicting successful baseball players, we sought to use a similar ‘‘moneyball’’ approach that analyzes overlooked features to predict clinical toxicity. We introduce a new data-driven approach (PrOCTOR) that directly predicts the likelihood of toxicity in clinical trials. PrOCTOR integrates the properties of a compound’s targets and its structure to provide a new measure, the PrOCTOR score. Drug target network connectivity and expression levels, along with molecular weight, were identiﬁed as important indicators of adverse clinical events. Our method provides a data-driven, broadly applicable strategy to identify drugs likely to possess manageable toxicity in clinical trials and will help drive the design of therapeutic agents with less toxicity.},
	language = {en},
	number = {10},
	urldate = {2022-01-07},
	journal = {Cell Chemical Biology},
	author = {Gayvert, Kaitlyn M. and Madhukar, Neel S. and Elemento, Olivier},
	month = oct,
	year = {2016},
	pages = {1294--1301},
	file = {Gayvert et al. - 2016 - A Data-Driven Approach to Predicting Successes and.pdf:/Users/alexwjung/Documents/library/storage/BH9EVYEC/Gayvert et al. - 2016 - A Data-Driven Approach to Predicting Successes and.pdf:application/pdf},
}

@article{aalen_nonparametric_1978-1,
	title = {Nonparametric {Inference} for a {Family} of {Counting} {Processes}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-4/Nonparametric-Inference-for-a-Family-of-Counting-Processes/10.1214/aos/1176344247.full},
	doi = {10.1214/aos/1176344247},
	abstract = {Let \${\textbackslash}mathbf\{B\} = (N\_1, {\textbackslash}cdots, N\_k)\$ be a multivariate counting process and let \${\textbackslash}mathscr\{F\}\_t\$ be the collection of all events observed on the time interval \${\textbackslash}lbrack 0, t{\textbackslash}rbrack.\$ The intensity process is given by \${\textbackslash}Lambda\_i(t) = {\textbackslash}lim\_\{h {\textbackslash}downarrow 0\} {\textbackslash}frac\{1\}\{h\}E(N\_i(t + h) - N\_i(t) {\textbackslash}mid {\textbackslash}mathscr\{F\}\_t){\textbackslash}quad i = 1, {\textbackslash}cdots, k.\$ We give an application of the recently developed martingale-based approach to the study of \${\textbackslash}mathbf\{N\}\$ via \${\textbackslash}mathbf\{{\textbackslash}Lambda\}.\$ A statistical model is defined by letting \${\textbackslash}Lambda\_i(t) = {\textbackslash}alpha\_i(t)Y\_i(t), i = 1, {\textbackslash}cdots, k,\$ where \${\textbackslash}mathbf\{{\textbackslash}alpha\} = ({\textbackslash}alpha\_1, {\textbackslash}cdots, {\textbackslash}alpha\_k)\$ is an unknown nonnegative function while \${\textbackslash}mathbf\{Y\} = (Y\_1, {\textbackslash}cdots, Y\_k),\$ together with \${\textbackslash}mathbf\{N\},\$ is a process observable over a certain time interval. Special cases are time-continuous Markov chains on finite state spaces, birth and death processes and models for survival analysis with censored data. The model is termed nonparametric when \${\textbackslash}mathbf\{{\textbackslash}alpha\}\$ is allowed to vary arbitrarily except for regularity conditions. The existence of complete and sufficient statistics for this model is studied. An empirical process estimating \${\textbackslash}beta\_i(t) = {\textbackslash}int{\textasciicircum}t\_0 {\textbackslash}alpha\_i(s) ds\$ is given and studied by means of the theory of stochastic integrals. This empirical process is intended for plotting purposes and it generalizes the empirical cumulative hazard rate from survival analysis and is related to the product limit estimator. Consistency and weak convergence results are given. Tests for comparison of two counting processes, generalizing the two sample rank tests, are defined and studied. Finally, an application to a set of biological data is given.},
	number = {4},
	urldate = {2022-01-11},
	journal = {The Annals of Statistics},
	author = {Aalen, Odd},
	month = jul,
	year = {1978},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60G45, 60H05, 62G05, 62G10, 62M05, 62M99, 62N05, counting process, empirical process, Inference for stochastic processes, intensity process, Martingales, nonparametric theory, point process, stochastic integrals, Survival analysis},
	pages = {701--726},
}

@article{alvares_bayesian_2021-1,
	title = {Bayesian survival analysis with {BUGS}},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8933},
	doi = {10.1002/sim.8933},
	abstract = {Survival analysis is one of the most important fields of statistics in medicine and biological sciences. In addition, the computational advances in the last decades have favored the use of Bayesian methods in this context, providing a flexible and powerful alternative to the traditional frequentist approach. The objective of this article is to summarize some of the most popular Bayesian survival models, such as accelerated failure time, proportional hazards, mixture cure, competing risks, multi-state, frailty, and joint models of longitudinal and survival data. Moreover, an implementation of each presented model is provided using a BUGS syntax that can be run with JAGS from the R programming language. Reference to other Bayesian R-packages is also discussed.},
	language = {en},
	number = {12},
	urldate = {2022-01-11},
	journal = {Statistics in Medicine},
	author = {Alvares, Danilo and Lázaro, Elena and Gómez-Rubio, Virgilio and Armero, Carmen},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8933},
	keywords = {Bayesian inference, JAGS, R-packages, time-to-event analysis},
	pages = {2975--3020},
}

@article{arjas_marked_1984-1,
	title = {A {Marked} {Point} {Process} {Approach} to {Censored} {Failure} {Data} with {Complicated} {Covariates}},
	volume = {11},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/4615959},
	abstract = {Complicated failure time data which can involve, e.g., random covariates, censored observations and multiple failures, is here considered as a sample path of a marked point process (MPP). Our main task is to derive likelihood expressions for parametric statistical models under such general circumstances. To do this, and motivated by concrete examples, we split each marked point into two characteristic parts, called innovation and non-innovation, and then characterize this representation in terms of the statistical model. Technically the paper is based on the martingale approach to point processes.},
	number = {4},
	urldate = {2022-01-11},
	journal = {Scandinavian Journal of Statistics},
	author = {Arjas, Elja and Haara, Pentti},
	year = {1984},
	note = {Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	pages = {193--209},
}

@article{austin_review_2020-1,
	title = {A review of the use of time-varying covariates in the {Fine}-{Gray} subdistribution hazard competing risk regression model},
	volume = {39},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8399},
	doi = {10.1002/sim.8399},
	abstract = {In survival analysis, time-varying covariates are covariates whose value can change during follow-up. Outcomes in medical research are frequently subject to competing risks (events precluding the occurrence of the primary outcome). We review the types of time-varying covariates and highlight the effect of their inclusion in the subdistribution hazard model. External time-dependent covariates are external to the subject, can effect the failure process, but are not otherwise involved in the failure mechanism. Internal time-varying covariates are measured on the subject, can effect the failure process directly, and may also be impacted by the failure mechanism. In the absence of competing risks, a consequence of including internal time-dependent covariates in the Cox model is that one cannot estimate the survival function or the effect of covariates on the survival function. In the presence of competing risks, the inclusion of internal time-varying covariates in a subdistribution hazard model results in the loss of the ability to estimate the cumulative incidence function (CIF) or the effect of covariates on the CIF. Furthermore, the definition of the risk set for the subdistribution hazard function can make defining internal time-varying covariates difficult or impossible. We conducted a review of the use of time-varying covariates in subdistribution hazard models in articles published in the medical literature in 2015 and in the first 5 months of 2019. Seven percent of articles published included a time-varying covariate. Several inappropriately described a time-varying covariate as having an association with the risk of the outcome.},
	language = {en},
	number = {2},
	urldate = {2022-01-11},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Latouche, Aurélien and Fine, Jason P.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8399},
	keywords = {competing risks, subdistribution hazard model, survival analysis, time-varying covariate},
	pages = {103--113},
}

@inproceedings{bardenet_towards_2014,
	address = {Bejing, China},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Towards scaling up {Markov} chain {Monte} {Carlo}: an adaptive subsampling approach},
	volume = {32},
	url = {https://proceedings.mlr.press/v32/bardenet14.html},
	abstract = {Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICML’14), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bardenet, Rémi and Doucet, Arnaud and Holmes, Chris},
	editor = {Xing, Eric P. and Jebara, Tony},
	month = jun,
	year = {2014},
	note = {Issue: 1},
	pages = {405--413},
}

@inproceedings{betancourt_fundamental_2015-1,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Fundamental} {Incompatibility} of {Scalable} {Hamiltonian} {Monte} {Carlo} and {Naive} {Data} {Subsampling}},
	volume = {37},
	url = {https://proceedings.mlr.press/v37/betancourt15.html},
	abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Betancourt, Michael},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	pages = {533--540},
}

@book{who_international_2015,
	edition = {10th revision, Fifth edition, 2016},
	title = {International statistical classification of diseases and related health problems},
	publisher = {World Health Organization},
	author = {WHO, World Health Organization},
	year = {2015},
	note = {Type: Publications},
}

@article{chen_china_2011-1,
	title = {China {Kadoorie} {Biobank} of 0.5 million people: survey methods, baseline characteristics and long-term follow-up},
	volume = {40},
	issn = {1464-3685},
	shorttitle = {China {Kadoorie} {Biobank} of 0.5 million people},
	doi = {10.1093/ije/dyr120},
	abstract = {BACKGROUND: Large blood-based prospective studies can provide reliable assessment of the complex interplay of lifestyle, environmental and genetic factors as determinants of chronic disease.
METHODS: The baseline survey of the China Kadoorie Biobank took place during 2004-08 in 10 geographically defined regions, with collection of questionnaire data, physical measurements and blood samples. Subsequently, a re-survey of 25,000 randomly selected participants was done (80\% responded) using the same methods as in the baseline. All participants are being followed for cause-specific mortality and morbidity, and for any hospital admission through linkages with registries and health insurance (HI) databases.
RESULTS: Overall, 512,891 adults aged 30-79 years were recruited, including 41\% men, 56\% from rural areas and mean age was 52 years. The prevalence of ever-regular smoking was 74\% in men and 3\% in women. The mean blood pressure was 132/79 mmHg in men and 130/77 mmHg in women. The mean body mass index (BMI) was 23.4 kg/m(2) in men and 23.8 kg/m(2) in women, with only 4\% being obese ({\textgreater}30 kg/m(2)), and 3.2\% being diabetic. Blood collection was successful in 99.98\% and the mean delay from sample collection to processing was 10.6 h. For each of the main baseline variables, there is good reproducibility but large heterogeneity by age, sex and study area. By 1 January 2011, over 10,000 deaths had been recorded, with 91\% of surviving participants already linked to HI databases.
CONCLUSION: This established large biobank will be a rich and powerful resource for investigating genetic and non-genetic causes of many common chronic diseases in the Chinese population.},
	language = {eng},
	number = {6},
	journal = {International Journal of Epidemiology},
	author = {Chen, Zhengming and Chen, Junshi and Collins, Rory and Guo, Yu and Peto, Richard and Wu, Fan and Li, Liming and {China Kadoorie Biobank (CKB) collaborative group}},
	month = dec,
	year = {2011},
	pmid = {22158673},
	pmcid = {PMC3235021},
	keywords = {Adult, Age Factors, Aged, Blood Pressure, Body Mass Index, Body Weights and Measures, China, Chronic Disease, Data Collection, Databases, Factual, Databases, Genetic, Environment, Female, Follow-Up Studies, Genetic Predisposition to Disease, Genetic Testing, Health Behavior, Humans, Life Style, Male, Middle Aged, Prospective Studies, Residence Characteristics, Rural Population, Sex Factors, Smoking, Socioeconomic Factors, Surveys and Questionnaires},
	pages = {1652--1666},
}

@article{carey_prevention_2018-1,
	title = {Prevention, {Detection}, {Evaluation}, and {Management} of {High} {Blood} {Pressure} in {Adults}: {Synopsis} of the 2017 {American} {College} of {Cardiology}/{American} {Heart} {Association} {Hypertension} {Guideline}},
	volume = {168},
	issn = {1539-3704},
	shorttitle = {Prevention, {Detection}, {Evaluation}, and {Management} of {High} {Blood} {Pressure} in {Adults}},
	doi = {10.7326/M17-3203},
	abstract = {Description: In November 2017, the American College of Cardiology (ACC) and the American Heart Association (AHA) released a clinical practice guideline for the prevention, detection, evaluation, and treatment of high blood pressure (BP) in adults. This article summarizes the major recommendations.
Methods: In 2014, the ACC and the AHA appointed a multidisciplinary committee to update previous reports of the Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure. The committee reviewed literature and commissioned systematic reviews and meta-analyses on out-of-office BP monitoring, the optimal target for BP lowering, the comparative benefits and harms of different classes of antihypertensive agents, and the comparative benefits and harms of initiating therapy with a single antihypertensive agent or a combination of 2 agents.
Recommendations: This article summarizes key recommendations in the following areas: BP classification, BP measurement, screening for secondary hypertension, nonpharmacologic therapy, BP thresholds and cardiac risk estimation to guide drug treatment, treatment goals (general and for patients with diabetes mellitus, chronic kidney disease, and advanced age), choice of initial drug therapy, resistant hypertension, and strategies to improve hypertension control.},
	language = {eng},
	number = {5},
	journal = {Annals of Internal Medicine},
	author = {Carey, Robert M. and Whelton, Paul K. and {2017 ACC/AHA Hypertension Guideline Writing Committee}},
	month = mar,
	year = {2018},
	pmid = {29357392},
	keywords = {Adult, Antihypertensive Agents, Blood Pressure Determination, Comorbidity, Humans, Hypertension, Mass Screening, Secondary Prevention},
	pages = {351--358},
}

@article{breslow_covariance_1974-1,
	title = {Covariance analysis of censored survival data},
	volume = {30},
	issn = {0006-341X},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Breslow, N.},
	month = mar,
	year = {1974},
	pmid = {4813387},
	keywords = {Age Factors, Child, Preschool, Dactinomycin, Humans, Leukemia, Lymphoid, Leukocyte Count, Mercaptopurine, Methotrexate, Models, Biological, Nitrogen Mustard Compounds, Prognosis, Regression Analysis, Remission, Spontaneous, Statistics as Topic, Time Factors},
	pages = {89--99},
}

@article{clift_living_2020-1,
	title = {Living risk prediction algorithm ({QCOVID}) for risk of hospital admission and mortality from coronavirus 19 in adults: national derivation and validation cohort study},
	volume = {371},
	issn = {1756-1833},
	shorttitle = {Living risk prediction algorithm ({QCOVID}) for risk of hospital admission and mortality from coronavirus 19 in adults},
	doi = {10.1136/bmj.m3731},
	abstract = {OBJECTIVE: To derive and validate a risk prediction algorithm to estimate hospital admission and mortality outcomes from coronavirus disease 2019 (covid-19) in adults.
DESIGN: Population based cohort study.
SETTING AND PARTICIPANTS: QResearch database, comprising 1205 general practices in England with linkage to covid-19 test results, Hospital Episode Statistics, and death registry data. 6.08 million adults aged 19-100 years were included in the derivation dataset and 2.17 million in the validation dataset. The derivation and first validation cohort period was 24 January 2020 to 30 April 2020. The second temporal validation cohort covered the period 1 May 2020 to 30 June 2020.
MAIN OUTCOME MEASURES: The primary outcome was time to death from covid-19, defined as death due to confirmed or suspected covid-19 as per the death certification or death occurring in a person with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in the period 24 January to 30 April 2020. The secondary outcome was time to hospital admission with confirmed SARS-CoV-2 infection. Models were fitted in the derivation cohort to derive risk equations using a range of predictor variables. Performance, including measures of discrimination and calibration, was evaluated in each validation time period.
RESULTS: 4384 deaths from covid-19 occurred in the derivation cohort during follow-up and 1722 in the first validation cohort period and 621 in the second validation cohort period. The final risk algorithms included age, ethnicity, deprivation, body mass index, and a range of comorbidities. The algorithm had good calibration in the first validation cohort. For deaths from covid-19 in men, it explained 73.1\% (95\% confidence interval 71.9\% to 74.3\%) of the variation in time to death (R2); the D statistic was 3.37 (95\% confidence interval 3.27 to 3.47), and Harrell's C was 0.928 (0.919 to 0.938). Similar results were obtained for women, for both outcomes, and in both time periods. In the top 5\% of patients with the highest predicted risks of death, the sensitivity for identifying deaths within 97 days was 75.7\%. People in the top 20\% of predicted risk of death accounted for 94\% of all deaths from covid-19.
CONCLUSION: The QCOVID population based risk algorithm performed well, showing very high levels of discrimination for deaths and hospital admissions due to covid-19. The absolute risks presented, however, will change over time in line with the prevailing SARS-C0V-2 infection rate and the extent of social distancing measures in place, so they should be interpreted with caution. The model can be recalibrated for different time periods, however, and has the potential to be dynamically updated as the pandemic evolves.},
	language = {eng},
	journal = {BMJ (Clinical research ed.)},
	author = {Clift, Ash K. and Coupland, Carol A. C. and Keogh, Ruth H. and Diaz-Ordaz, Karla and Williamson, Elizabeth and Harrison, Ewen M. and Hayward, Andrew and Hemingway, Harry and Horby, Peter and Mehta, Nisha and Benger, Jonathan and Khunti, Kamlesh and Spiegelhalter, David and Sheikh, Aziz and Valabhji, Jonathan and Lyons, Ronan A. and Robson, John and Semple, Malcolm G. and Kee, Frank and Johnson, Peter and Jebb, Susan and Williams, Tony and Hippisley-Cox, Julia},
	month = oct,
	year = {2020},
	pmid = {33082154},
	pmcid = {PMC7574532},
	keywords = {Adult, Aged, 80 and over, Algorithms, Betacoronavirus, Clinical Decision Rules, Cohort Studies, Coronavirus Infections, COVID-19, Databases, Factual, England, Female, Hospitalization, Humans, Male, Mortality, Pandemics, Pneumonia, Viral, Prognosis, Reproducibility of Results, Risk Assessment, SARS-CoV-2},
	pages = {m3731},
}

@article{cox_general_1968-1,
	title = {A {General} {Definition} of {Residuals}},
	volume = {30},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984505},
	abstract = {Residuals are usually defined in connection with linear models. Here a more general definition is given and some asymptotic properties found. Some illustrative examples are discussed, including a regression problem involving exponentially distributed errors and some problems concerning Poisson and binomially distributed observations.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R. and Snell, E. J.},
	year = {1968},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {248--275},
}

@article{crowley_covariance_1977-1,
	title = {Covariance {Analysis} of {Heart} {Transplant} {Survival} {Data}},
	volume = {72},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286902},
	doi = {10.2307/2286902},
	abstract = {This paper presents a number of analyses to assess the effects of various covariates on the survival of patients in the Stanford Heart Transplantation Program. The data have been updated from previously published versions and include some additional covariates, such as measures of tissue typing. The methods used allow for simultaneous investigation of several covariates and provide estimates of the relative risk of transplantation as well as significance tests.},
	number = {357},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Crowley, John and Hu, Marie},
	year = {1977},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {27--36},
}

@article{davison_deviance_1989-1,
	title = {Deviance residuals and normal scores plots},
	volume = {76},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/76.2.211},
	doi = {10.1093/biomet/76.2.211},
	abstract = {We discuss the use of normal order statistics plots, based on deviance residuals, to check distributional assumptions in regression models. Continuous and discrete error distributions are considered, as are censored data. Misspecified error distributions and discrimination between competing models are discussed, with an example.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Davison, A. C. and Gigli, A.},
	month = jun,
	year = {1989},
	pages = {211--221},
}

@article{dawber_epidemiological_1951-1,
	title = {Epidemiological approaches to heart disease: the {Framingham} {Study}},
	volume = {41},
	issn = {0002-9572},
	shorttitle = {Epidemiological approaches to heart disease},
	doi = {10.2105/ajph.41.3.279},
	language = {eng},
	number = {3},
	journal = {American Journal of Public Health and the Nation's Health},
	author = {Dawber, T. R. and Meadors, G. F. and Moore, F. E.},
	month = mar,
	year = {1951},
	pmid = {14819398},
	pmcid = {PMC1525365},
	keywords = {HEART DISEASE, Heart Diseases, Humans},
	pages = {279--281},
}

@article{egeberg_assessment_2016-1,
	title = {Assessment of the risk of cardiovascular disease in patients with rosacea},
	volume = {75},
	issn = {1097-6787},
	doi = {10.1016/j.jaad.2016.02.1158},
	abstract = {BACKGROUND: Recent studies have shown a higher prevalence of cardiovascular (CV) risk factors in patients with rosacea. However, it remains unknown whether rosacea represents an independent CV risk factor.
OBJECTIVE: We evaluated the risk of myocardial infarction, stroke, CV death, major adverse CV events, and all-cause mortality, respectively.
METHODS: Between January 1, 1997, and December 31, 2012, a total of 4948 patients with rosacea were identified and matched with 23,823 control subjects. We used Poisson regression to calculate incidence rate ratios.
RESULTS: Adjusted incidence rate ratios were 0.75 (95\% confidence intervals [CI] 0.57-1.00) for myocardial infarction, 1.08 (95\% CI 0.86-1.35) for ischemic stroke, 1.01 (95\% CI 0.61-1.67) for hemorrhagic stroke, 0.99 (95\% CI 0.80-1.24) for CV death, 0.99 (95\% CI 0.86-1.15) for major adverse CV events, and 0.95 (95\% CI 0.85-1.06) for all-cause mortality.
LIMITATIONS: We were unable to distinguish between the different subtypes and severities of rosacea.
CONCLUSIONS: In this population-based study, rosacea was not associated with increased risk of adverse CV outcomes or death.},
	language = {eng},
	number = {2},
	journal = {Journal of the American Academy of Dermatology},
	author = {Egeberg, Alexander and Hansen, Peter R. and Gislason, Gunnar H. and Thyssen, Jacob P.},
	month = aug,
	year = {2016},
	pmid = {27444070},
	keywords = {cardiovascular disease, Cardiovascular Diseases, Case-Control Studies, Cause of Death, Comorbidity, epidemiology, Female, Humans, Incidence, Male, Middle Aged, Poisson Distribution, risk factors, Risk Factors, rosacea, Rosacea},
	pages = {336--339},
}

@article{fine_proportional_1999-1,
	title = {A {Proportional} {Hazards} {Model} for the {Subdistribution} of a {Competing} {Risk}},
	volume = {94},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10474144},
	doi = {10.1080/01621459.1999.10474144},
	abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.},
	number = {446},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Fine, Jason P. and Gray, Robert J.},
	month = jun,
	year = {1999},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10474144},
	keywords = {Hazard of subdistribution, Martingale, Partial likelihood, Transformation model},
	pages = {496--509},
}

@article{friedman_regularization_2010-1,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	language = {eng},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
}

@article{gabry_visualization_2019-1,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {1467-985X},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Place: United Kingdom
Publisher: Wiley-Blackwell Publishing Ltd.},
	keywords = {Bayesian Analysis, Statistical Data, Statistical Norms},
	pages = {389--402},
}

@article{halabi_score_2020-1,
	title = {Score and deviance residuals based on the full likelihood approach in survival analysis},
	volume = {19},
	issn = {1539-1612},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2047},
	doi = {10.1002/pst.2047},
	abstract = {Assuming the proportional hazards model and non-informative censoring, the full likelihood approach is used to obtain two new residuals. The first residual is based on the ideas used in obtaining score-type residuals similar to the partial likelihood approach. The second type of residual is based on the concept of deviance residuals. Extensive simulations are conducted to compare the performance of the residuals from the full likelihood-based approach with those of the partial likelihood method. We demonstrate through simulation studies that the full likelihood-based residuals are more efficient than their partial likelihood counterpart in identifying potential outliers when the censoring proportion is high. The graphical techniques are used to illustrate the applications of these residuals using some examples.},
	language = {en},
	number = {6},
	urldate = {2022-01-11},
	journal = {Pharmaceutical Statistics},
	author = {Halabi, Susan and Dutta, Sandipan and Wu, Yuan and Liu, Aiyi},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.2047},
	keywords = {deviance residuals, full likelihood, non-informative censoring, partial likelihood, proportional hazards, score-type residuals},
	pages = {940--954},
}

@article{hippisley-cox_predicting_2021-1,
	title = {Predicting the risk of prostate cancer in asymptomatic men: a cohort study to develop and validate a novel algorithm},
	volume = {71},
	issn = {1478-5242},
	shorttitle = {Predicting the risk of prostate cancer in asymptomatic men},
	doi = {10.3399/bjgp20X714137},
	abstract = {BACKGROUND: Diagnosis of prostate cancer at an early stage can potentially identify tumours when intervention may improve treatment options and survival.
AIM: To develop and validate an equation to predict absolute risk of prostate cancer in asymptomatic men with prostate specific antigen (PSA) tests in primary care.
DESIGN AND SETTING: Cohort study using data from English general practices, held in the QResearch database.
METHOD: Routine data were collected from 1098 QResearch English general practices linked to mortality, hospital, and cancer records for model development. Two separate sets of practices were used for validation. In total, there were 844 455 men aged 25-84 years with PSA tests recorded who were free of prostate cancer at baseline in the derivation cohort; the validation cohorts comprised 292 084 and 316 583 men. The primary outcome was incident prostate cancer. Cox proportional hazards models were used to derive 10-year risk equations. Measures of performance were determined in both validation cohorts.
RESULTS: There were 40 821 incident cases of prostate cancer in the derivation cohort. The risk equation included PSA level, age, deprivation, ethnicity, smoking status, serious mental illness, diabetes, BMI, and family history of prostate cancer. The risk equation explained 70.4\% (95\% CI = 69.2 to 71.6) of the variation in time to diagnosis of prostate cancer (R 2) (D statistic 3.15, 95\% CI = 3.06 to 3.25; Harrell's C-index 0.917, 95\% CI = 0.915 to 0.919). Two-step approach had higher sensitivity than a fixed PSA threshold at identifying prostate cancer cases (identifying 68.2\% versus 43.9\% of cases), high-grade cancers (49.2\% versus 40.3\%), and deaths (67.0\% versus 31.5\%).
CONCLUSION: The risk equation provided valid measures of absolute risk and had higher sensitivity for incident prostate cancer, high-grade cancers, and prostate cancer mortality than a simple approach based on age and PSA threshold.},
	language = {eng},
	number = {706},
	journal = {The British Journal of General Practice: The Journal of the Royal College of General Practitioners},
	author = {Hippisley-Cox, Julia and Coupland, Carol},
	month = may,
	year = {2021},
	pmid = {33875417},
	pmcid = {PMC8087311},
	keywords = {Algorithms, cohort studies, Cohort Studies, Humans, Male, primary health care, Prospective Studies, prostate cancer, prostate-specific antigen, Prostate-Specific Antigen, Prostatic Neoplasms, Risk Assessment, Risk Factors, risk prediction},
	pages = {e364--e371},
}

@article{hippisley-cox_development_2017-1,
	title = {Development and validation of {QRISK3} risk prediction algorithms to estimate future risk of cardiovascular disease: prospective cohort study},
	volume = {357},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions 				.  					This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {1756-1833},
	shorttitle = {Development and validation of {QRISK3} risk prediction algorithms to estimate future risk of cardiovascular disease},
	url = {https://www.bmj.com/content/357/bmj.j2099},
	doi = {10.1136/bmj.j2099},
	abstract = {Objectives To develop and validate updated QRISK3 prediction algorithms to estimate the 10 year risk of cardiovascular disease in women and men accounting for potential new risk factors.
Design Prospective open cohort study.
Setting General practices in England providing data for the QResearch database.
Participants 1309 QResearch general practices in England: 981 practices were used to develop the scores and a separate set of 328 practices were used to validate the scores. 7.89 million patients aged 25-84 years were in the derivation cohort and 2.67 million patients in the validation cohort. Patients were free of cardiovascular disease and not prescribed statins at baseline.
Methods Cox proportional hazards models in the derivation cohort to derive separate risk equations in men and women for evaluation at 10 years. Risk factors considered included those already in QRISK2 (age, ethnicity, deprivation, systolic blood pressure, body mass index, total cholesterol: high density lipoprotein cholesterol ratio, smoking, family history of coronary heart disease in a first degree relative aged less than 60 years, type 1 diabetes, type 2 diabetes, treated hypertension, rheumatoid arthritis, atrial fibrillation, chronic kidney disease (stage 4 or 5)) and new risk factors (chronic kidney disease (stage 3, 4, or 5), a measure of systolic blood pressure variability (standard deviation of repeated measures), migraine, corticosteroids, systemic lupus erythematosus (SLE), atypical antipsychotics, severe mental illness, and HIV/AIDs). We also considered erectile dysfunction diagnosis or treatment in men. Measures of calibration and discrimination were determined in the validation cohort for men and women separately and for individual subgroups by age group, ethnicity, and baseline disease status.
Main outcome measures Incident cardiovascular disease recorded on any of the following three linked data sources: general practice, mortality, or hospital admission records.
Results 363 565 incident cases of cardiovascular disease were identified in the derivation cohort during follow-up arising from 50.8 million person years of observation. All new risk factors considered met the model inclusion criteria except for HIV/AIDS, which was not statistically significant. The models had good calibration and high levels of explained variation and discrimination. In women, the algorithm explained 59.6\% of the variation in time to diagnosis of cardiovascular disease (R2, with higher values indicating more variation), and the D statistic was 2.48 and Harrell’s C statistic was 0.88 (both measures of discrimination, with higher values indicating better discrimination). The corresponding values for men were 54.8\%, 2.26, and 0.86. Overall performance of the updated QRISK3 algorithms was similar to the QRISK2 algorithms.
Conclusion Updated QRISK3 risk prediction models were developed and validated. The inclusion of additional clinical variables in QRISK3 (chronic kidney disease, a measure of systolic blood pressure variability (standard deviation of repeated measures), migraine, corticosteroids, SLE, atypical antipsychotics, severe mental illness, and erectile dysfunction) can help enable doctors to identify those at most risk of heart disease and stroke.},
	language = {en},
	urldate = {2022-01-11},
	journal = {BMJ},
	author = {Hippisley-Cox, Julia and Coupland, Carol and Brindle, Peter},
	month = may,
	year = {2017},
	pmid = {28536104},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	pages = {j2099},
}

@article{ibrahim_bayesian_1999-1,
	title = {Bayesian {Variable} {Selection} for {Proportional} {Hazards} {Models}},
	volume = {27},
	issn = {0319-5724},
	url = {https://www.jstor.org/stable/3316126},
	doi = {10.2307/3316126},
	abstract = {The authors consider the problem of Bayesian variable selection for proportional hazards regression models with right censored data. They propose a semi-parametric approach in which a nonparametric prior is specified for the baseline hazard rate and a fully parametric prior is specified for the regression coefficients. For the baseline hazard, they use a discrete gamma process prior, and for the regression coefficients and the model space, they propose a semi-automatic parametric informative prior specification that focuses on the observables rather than the parameters. To implement the methodology, they propose a Markov chain Monte Carlo method to compute the posterior model probabilities. Examples using simulated and real data are given to demonstrate the methodology. /// Les auteurs abordent d'un point de vue bayésien le problème de la sélection de variables dans les modèles de régression des risques proportionnels en présence de censure à droite. Ils proposent une approche semi-paramétrique dans laquelle la loi a priori du taux de base est non paramétrique, mais celle des coefficients de régression est complètement paramétrique. L'information concernant le taux de base est représentée par la loi a priori issue d'un processus gamma discret; quant à la loi a priori des paramètres du modèle de régression, elle est choisie dans une classe de lois paramétriques au moyen d'une procédure semi-automatique centrée sur les données plutôt que sur les paramètres. La mise à jour de l'information se fait au moyen d'un algorithme de Monte-Carlo à chaîne de Markov. Des données réelles et simulées permettent d'illustrer la méthode.},
	number = {4},
	urldate = {2022-01-11},
	journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and MacEachern, Steven N.},
	year = {1999},
	note = {Publisher: [Statistical Society of Canada, Wiley]},
	pages = {701--717},
}

@article{kannel_lessons_1976-1,
	title = {Some lessons in cardiovascular epidemiology from {Framingham}},
	volume = {37},
	issn = {0002-9149},
	doi = {10.1016/0002-9149(76)90323-4},
	abstract = {Epidemiologic investigations have provided a portrait of the potential candidate for coronary heart disease. This is important because studies of the evolution of coronary disease in the general population reveal that it is a common disease that frequently attacks without warning, can be silent in its most dangerous form and can present with sudden death as the first symptom. Progress in identifyin- persons in jeopardy and the factors needing correction makes it theoretically possible to interrupt the chain of factors that eventuate in this disease. Coronary disease does not really begin with crushing chest pain, pulmonary edema, shock, angina or ventricular fibrillation, but rather with more subtle signs like a poor coronary risk profile. The risk factors can be treated quantitatively as ingredients of a cardiovascular risk profile and their joint effect estimated. An efficient practicable set of variables for this purpose is a casual blood test for cholesterol and sugar, a blood pressure determination, an electrocardiogram and a cigarette smoking history. With this set of variables the risk of coronary heart diseases can be estimated over a 30-fold range and 10 percent of the asymptomatic population identified in whom 25 percent of the coronary disease, 40 percent of the occlusive peripheral arterial disease and 50 percent of the strokes and congestive heart failure will evolve. The periodic use of the electrocardiogram at rest and after exercise in persons with a poor risk profile can demonstrate persons with asymptomatic ischemic cardiomyopathy due to advanced coronary artery disease. Most cases of angina pectoris or myocardial infarction represent medical failures; the conditions should have been detected years earlier for preventive management. About 30 percent of patients with infraction will shortly experience new angina, have an annual death rate of 4 percent and a fourfold increased risk of sudden death. Reinfarction will occur at an annual rate of 6 percent, and half the recurrences will be fatal. Congestive heart failure must be expected at 10 times and strokes at 5 times the rate found in the general population. Although no major innovations are required to identify candidates for coronary disease and to estimate their risk, we have much to learn about motivating changes in behavior to control risk factors. Approaches to prevention of coronary heart disease include public health measures to alter the ecology in favor of cardiovascular health, preventive medicine directed at highly vulnerable candidates and hygienic measures initiated by an informed public in its own behalf.},
	language = {eng},
	number = {2},
	journal = {The American Journal of Cardiology},
	author = {Kannel, W. B.},
	month = feb,
	year = {1976},
	pmid = {1246956},
	keywords = {Adult, Age Factors, Aged, Cardiovascular Diseases, Coronary Disease, Electrocardiography, Environment, Female, Humans, Hypertension, Lipids, Male, Massachusetts, Methods, Middle Aged, Population Surveillance, Smoking},
	pages = {269--282},
}

@article{laurie_surgical_1989-1,
	title = {Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. {The} {North} {Central} {Cancer} {Treatment} {Group} and the {Mayo} {Clinic}},
	volume = {7},
	issn = {0732-183X},
	shorttitle = {Surgical adjuvant therapy of large-bowel carcinoma},
	doi = {10.1200/JCO.1989.7.10.1447},
	abstract = {A total of 401 eligible patients with resected stages B and C colorectal carcinoma were randomly assigned to no-further therapy or to adjuvant treatment with either levamisole alone, 150 mg/d for 3 days every 2 weeks for 1 year, or levamisole plus fluorouracil (5-FU), 450 mg/m2/d intravenously (IV) for 5 days and beginning at 28 days, 450 mg/m2 weekly for 1 year. Levamisole plus 5-FU, and to a lesser extent levamisole alone, reduced cancer recurrence in comparison with no adjuvant therapy. These differences, after correction for imbalances in prognostic variables, were only suggestive for levamisole alone (P = .05) but quite significant for levamisole plus 5-FU (P = .003). Whereas both treatment regimens were associated with overall improvements in survival, these improvements reached borderline significance only for stage C patients treated with levamisole plus 5-FU (P = .03). Therapy was clinically tolerable with either regimen and severe toxicity was uncommon. These promising results have led to a large national intergroup confirmatory trial currently in progress.},
	language = {eng},
	number = {10},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Laurie, J. A. and Moertel, C. G. and Fleming, T. R. and Wieand, H. S. and Leigh, J. E. and Rubin, J. and McCormack, G. W. and Gerstner, J. B. and Krook, J. E. and Malliard, J.},
	month = oct,
	year = {1989},
	pmid = {2778478},
	keywords = {Adult, Aged, Aged, 80 and over, Colorectal Neoplasms, Female, Fluorouracil, Humans, Levamisole, Lymphatic Metastasis, Male, Middle Aged, Neoplasm Recurrence, Local, Neoplasm Staging, Neoplasms, Multiple Primary, Patient Compliance, Random Allocation},
	pages = {1447--1456},
}

@article{loprinzi_prospective_1994-1,
	title = {Prospective evaluation of prognostic variables from patient-completed questionnaires. {North} {Central} {Cancer} {Treatment} {Group}},
	volume = {12},
	issn = {0732-183X},
	doi = {10.1200/JCO.1994.12.3.601},
	abstract = {PURPOSE: This study was developed to determine whether descriptive information from a patient-completed questionnaire could provide prognostic information that was independent from that already obtained by the patient's physician.
PATIENTS AND METHODS: An initial detailed questionnaire was administered to approximately 150 patients with advanced cancer. This questionnaire was subsequently revised and given to a total of 1,115 patients with advanced colorectal or lung cancer. Univariate and multivariate analyses were performed to evaluate the data from these questionnaires.
RESULTS: A total of 36 variables showed statistically significant prognostic information for survival in univariate analyses, even though many of these variables were associated with only a minimal increase in risk. A multivariate analysis demonstrated that there was a high correlation between many variables. Three major groups of variables became apparent as providing strong prognostic information. These included the following: (1) a physician's assessment of performance status (PS); (2) a patient's assessment of their own PS; and (3) a nutritional factor such as appetite, caloric intake, or overall food intake.
CONCLUSION: Data generated by a patient-completed questionnaire can provide important prognostic information independent from that obtained by other physician-determined prognostic factors.},
	language = {eng},
	number = {3},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Loprinzi, C. L. and Laurie, J. A. and Wieand, H. S. and Krook, J. E. and Novotny, P. J. and Kugler, J. W. and Bartel, J. and Law, M. and Bateman, M. and Klatt, N. E.},
	month = mar,
	year = {1994},
	pmid = {8120560},
	keywords = {Analysis of Variance, Colorectal Neoplasms, Humans, Karnofsky Performance Status, Lung Neoplasms, Neoplasms, Prognosis, Proportional Hazards Models, Prospective Studies, Severity of Illness Index, Surveys and Questionnaires},
	pages = {601--607},
}

@article{mahmood_framingham_2014-1,
	title = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease: a historical perspective},
	volume = {383},
	issn = {1474-547X},
	shorttitle = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease},
	doi = {10.1016/S0140-6736(13)61752-3},
	abstract = {On Sept 29, 2013, the Framingham Heart Study will celebrate 65 years since the examination of the first volunteer in 1948. During this period, the study has provided substantial insight into the epidemiology and risk factors of cardiovascular disease. The origins of the study are closely linked to the cardiovascular health of President Franklin D Roosevelt and his premature death from hypertensive heart disease and stroke in 1945. In this Review we describe the events leading to the foundation of the Framingham Heart Study, and provide a brief historical overview of selected contributions from the study.},
	language = {eng},
	number = {9921},
	journal = {Lancet (London, England)},
	author = {Mahmood, Syed S. and Levy, Daniel and Vasan, Ramachandran S. and Wang, Thomas J.},
	month = mar,
	year = {2014},
	pmid = {24084292},
	pmcid = {PMC4159698},
	keywords = {Cardiovascular Diseases, Epidemiology, History, 20th Century, History, 21st Century, Humans, Longitudinal Studies, Massachusetts, Research Support as Topic, Risk Factors},
	pages = {999--1008},
}

@article{nelson_theory_1972-1,
	title = {Theory and {Applications} of {Hazard} {Plotting} for {Censored} {Failure} {Data}},
	volume = {14},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1267144},
	doi = {10.2307/1267144},
	abstract = {This paper presents theory and applications of a simple graphical method, called hazard plotting, for the analysis of multiply censored life data consisting of failure times of failed units intermixed with running times on unfailed units. Applications of the method are given for multiply censored data on service life of equipment, for strength data on an item with different failure modes, and for biological data multiply censored on both sides from paired comparisons. Theory for the hazard plotting method, which is based on the hazard function of a distribution, is developed from the properties of order statistics from Type II multiply censored samples.},
	number = {4},
	urldate = {2022-01-11},
	journal = {Technometrics},
	author = {Nelson, Wayne},
	year = {1972},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {945--966},
}

@article{nikooienejad_bayesian_2020-1,
	title = {Bayesian variable selection for survival data using inverse moment priors},
	volume = {14},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-14/issue-2/Bayesian-variable-selection-for-survival-data-using-inverse-moment-priors/10.1214/20-AOAS1325.full},
	doi = {10.1214/20-AOAS1325},
	abstract = {Efficient variable selection in high-dimensional cancer genomic studies is critical for discovering genes associated with specific cancer types and for predicting response to treatment. Censored survival data is prevalent in such studies. In this article we introduce a Bayesian variable selection procedure that uses a mixture prior composed of a point mass at zero and an inverse moment prior in conjunction with the partial likelihood defined by the Cox proportional hazard model. The procedure is implemented in the R package BVSNLP, which supports parallel computing and uses a stochastic search method to explore the model space. Bayesian model averaging is used for prediction. The proposed algorithm provides better performance than other variable selection procedures in simulation studies and appears to provide more consistent variable selection when applied to actual genomic datasets.},
	number = {2},
	urldate = {2022-01-11},
	journal = {The Annals of Applied Statistics},
	author = {Nikooienejad, Amir and Wang, Wenyi and Johnson, Valen E.},
	month = jun,
	year = {2020},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayesian variable selection, Cancer genomics, Cox proportional hazard model, High-dimensional data, nonlocal prior, survival data analysis},
	pages = {809--828},
}

@article{piironen_sparsity_2017-1,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full},
	doi = {10.1214/17-EJS1337SI},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	month = jan,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62F15, Bayesian inference, horseshoe prior, shrinkage priors, Sparse estimation},
	pages = {5018--5051},
}

@article{qian_fast_2020,
	title = {A fast and scalable framework for large-scale and ultrahigh-dimensional sparse regression with application to the {UK} {Biobank}},
	volume = {16},
	issn = {1553-7404},
	url = {https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1009141},
	doi = {10.1371/journal.pgen.1009141},
	abstract = {The UK Biobank is a very large, prospective population-based cohort study across the United Kingdom. It provides unprecedented opportunities for researchers to investigate the relationship between genotypic information and phenotypes of interest. Multiple regression methods, compared with genome-wide association studies (GWAS), have already been showed to greatly improve the prediction performance for a variety of phenotypes. In the high-dimensional settings, the lasso, since its first proposal in statistics, has been proved to be an effective method for simultaneous variable selection and estimation. However, the large-scale and ultrahigh dimension seen in the UK Biobank pose new challenges for applying the lasso method, as many existing algorithms and their implementations are not scalable to large applications. In this paper, we propose a computational framework called batch screening iterative lasso (BASIL) that can take advantage of any existing lasso solver and easily build a scalable solution for very large data, including those that are larger than the memory size. We introduce snpnet, an R package that implements the proposed algorithm on top of glmnet and optimizes for single nucleotide polymorphism (SNP) datasets. It currently supports ℓ1-penalized linear model, logistic regression, Cox model, and also extends to the elastic net with ℓ1/ℓ2 penalty. We demonstrate results on the UK Biobank dataset, where we achieve competitive predictive performance for all four phenotypes considered (height, body mass index, asthma, high cholesterol) using only a small fraction of the variants compared with other established polygenic risk score methods.},
	language = {en},
	number = {10},
	urldate = {2022-01-11},
	journal = {PLOS Genetics},
	author = {Qian, Junyang and Tanigawa, Yosuke and Du, Wenfei and Aguirre, Matthew and Chang, Chris and Tibshirani, Robert and Rivas, Manuel A. and Hastie, Trevor},
	month = oct,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Asthma, Body mass index, Genetics, Genome-wide association studies, Heredity, Hypercholesterolemia, Single nucleotide polymorphisms},
	pages = {e1009141},
}

@article{schoenfeld_partial_1982-1,
	title = {Partial {Residuals} for {The} {Proportional} {Hazards} {Regression} {Model}},
	volume = {69},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335876},
	doi = {10.2307/2335876},
	abstract = {Residuals are defined for the proportional hazards regression model introduced by Cox (1972). These residuals can be plotted against time to test the proportional hazards assumption. Histograms of these residuals can be used to examine fit and detect outlying covariate values.},
	number = {1},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Schoenfeld, David},
	year = {1982},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {239--241},
}

@article{segall_heart_2014-1,
	title = {Heart failure in patients with chronic kidney disease: a systematic integrative review},
	volume = {2014},
	issn = {2314-6141},
	shorttitle = {Heart failure in patients with chronic kidney disease},
	doi = {10.1155/2014/937398},
	abstract = {INTRODUCTION: Heart failure (HF) is highly prevalent in patients with chronic kidney disease (CKD) and end-stage renal disease (ESRD) and is strongly associated with mortality in these patients. However, the treatment of HF in this population is largely unclear.
STUDY DESIGN: We conducted a systematic integrative review of the literature to assess the current evidence of HF treatment in CKD patients, searching electronic databases in April 2014. Synthesis used narrative methods.
SETTING AND POPULATION: We focused on adults with a primary diagnosis of CKD and HF.
SELECTION CRITERIA FOR STUDIES: We included studies of any design, quantitative or qualitative.
INTERVENTIONS: HF treatment was defined as any formal means taken to improve the symptoms of HF and/or the heart structure and function abnormalities.
OUTCOMES: Measures of all kinds were considered of interest.
RESULTS: Of 1,439 results returned by database searches, 79 articles met inclusion criteria. A further 23 relevant articles were identified by hand searching.
CONCLUSIONS: Control of fluid overload, the use of beta-blockers and angiotensin-converting enzyme inhibitors or angiotensin receptor blockers, and optimization of dialysis appear to be the most important methods to treat HF in CKD and ESRD patients. Aldosterone antagonists and digitalis glycosides may additionally be considered; however, their use is associated with significant risks. The role of anemia correction, control of CKD-mineral and bone disorder, and cardiac resynchronization therapy are also discussed.},
	language = {eng},
	journal = {BioMed Research International},
	author = {Segall, Liviu and Nistor, Ionut and Covic, Adrian},
	year = {2014},
	pmid = {24959595},
	pmcid = {PMC4052068},
	keywords = {Angiotensin Receptor Antagonists, Angiotensin-Converting Enzyme Inhibitors, Glomerular Filtration Rate, Heart Failure, Humans, Kidney Failure, Chronic, Renal Insufficiency, Chronic, Risk Factors},
	pages = {937398},
}

@article{shin_scalable_2018-1,
	title = {{SCALABLE} {BAYESIAN} {VARIABLE} {SELECTION} {USING} {NONLOCAL} {PRIOR} {DENSITIES} {IN} {ULTRAHIGH}-{DIMENSIONAL} {SETTINGS}},
	volume = {28},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/44841937},
	abstract = {Bayesian model selection procedures based on nonlocal alternative prior densities are extended to ultrahigh dimensional settings and compared to other variable selection procedures using precision-recall curves. Variable selection procedures included in these comparisons include methods based on g-priors, reciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria. The use of precision-recall curves eliminates the sensitivity of our conclusions to the choice of tuning parameters. We find that Bayesian variable selection procedures based on nonlocal priors are competitive to all other procedures in a range of simulation scenarios, and we subsequently explain this favorable performance through a theoretical examination of their consistency properties. When certain regularity conditions apply, we demonstrate that the nonlocal procedures are consistent for linear models even when the number of covariates increases sub-exponentially with the sample size n. A model selection procedure based on Zellner's g-prior is also found to be competitive with penalized likelihood methods in identifying the true model, but the posterior distribution on the model space induced by this method is much more dispersed than the posterior distribution induced on the model space by the nonlocal prior methods. We investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and show that it attains a unique term that cannot be derived from the other Bayesian model selection procedures. We also propose a scalable and efficient algorithm called Simplified Shotgun Stochastic Search with Screening (S5) to explore the enormous model space, and we show that S5 dramatically reduces the computing time without losing the capacity to search the interesting region in the model space, at least in the simulation settings considered. The S5 algorithm is available in an Rpackage BayesS5 on CRAN.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Statistica Sinica},
	author = {Shin, Minsuk and Bhattacharya, Anirban and Johnson, Valen E.},
	year = {2018},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {1053--1078},
}

@article{sleeper_regression_1990-1,
	title = {Regression {Splines} in the {Cox} {Model} with {Application} to {Covariate} {Effects} in {Liver} {Disease}},
	volume = {85},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474965},
	doi = {10.1080/01621459.1990.10474965},
	abstract = {The Cox proportional hazards model restricts the log hazard ratio to be linear in the covariates. A smooth nonlinear covariate effect may go undetected in this model but can be well approximated by a spline function. A survival model based on data from a clinical trial of primary biliary cirrhosis is developed using regression splines, and the resulting log hazard ratio estimates are compared with those from nonparametric methods. We remove the linear restriction on the log hazard ratio by transforming a continuous covariate into a vector of fixed knot basis splines (B-splines). B-splines are known to produce better-conditioned systems of equations than the truncated power basis when used as interpolants, and show similar behavior when fitting proportional hazards models. We describe the procedures for, and the issues arising in, the estimation and the testing of the B-spline coefficients. Although inference is not well developed for some nonparametric methods that estimate covariate effects, the asymptotic theory for Cox model B-spline estimates is relatively straightforward. An S function for fitting B-splines in Cox regression models is available.},
	number = {412},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Sleeper, Lynn A. and Harrington, David P.},
	month = dec,
	year = {1990},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1990.10474965},
	keywords = {B-spline, Proportional hazards, Relative risk, Spline approximation, Survival analysis},
	pages = {941--949},
}

@article{sudlow_uk_2015-1,
	title = {{UK} biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age},
	volume = {12},
	issn = {1549-1676},
	shorttitle = {{UK} biobank},
	doi = {10.1371/journal.pmed.1001779},
	abstract = {Cathie Sudlow and colleagues describe the UK Biobank, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
	language = {eng},
	number = {3},
	journal = {PLoS medicine},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins, Rory},
	month = mar,
	year = {2015},
	pmid = {25826379},
	pmcid = {PMC4380465},
	keywords = {Access to Information, Adult, Aged, Aging, Biological Specimen Banks, Databases, Factual, Genotype, Humans, Middle Aged, Neoplasms, Phenotype, Prospective Studies, Research, United Kingdom},
	pages = {e1001779},
}

@article{therneau_martingale-based_1990-1,
	title = {Martingale-{Based} {Residuals} for {Survival} {Models}},
	volume = {77},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2336057},
	doi = {10.2307/2336057},
	abstract = {Graphical methods based on the analysis of residuals are considered for the setting of the highly-used Cox (1972) regression model and for the Andersen-Gill (1982) generalization of that model. We start with a class of martingale-based residuals as proposed by Barlow \& Prentice (1988). These residuals and/or their transforms are useful for investigating the functional form of a covariate, the proportional hazards assumption, the leverage of each subject upon the estimates of β, and the lack of model fit to a given subject.},
	number = {1},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Therneau, Terry M. and Grambsch, Patricia M. and Fleming, Thomas R.},
	year = {1990},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {147--160},
}

@article{therneau_package_2014,
	title = {Package ‘survival’},
	volume = {2},
	number = {3},
	journal = {Survival analysis Published on CRAN},
	author = {Therneau, Terry M and Lumley, Thomas},
	year = {2014},
	pages = {119},
}

@book{therneau_package_2021,
	title = {A {Package} for {Survival} {Analysis} in {R}},
	url = {https://CRAN.R-project.org/package=survival},
	author = {Therneau, Terry M.},
	year = {2021},
}

@article{virani_heart_2021-1,
	title = {Heart {Disease} and {Stroke} {Statistics}-2021 {Update}: {A} {Report} {From} the {American} {Heart} {Association}},
	volume = {143},
	issn = {1524-4539},
	shorttitle = {Heart {Disease} and {Stroke} {Statistics}-2021 {Update}},
	doi = {10.1161/CIR.0000000000000950},
	abstract = {BACKGROUND: The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs).
METHODS: The American Heart Association, through its Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update. The 2021 Statistical Update is the product of a full year's worth of effort by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. This year's edition includes data on the monitoring and benefits of cardiovascular health in the population, an enhanced focus on social determinants of health, adverse pregnancy outcomes, vascular contributions to brain health, the global burden of cardiovascular disease, and further evidence-based approaches to changing behaviors related to cardiovascular disease.
RESULTS: Each of the 27 chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics.
CONCLUSIONS: The Statistical Update represents a critical resource for the lay public, policy makers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.},
	language = {eng},
	number = {8},
	journal = {Circulation},
	author = {Virani, Salim S. and Alonso, Alvaro and Aparicio, Hugo J. and Benjamin, Emelia J. and Bittencourt, Marcio S. and Callaway, Clifton W. and Carson, April P. and Chamberlain, Alanna M. and Cheng, Susan and Delling, Francesca N. and Elkind, Mitchell S. V. and Evenson, Kelly R. and Ferguson, Jane F. and Gupta, Deepak K. and Khan, Sadiya S. and Kissela, Brett M. and Knutson, Kristen L. and Lee, Chong D. and Lewis, Tené T. and Liu, Junxiu and Loop, Matthew Shane and Lutsey, Pamela L. and Ma, Jun and Mackey, Jason and Martin, Seth S. and Matchar, David B. and Mussolino, Michael E. and Navaneethan, Sankar D. and Perak, Amanda Marma and Roth, Gregory A. and Samad, Zainab and Satou, Gary M. and Schroeder, Emily B. and Shah, Svati H. and Shay, Christina M. and Stokes, Andrew and VanWagner, Lisa B. and Wang, Nae-Yuh and Tsao, Connie W. and {American Heart Association Council on Epidemiology and Prevention Statistics Committee and Stroke Statistics Subcommittee}},
	month = feb,
	year = {2021},
	pmid = {33501848},
	keywords = {AHA Scientific Statements, American Heart Association, Blood Pressure, cardiovascular diseases, Cholesterol, Diabetes Mellitus, Diet, Healthy, epidemiology, Exercise, Global Burden of Disease, Health Behavior, Heart Diseases, Hospitalization, Humans, Obesity, Prevalence, risk factors, Risk Factors, Smoking, statistics, stroke, Stroke, United States},
	pages = {e254--e743},
}

@inproceedings{yao_yes_2018-1,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Yes, but {Did} {It} {Work}?: {Evaluating} {Variational} {Inference}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/yao18a.html},
	abstract = {While it’s always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation. We propose two diagnostic algorithms to alleviate this problem. The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational simulation-based calibration (VSBC) assesses the average performance of point estimates.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {5581--5590},
}

@misc{noauthor_adaptive_nodate,
	title = {Adaptive {Lasso} for {Cox}'s {Proportional} {Hazards} {Model} on {JSTOR}},
	url = {https://www.jstor.org/stable/20441405?seq=1#metadata_info_tab_contents},
	urldate = {2022-01-11},
}

@article{breheny_coordinate_2011,
	title = {Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection},
	volume = {5},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-1/Coordinate-descent-algorithms-for-nonconvex-penalized-regression-with-applications-to/10.1214/10-AOAS388.full},
	doi = {10.1214/10-AOAS388},
	abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
	number = {1},
	urldate = {2022-01-11},
	journal = {The Annals of Applied Statistics},
	author = {Breheny, Patrick and Huang, Jian},
	month = mar,
	year = {2011},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Coordinate descent, Lasso, MCP, optimization, penalized regression, SCAD},
	pages = {232--253},
}

@article{zou_adaptive_2006,
	title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000735},
	doi = {10.1198/016214506000000735},
	abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ℓ1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
	number = {476},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Zou, Hui},
	month = dec,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000000735},
	keywords = {Asymptotic normality, Lasso, Minimax, Oracle inequality, Oracle procedure, Variable selection},
	pages = {1418--1429},
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s proportional hazards model},
	volume = {94},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asm037},
	doi = {10.1093/biomet/asm037},
	abstract = {We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L1 penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively.},
	number = {3},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Zhang, Hao Helen and Lu, Wenbin},
	month = aug,
	year = {2007},
	pages = {691--703},
}

@article{kropko_beyond_2020,
	title = {Beyond the {Hazard} {Ratio}: {Generating} {Expected} {Durations} from the {Cox} {Proportional} {Hazards} {Model}},
	volume = {50},
	issn = {0007-1234, 1469-2112},
	shorttitle = {Beyond the {Hazard} {Ratio}},
	url = {https://www.cambridge.org/core/journals/british-journal-of-political-science/article/abs/beyond-the-hazard-ratio-generating-expected-durations-from-the-cox-proportional-hazards-model/B161DC9C5C9C83B16B27E2578F35FCBB},
	doi = {10.1017/S000712341700045X},
	abstract = {The Cox proportional hazards model is a commonly used method for duration analysis in political science. Typical quantities of interest used to communicate results come from the hazard function (for example, hazard ratios or percentage changes in the hazard rate). These quantities are substantively vague, difficult for many audiences to understand and incongruent with researchers’ substantive focus on duration. We propose methods for computing expected durations and marginal changes in duration for a specified change in a covariate from the Cox model. These duration-based quantities closely match researchers’ theoretical interests and are easily understood by most readers. We demonstrate the substantive improvements in interpretation of Cox model results afforded by the methods with reanalyses of articles from three subfields of political science.},
	language = {en},
	number = {1},
	urldate = {2022-01-11},
	journal = {British Journal of Political Science},
	author = {Kropko, Jonathan and Harden, Jeffrey J.},
	month = jan,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {Cox proportional hazards model, duration models, expected durations, quantities of interest},
	pages = {303--320},
}
