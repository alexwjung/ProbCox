
@article{sinha_bayesian_2003,
	title = {A {Bayesian} justification of {Cox}'s partial likelihood},
	volume = {90},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/90.3.629},
	doi = {10.1093/biomet/90.3.629},
	abstract = {In this paper, we establish both naive and formal Bayesian justifications of partial likelihood and its various modifications. We extend the original work of K (1978), who showed that the partial likelihood is a limiting marginal poste noninformative priors for baseline hazards. We extend the result to scenarios dependent covariates and time-varying regression parameters. We establish continuous time as well as grouped survival data. In addition, we present a justification of a modified partial likelihood for handling ties. We also prese simplification of the Gibbs sampling algorithm for implementing partial like Bayesian inference in various practical applications.},
	language = {en},
	number = {3},
	urldate = {2021-03-10},
	journal = {Biometrika},
	author = {Sinha, D.},
	month = sep,
	year = {2003},
	keywords = {theory},
	pages = {629--641},
	file = {Sinha - 2003 - A Bayesian justification of Cox's partial likeliho.pdf:/Users/alexwjung/Google Drive/library/storage/Z3HYQESU/Sinha - 2003 - A Bayesian justification of Cox's partial likeliho.pdf:application/pdf},
}

@article{yusuf_modifiable_2020,
	title = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE}): a prospective cohort study},
	volume = {395},
	issn = {01406736},
	shorttitle = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673619320082},
	doi = {10.1016/S0140-6736(19)32008-2},
	abstract = {Background Global estimates of the effect of common modifiable risk factors on cardiovascular disease and mortality are largely based on data from separate studies, using different methodologies. The Prospective Urban Rural Epidemiology (PURE) study overcomes these limitations by using similar methods to prospectively measure the effect of modifiable risk factors on cardiovascular disease and mortality across 21 countries (spanning five continents) grouped by different economic levels.},
	language = {en},
	number = {10226},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Yusuf, Salim and Joseph, Philip and Rangarajan, Sumathy and Islam, Shofiqul and Mente, Andrew and Hystad, Perry and Brauer, Michael and Kutty, Vellappillil Raman and Gupta, Rajeev and Wielgosz, Andreas and AlHabib, Khalid F and Dans, Antonio and Lopez-Jaramillo, Patricio and Avezum, Alvaro and Lanas, Fernando and Oguz, Aytekin and Kruger, Iolanthe M and Diaz, Rafael and Yusoff, Khalid and Mony, Prem and Chifamba, Jephat and Yeates, Karen and Kelishadi, Roya and Yusufali, Afzalhussein and Khatib, Rasha and Rahman, Omar and Zatonska, Katarzyna and Iqbal, Romaina and Wei, Li and Bo, Hu and Rosengren, Annika and Kaur, Manmeet and Mohan, Viswanathan and Lear, Scott A and Teo, Koon K and Leong, Darryl and O'Donnell, Martin and McKee, Martin and Dagenais, Gilles},
	month = mar,
	year = {2020},
	keywords = {application},
	pages = {795--808},
	file = {Yusuf et al. - 2020 - Modifiable risk factors, cardiovascular disease, a.pdf:/Users/alexwjung/Google Drive/library/storage/EWBQVYQD/Yusuf et al. - 2020 - Modifiable risk factors, cardiovascular disease, a.pdf:application/pdf},
}

@article{mortensen_elevated_2020,
	title = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years: a contemporary primary prevention cohort},
	volume = {396},
	issn = {01406736},
	shorttitle = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673620322339},
	doi = {10.1016/S0140-6736(20)32233-9},
	abstract = {Background Findings of historical studies suggest that elevated LDL cholesterol is not associated with increased risk of myocardial infarction and atherosclerotic cardiovascular disease in patients older than 70 years. We aimed to test this hypothesis in a contemporary population of individuals aged 70–100 years.},
	language = {en},
	number = {10263},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Mortensen, Martin Bødtker and Nordestgaard, Børge Grønne},
	month = nov,
	year = {2020},
	keywords = {application},
	pages = {1644--1652},
	file = {Mortensen and Nordestgaard - 2020 - Elevated LDL cholesterol and increased risk of myo.pdf:/Users/alexwjung/Google Drive/library/storage/DPT979F9/Mortensen and Nordestgaard - 2020 - Elevated LDL cholesterol and increased risk of myo.pdf:application/pdf},
}

@article{millett_sex_2018,
	title = {Sex differences in risk factors for myocardial infarction: cohort study of {UK} {Biobank} participants},
	issn = {0959-8138, 1756-1833},
	shorttitle = {Sex differences in risk factors for myocardial infarction},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.k4247},
	doi = {10.1136/bmj.k4247},
	abstract = {Objectives To investigate sex differences in risk factors for incident myocardial infarction (MI) and whether they vary with age. Design Prospective population based study. Setting UK Biobank. Participants 471 998 participants (56\% women; mean age 56.2) with no history of cardiovascular disease. Main outcome measure Incident (fatal and non-fatal) MI. Results 5081 participants (1463 (28.8\%) of whom were women) had MI over seven years’ mean follow-up, resulting in an incidence per 10 000 person years of 7.76 (95\% confidence interval 7.37 to 8.16) for women and 24.35 (23.57 to 25.16) for men. Higher blood pressure indices, smoking intensity, body mass index, and the presence of diabetes were associated with an increased risk of MI in men and women, but associations were attenuated with age. In women, systolic blood pressure and hypertension, smoking status and intensity, and diabetes were associated with higher hazard ratios for MI compared with men: ratio of hazard ratios 1.09 (95\% confidence interval 1.02 to 1.16) for systolic blood pressure, 1.55 (1.32 to 1.83) for current smoking, 2.91 (1.56 to 5.45) for type 1 diabetes, and 1.47 (1.16 to 1.87) for type 2 diabetes. There was no evidence that any of these ratios of hazard ratios decreased with age (P{\textgreater}0.2). With the exception of type 1 diabetes, the incidence of MI was higher in men than in women for all risk factors.},
	language = {en},
	urldate = {2021-03-10},
	journal = {BMJ},
	author = {Millett, Elizabeth R C and Peters, Sanne A E and Woodward, Mark},
	month = nov,
	year = {2018},
	keywords = {application},
	pages = {k4247},
	file = {Millett et al. - 2018 - Sex differences in risk factors for myocardial inf.pdf:/Users/alexwjung/Google Drive/library/storage/J6GE6Q3E/Millett et al. - 2018 - Sex differences in risk factors for myocardial inf.pdf:application/pdf},
}

@article{andersen_analysis_2021,
	title = {Analysis of time-to-event for observational studies: {Guidance} to the use of intensity models},
	volume = {40},
	issn = {1097-0258},
	shorttitle = {Analysis of time-to-event for observational studies},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8757},
	doi = {https://doi.org/10.1002/sim.8757},
	abstract = {This paper provides guidance for researchers with some mathematical background on the conduct of time-to-event analysis in observational studies based on intensity (hazard) models. Discussions of basic concepts like time axis, event definition and censoring are given. Hazard models are introduced, with special emphasis on the Cox proportional hazards regression model. We provide check lists that may be useful both when fitting the model and assessing its goodness of fit and when interpreting the results. Special attention is paid to how to avoid problems with immortal time bias by introducing time-dependent covariates. We discuss prediction based on hazard models and difficulties when attempting to draw proper causal conclusions from such models. Finally, we present a series of examples where the methods and check lists are exemplified. Computational details and implementation using the freely available R software are documented in Supplementary Material. The paper was prepared as part of the STRATOS initiative.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Andersen, Per Kragh and Perme, Maja Pohar and Houwelingen, Hans C. van and Cook, Richard J. and Joly, Pierre and Martinussen, Torben and Taylor, Jeremy M. G. and Abrahamowicz, Michal and Therneau, Terry M.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8757},
	keywords = {censoring, Cox regression model, hazard function, immortal time bias, multistate model, prediction, STRATOS initiative, survival analysis, time-dependent covariates, theory},
	pages = {185--211},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/QSH8I3XC/sim.html:text/html;Full Text PDF:/Users/alexwjung/Google Drive/library/storage/KL36D8GL/Andersen et al. - 2021 - Analysis of time-to-event for observational studie.pdf:application/pdf},
}

@article{sylvestre_comparison_2008,
	title = {Comparison of algorithms to generate event times conditional on time-dependent covariates},
	volume = {27},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3092},
	doi = {https://doi.org/10.1002/sim.3092},
	abstract = {The Cox proportional hazards model with time-dependent covariates (TDC) is now a part of the standard statistical analysis toolbox in medical research. As new methods involving more complex modeling of time-dependent variables are developed, simulations could often be used to systematically assess the performance of these models. Yet, generating event times conditional on TDC requires well-designed and efficient algorithms. We compare two classes of such algorithms: permutational algorithms (PAs) and algorithms based on a binomial model. We also propose a modification of the PA to incorporate a rejection sampler. We performed a simulation study to assess the accuracy, stability, and speed of these algorithms in several scenarios. Both classes of algorithms generated data sets that, once analyzed, provided virtually unbiased estimates with comparable variances. In terms of computational efficiency, the PA with the rejection sampler reduced the time necessary to generate data by more than 50 per cent relative to alternative methods. The PAs also allowed more flexibility in the specification of the marginal distributions of event times and required less calibration. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {14},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Sylvestre, Marie-Pierre and Abrahamowicz, Michal},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3092},
	keywords = {survival analysis, time-dependent covariates, algorithms, proportional hazards model, rejection sampling, simulations, simulation},
	pages = {2618--2634},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/5T23G3IV/sim.html:text/html;Full Text PDF:/Users/alexwjung/Google Drive/library/storage/QIJWXLTC/Sylvestre and Abrahamowicz - 2008 - Comparison of algorithms to generate event times c.pdf:application/pdf},
}

@article{alaa_cardiovascular_2019,
	title = {Cardiovascular disease risk prediction using automated machine learning: {A} prospective study of 423,604 {UK} {Biobank} participants},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Cardiovascular disease risk prediction using automated machine learning},
	url = {https://dx.plos.org/10.1371/journal.pone.0213653},
	doi = {10.1371/journal.pone.0213653},
	language = {en},
	number = {5},
	urldate = {2021-03-10},
	journal = {PLOS ONE},
	author = {Alaa, Ahmed M. and Bolton, Thomas and Di Angelantonio, Emanuele and Rudd, James H. F. and van der Schaar, Mihaela},
	editor = {Aalto-Setala, Katriina},
	month = may,
	year = {2019},
	keywords = {application},
	pages = {e0213653},
	file = {Alaa et al. - 2019 - Cardiovascular disease risk prediction using autom.pdf:/Users/alexwjung/Google Drive/library/storage/NAQ7EAFZ/Alaa et al. - 2019 - Cardiovascular disease risk prediction using autom.pdf:application/pdf},
}

@article{dagostino_ralph_b_general_2008,
	title = {General {Cardiovascular} {Risk} {Profile} for {Use} in {Primary} {Care}},
	volume = {117},
	url = {https://www.ahajournals.org/doi/10.1161/circulationaha.107.699579},
	doi = {10.1161/CIRCULATIONAHA.107.699579},
	abstract = {Background— Separate multivariable risk algorithms are commonly used to assess risk of specific atherosclerotic cardiovascular disease (CVD) events, ie, coronary heart disease, cerebrovascular disease, peripheral vascular disease, and heart failure. The present report presents a single multivariable risk function that predicts risk of developing all CVD and of its constituents.Methods and Results— We used Cox proportional-hazards regression to evaluate the risk of developing a first CVD event in 8491 Framingham study participants (mean age, 49 years; 4522 women) who attended a routine examination between 30 and 74 years of age and were free of CVD. Sex-specific multivariable risk functions (“general CVD” algorithms) were derived that incorporated age, total and high-density lipoprotein cholesterol, systolic blood pressure, treatment for hypertension, smoking, and diabetes status. We assessed the performance of the general CVD algorithms for predicting individual CVD events (coronary heart disease, stroke, peripheral artery disease, or heart failure). Over 12 years of follow-up, 1174 participants (456 women) developed a first CVD event. All traditional risk factors evaluated predicted CVD risk (multivariable-adjusted P{\textless}0.0001). The general CVD algorithm demonstrated good discrimination (C statistic, 0.763 [men] and 0.793 [women]) and calibration. Simple adjustments to the general CVD risk algorithms allowed estimation of the risks of each CVD component. Two simple risk scores are presented, 1 based on all traditional risk factors and the other based on non–laboratory-based predictors.Conclusions— A sex-specific multivariable risk factor algorithm can be conveniently used to assess general CVD risk and risk of individual CVD events (coronary, cerebrovascular, and peripheral arterial disease and heart failure). The estimated absolute CVD event rates can be used to quantify risk and to guide preventive care.},
	number = {6},
	urldate = {2021-03-10},
	journal = {Circulation},
	author = {{D’Agostino Ralph B.} and {Vasan Ramachandran S.} and {Pencina Michael J.} and {Wolf Philip A.} and {Cobain Mark} and {Massaro Joseph M.} and {Kannel William B.}},
	month = feb,
	year = {2008},
	note = {Publisher: American Heart Association},
	keywords = {application},
	pages = {743--753},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/86K9WQCR/D’Agostino Ralph B. et al. - 2008 - General Cardiovascular Risk Profile for Use in Pri.pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	language = {en},
	number = {518},
	urldate = {2021-03-10},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {theory},
	pages = {859--877},
	file = {Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:/Users/alexwjung/Google Drive/library/storage/DEX5499V/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf},
}

@article{andersen_coxs_1982,
	title = {Cox's {Regression} {Model} for {Counting} {Processes}: {A} {Large} {Sample} {Study}},
	volume = {10},
	issn = {0090-5364},
	shorttitle = {Cox's {Regression} {Model} for {Counting} {Processes}},
	url = {http://projecteuclid.org/euclid.aos/1176345976},
	doi = {10.1214/aos/1176345976},
	language = {en},
	number = {4},
	urldate = {2021-03-10},
	journal = {The Annals of Statistics},
	author = {Andersen, P. K. and Gill, R. D.},
	month = dec,
	year = {1982},
	keywords = {theory},
	pages = {1100--1120},
	file = {Andersen and Gill - 1982 - Cox's Regression Model for Counting Processes A L.pdf:/Users/alexwjung/Google Drive/library/storage/9GL93LQE/Andersen and Gill - 1982 - Cox's Regression Model for Counting Processes A L.pdf:application/pdf},
}

@article{mittal_high-dimensional_2014,
	title = {High-dimensional, massive sample-size {Cox} proportional hazards regression for survival analysis},
	volume = {15},
	issn = {1465-4644},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3944969/},
	doi = {10.1093/biostatistics/kxt043},
	abstract = {Survival analysis endures as an old, yet active research field with applications that spread across many domains. Continuing improvements in data acquisition techniques pose constant challenges in applying existing survival analysis methods to these emerging data sets. In this paper, we present tools for fitting regularized Cox survival analysis models on high-dimensional, massive sample-size (HDMSS) data using a variant of the cyclic coordinate descent optimization technique tailored for the sparsity that HDMSS data often present. Experiments on two real data examples demonstrate that efficient analyses of HDMSS data using these tools result in improved predictive performance and calibration.},
	number = {2},
	urldate = {2021-03-10},
	journal = {Biostatistics (Oxford, England)},
	author = {Mittal, Sushil and Madigan, David and Burd, Randall S. and Suchard, Marc A.},
	month = apr,
	year = {2014},
	pmid = {24096388},
	pmcid = {PMC3944969},
	keywords = {theory},
	pages = {207--221},
	file = {PubMed Central Full Text PDF:/Users/alexwjung/Google Drive/library/storage/X5MVEAKI/Mittal et al. - 2014 - High-dimensional, massive sample-size Cox proporti.pdf:application/pdf},
}

@article{tarkhan_bigsurvsgd_2020,
	title = {{BigSurvSGD}: {Big} {Survival} {Data} {Analysis} via {Stochastic} {Gradient} {Descent}},
	shorttitle = {{BigSurvSGD}},
	url = {http://arxiv.org/abs/2003.00116},
	abstract = {In many biomedical applications, outcome is measured as a ``time-to-event'' (eg. disease progression or death). To assess the connection between features of a patient and this outcome, it is common to assume a proportional hazards model, and fit a proportional hazards regression (or Cox regression). To fit this model, a log-concave objective function known as the ``partial likelihood'' is maximized. For moderate-sized datasets, an efficient Newton-Raphson algorithm that leverages the structure of the objective can be employed. However, in large datasets this approach has two issues: 1) The computational tricks that leverage structure can also lead to computational instability; 2) The objective does not naturally decouple: Thus, if the dataset does not fit in memory, the model can be very computationally expensive to fit. This additionally means that the objective is not directly amenable to stochastic gradient-based optimization methods. To overcome these issues, we propose a simple, new framing of proportional hazards regression: This results in an objective function that is amenable to stochastic gradient descent. We show that this simple modification allows us to efficiently fit survival models with very large datasets. This also facilitates training complex, eg. neural-network-based, models with survival data.},
	urldate = {2021-03-10},
	journal = {arXiv:2003.00116 [math, stat]},
	author = {Tarkhan, Aliasghar and Simon, Noah},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.00116},
	keywords = {Mathematics - Statistics Theory, theory},
	file = {arXiv Fulltext PDF:/Users/alexwjung/Google Drive/library/storage/VKL4XUK6/Tarkhan and Simon - 2020 - BigSurvSGD Big Survival Data Analysis via Stochas.pdf:application/pdf;arXiv.org Snapshot:/Users/alexwjung/Google Drive/library/storage/NXBCJ9EL/2003.html:text/html},
}

@article{hendry_data_2014,
	title = {Data generation for the {Cox} proportional hazards model with time-dependent covariates: a method for medical researchers},
	volume = {33},
	issn = {02776715},
	shorttitle = {Data generation for the {Cox} proportional hazards model with time-dependent covariates},
	url = {http://doi.wiley.com/10.1002/sim.5945},
	doi = {10.1002/sim.5945},
	abstract = {The proliferation of longitudinal studies has increased the importance of statistical methods for time-to-event data that can incorporate time-dependent covariates. The Cox proportional hazards model is one such method that is widely used. As more extensions of the Cox model with time-dependent covariates are developed, simulations studies will grow in importance as well. An essential starting point for simulation studies of time-to-event models is the ability to produce simulated survival times from a known data generating process. This paper develops a method for the generation of survival times that follow a Cox proportional hazards model with time-dependent covariates. The method presented relies on a simple transformation of random variables generated according to a truncated piecewise exponential distribution, and allows practitioners great ﬂexibility and control over both the number of time-dependent covariates and the number of time periods in the duration of follow-up measurement. Within this framework, an additional argument is suggested that allows researchers to generate time-to-event data in which covariates change at integer-valued steps of the time scale. The purpose of this approach is to produce data for simulation experiments that mimic the types of data structures applied researchers encounter when using longitudinal biomedical data. Validity is assessed in a set of simulation experiments and results indicate that the proposed procedure performs well in producing data that conform to the assumptions of the Cox proportional hazards model.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Hendry, David J.},
	month = feb,
	year = {2014},
	keywords = {simulation},
	pages = {436--454},
	file = {Hendry - 2014 - Data generation for the Cox proportional hazards m.pdf:/Users/alexwjung/Google Drive/library/storage/RFEQZLKU/Hendry - 2014 - Data generation for the Cox proportional hazards m.pdf:application/pdf},
}

@article{austin_generating_2012,
	title = {Generating survival times to simulate {Cox} proportional hazards models with time-varying covariates},
	volume = {31},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3546387/},
	doi = {10.1002/sim.5452},
	abstract = {Simulations and Monte Carlo methods serve an important role in modern statistical research. They allow for an examination of the performance of statistical procedures in settings in which analytic and mathematical derivations may not be feasible. A key element in any statistical simulation is the existence of an appropriate data-generating process: one must be able to simulate data from a specified statistical model. We describe data-generating processes for the Cox proportional hazards model with time-varying covariates when event times follow an exponential, Weibull, or Gompertz distribution. We consider three types of time-varying covariates: first, a dichotomous time-varying covariate that can change at most once from untreated to treated (e.g., organ transplant); second, a continuous time-varying covariate such as cumulative exposure at a constant dose to radiation or to a pharmaceutical agent used for a chronic condition; third, a dichotomous time-varying covariate with a subject being able to move repeatedly between treatment states (e.g., current compliance or use of a medication). In each setting, we derive closed-form expressions that allow one to simulate survival times so that survival times are related to a vector of fixed or time-invariant covariates and to a single time-varying covariate. We illustrate the utility of our closed-form expressions for simulating event times by using Monte Carlo simulations to estimate the statistical power to detect as statistically significant the effect of different types of binary time-varying covariates. This is compared with the statistical power to detect as statistically significant a binary time-invariant covariate. Copyright © 2012 John Wiley \& Sons, Ltd.},
	number = {29},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C},
	month = dec,
	year = {2012},
	pmid = {22763916},
	pmcid = {PMC3546387},
	keywords = {simulation},
	pages = {3946--3958},
	file = {PubMed Central Full Text PDF:/Users/alexwjung/Google Drive/library/storage/QTWPFGRK/Austin - 2012 - Generating survival times to simulate Cox proporti.pdf:application/pdf},
}

@article{mackenzie_marginal_2002,
	title = {Marginal and hazard ratio specific random data generation: {Applications} to semi-parametric bootstrapping},
	volume = {12},
	issn = {1573-1375},
	shorttitle = {Marginal and hazard ratio specific random data generation},
	url = {https://doi.org/10.1023/A:1020750810409},
	doi = {10.1023/A:1020750810409},
	abstract = {Cox's partial likelihood for censored time-to-event data can be interpreted as a permutation probability, whereby covariate values are permuted to the observed times-to-event and censoring times. This interpretation facilitates a simple method for jointly generating times-to-event and covariate tuples with considerable flexibility, including time dependence of the hazard ratio and specification of both the marginal time-to-event and covariate distributions. This interpretation also facilitates a method for semi-parametric bootstrapping of hazard ratio estimators.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics and Computing},
	author = {Mackenzie, Todd and Abrahamowicz, Michal},
	month = jul,
	year = {2002},
	keywords = {simulation},
	pages = {245--252},
	file = {Springer Full Text PDF:/Users/alexwjung/Google Drive/library/storage/9S4NK72G/Mackenzie and Abrahamowicz - 2002 - Marginal and hazard ratio specific random data gen.pdf:application/pdf},
}

@article{kucukelbir_automatic_2017,
	title = {Automatic {Differentiation} {Variational} {Inference}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/16-107.html},
	number = {14},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	year = {2017},
	keywords = {theory},
	pages = {1--45},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/3UNJWF6D/Kucukelbir et al. - 2017 - Automatic Differentiation Variational Inference.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/Y7Z6FQKG/16-107.html:text/html},
}

@article{li_fast_2020,
	title = {Fast {Lasso} method for large-scale and ultrahigh-dimensional {Cox} model with applications to {UK} {Biobank}},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxaa038},
	doi = {10.1093/biostatistics/kxaa038},
	abstract = {We develop a scalable and highly efficient algorithm to fit a Cox proportional hazard model by maximizing the \$L{\textasciicircum}1\$-regularized (Lasso) partial likelihood function, based on the Batch Screening Iterative Lasso (BASIL) method developed in Qian and others (2019). Our algorithm is particularly suitable for large-scale and high-dimensional data that do not fit in the memory. The output of our algorithm is the full Lasso path, the parameter estimates at all predefined regularization parameters, as well as their validation accuracy measured using the concordance index (C-index) or the validation deviance. To demonstrate the effectiveness of our algorithm, we analyze a large genotype-survival time dataset across 306 disease outcomes from the UK Biobank (Sudlow and others, 2015). We provide a publicly available implementation of the proposed approach for genetics data on top of the PLINK2 package and name it snpnet-Cox.},
	number = {kxaa038},
	urldate = {2021-03-11},
	journal = {Biostatistics},
	author = {Li, Ruilin and Chang, Christopher and Justesen, Johanne M and Tanigawa, Yosuke and Qiang, Junyang and Hastie, Trevor and Rivas, Manuel A and Tibshirani, Robert},
	month = sep,
	year = {2020},
	keywords = {theory},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/FUJ68DML/Li et al. - 2020 - Fast Lasso method for large-scale and ultrahigh-di.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/JQZ4CAP3/5912682.html:text/html},
}

@article{gustafson_large_1997,
	title = {Large {Hierarchical} {Bayesian} {Analysis} of {Multivariate} {Survival} {Data}},
	volume = {53},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533110},
	doi = {10.2307/2533110},
	abstract = {Failure times that are grouped according to shared environments arise commonly in statistical practice. That is, multiple responses may be observed for each of many units. For instance, the units might be patients or centers in a clinical trial setting. Bayesian hierarchical models are appropriate for data analysis in this context. At the first stage of the model, survival times can be modelled via the Cox partial likelihood, using a justification due to Kalbfleisch (1978, Journal of the Royal Statistical Society, Series B 40, 214-221). Thus, questionable parametric assumptions are avoided. Conventional wisdom dictates that it is comparatively safe to make parametric assumptions at subsequent stages. Thus, unit-specific parameters are modelled parametrically. The posterior distribution of parameters given observed data is examined using Markov chain Monte Carlo methods. Specifically, the hybrid Monte Carlo method, as described by Neal (1993a, in Advances in Neural Information Processing 5, 475-482; 1993b, Probabilistic inference using Markov chain Monte Carlo methods), is utilized.},
	number = {1},
	urldate = {2021-03-11},
	journal = {Biometrics},
	author = {Gustafson, Paul},
	year = {1997},
	note = {Publisher: [Wiley, International Biometric Society]},
	keywords = {theory},
	pages = {230--242},
}

@article{kalbfleisch_marginal_1973,
	title = {Marginal {Likelihoods} {Based} on {Cox}'s {Regression} and {Life} {Model}},
	volume = {60},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334538},
	doi = {10.2307/2334538},
	abstract = {Marginal likelihoods are obtained for the regression parameters in the model presented by Cox (1972). If no ties occur in the recording of failure time data the results of Cox are given a straightforward justification. If ties occur in the data, results different from those suggested by Cox are obtained. Some Monte-Carlo comparisons of these competing results are made. A discrete model is developed for grouped data from Cox's model and estimates of the survivor function are given for both continuous and grouped data.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Kalbfleisch, J. D. and Prentice, R. L.},
	year = {1973},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {267--278},
}

@article{hjort_nonparametric_1990,
	title = {Nonparametric {Bayes} {Estimators} {Based} on {Beta} {Processes} in {Models} for {Life} {History} {Data}},
	volume = {18},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242052},
	abstract = {Several authors have constructed nonparametric Bayes estimators for a cumulative distribution function based on (possibly right-censored) data. The prior distributions have, for example, been Dirichlet processes or, more generally, processes neutral to the right. The present article studies the related problem of finding Bayes estimators for cumulative hazard rates and related quantities, w.r.t. prior distributions that correspond to cumulative hazard rate processes with nonnegative independent increments. A particular class of prior processes, termed beta processes, is introduced and is shown to constitute a conjugate class. To arrive at these, a nonparametric time-discrete framework for survival data, which has some independent interest, is studied first. An important bonus of the approach based on cumulative hazards is that more complicated models for life history data than the simple life table situation can be treated, for example, time-inhomogeneous Markov chains. We find posterior distributions and derive Bayes estimators in such models and also present a semiparametric Bayesian analysis of the Cox regression model. The Bayes estimators are easy to interpret and easy to compute. In the limiting case of a vague prior the Bayes solution for a cumulative hazard is the Nelson-Aalen estimator and the Bayes solution for a survival probability is the Kaplan-Meier estimator.},
	number = {3},
	urldate = {2021-03-11},
	journal = {The Annals of Statistics},
	author = {Hjort, Nils Lid},
	year = {1990},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {theory},
	pages = {1259--1294},
}

@article{cox_partial_1975,
	title = {Partial {Likelihood}},
	volume = {62},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335362},
	doi = {10.2307/2335362},
	abstract = {A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and inference in stochastic processes. It is shown that the usual large-sample properties of maximum likelihood estimates and tests apply when partial likelihood is used.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Cox, D. R.},
	year = {1975},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {269--276},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2985181},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	keywords = {theory},
	pages = {187--220},
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	number = {4},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	keywords = {theory},
	pages = {1303--1347},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/G6XIMQ2K/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/2C2NBLWT/hoffman13a.html:text/html},
}

@article{kvamme_time--event_2019,
	title = {Time-to-{Event} {Prediction} with {Neural} {Networks} and {Cox} {Regression}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-424.html},
	number = {129},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kvamme, Håvard and Borgan, Ørnulf and Scheel, Ida},
	year = {2019},
	keywords = {theory},
	pages = {1--30},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/ZUECMNHX/Kvamme et al. - 2019 - Time-to-Event Prediction with Neural Networks and .pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/2HUPU7WI/18-424.html:text/html},
}

@article{abrahamowicz_time-dependent_1996,
	title = {Time-{Dependent} {Hazard} {Ratio}: {Modeling} and {Hypothesis} {Testing} with {Application} in {Lupus} {Nephritis}},
	volume = {91},
	issn = {0162-1459},
	shorttitle = {Time-{Dependent} {Hazard} {Ratio}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476711},
	doi = {10.1080/01621459.1996.10476711},
	abstract = {We investigate the association between duration of untreated disease and survival in lupus nephritis, a rare rheumatologic disease. In this case, as in many other studies of survival, a priori considerations suggest that the effect of the predictor on hazard may change with increasing follow-up time. To accommodate such situations, we use regression splines to model the hazard ratio as a flexible function of time. We propose model-based tests of the hypotheses of hazards proportionality and of no association. We evaluate the accuracy of estimation and inference in simulations and also present analysis of a larger medical data set.},
	number = {436},
	urldate = {2021-03-12},
	journal = {Journal of the American Statistical Association},
	author = {Abrahamowicz, Michal and Mackenzie, Todd and Esdaile, John M.},
	month = dec,
	year = {1996},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476711},
	keywords = {Akaike information criterion, Partial likelihood, Proportional hazards, Regression splines, Survival analysis, simulation},
	pages = {1432--1439},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/PMLB62AC/01621459.1996.html:text/html},
}

@article{bender_generating_2005,
	title = {Generating survival times to simulate {Cox} proportional hazards models},
	volume = {24},
	issn = {0277-6715},
	doi = {10.1002/sim.2059},
	abstract = {Simulation studies present an important statistical tool to investigate the performance, properties and adequacy of statistical models in pre-specified situations. One of the most important statistical models in medical research is the proportional hazards model of Cox. In this paper, techniques to generate survival times for simulation studies regarding Cox proportional hazards models are presented. A general formula describing the relation between the hazard and the corresponding survival time of the Cox model is derived, which is useful in simulation studies. It is shown how the exponential, the Weibull and the Gompertz distribution can be applied to generate appropriate survival times for simulation studies. Additionally, the general relation between hazard and survival time can be used to develop own distributions for special situations and to handle flexibly parameterized proportional hazards models. The use of distributions other than the exponential distribution is indispensable to investigate the characteristics of the Cox proportional hazards model, especially in non-standard situations, where the partial likelihood depends on the baseline hazard. A simulation study investigating the effect of measurement errors in the German Uranium Miners Cohort Study is considered to illustrate the proposed simulation techniques and to emphasize the importance of a careful modelling of the baseline hazard in Cox models.},
	language = {eng},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
	month = jun,
	year = {2005},
	pmid = {15724232},
	keywords = {simulation, Cohort Studies, Computer Simulation, Germany, East, Humans, Mining, Neoplasms, Proportional Hazards Models, Radiation, Radon, Survival Analysis},
	pages = {1713--1723},
	file = {Submitted Version:/Users/alexwjung/Google Drive/library/storage/X7UWCE8M/Bender et al. - 2005 - Generating survival times to simulate Cox proporti.pdf:application/pdf},
}

@article{zhou_understanding_2001,
	title = {Understanding the {Cox} {Regression} {Models} with {Time}-{Change} {Covariates}},
	volume = {55},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2686004},
	abstract = {The Cox regression model is a cornerstone of modern survival analysis and is widely used in many other fields as well. But the Cox models with time-change covariates are not easy to understand or visualize. We therefore offer a simple and easy-to-understand interpretation of the (arbitrary) baseline hazard and time-change covariate. This interpretation also provides a way to simulate variables that follow a Cox model with arbitrary baseline hazard and time-change covariate.},
	number = {2},
	urldate = {2021-03-12},
	journal = {The American Statistician},
	author = {Zhou, Mai},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {simulation},
	pages = {153--155},
}

@article{leemis_variate_1990,
	title = {Variate generation for accelerated life and proportional hazards models with time dependent covariates},
	volume = {10},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/0167715290900529},
	doi = {10.1016/0167-7152(90)90052-9},
	abstract = {Variate generation algorithms for lifetimes when survival models incorporate time dependent covariates are presented. These algorithms are closed form for special cases of the function that links the covariate values to the survivor distribution. These algorithms are illustrated by several examples.},
	language = {en},
	number = {4},
	urldate = {2021-03-12},
	journal = {Statistics \& Probability Letters},
	author = {Leemis, Lawrence M. and Shih, Li-Hsing and Reynertson, Kurt},
	month = sep,
	year = {1990},
	keywords = {proportional hazards model, simulation, Accelerated life model, Monte Carlo simulation, time dependent covariates, variate generation},
	pages = {335--339},
	file = {ScienceDirect Full Text PDF:/Users/alexwjung/Google Drive/library/storage/6MXBJLHS/Leemis et al. - 1990 - Variate generation for accelerated life and propor.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexwjung/Google Drive/library/storage/WHHBYHWK/0167715290900529.html:text/html},
}

@article{shih_variate_1993,
	title = {Variate generation for a nonhomogeneous poisson process with time dependent covariates},
	volume = {44},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659308811457},
	doi = {10.1080/00949659308811457},
	abstract = {Algorithms are developed for generating a sequence of event times from a nonhomogeneous Poisson process that is influenced by the values of covariates that vary with time. Closed form expressions for random variate generation are shown for several baseline intensity and link functions. Two specific models linking the baseline process to the general model are considered: the accelerated time model and the proportional intensity model. In the accelerated time model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where z is a covariate vector, ⋀0(t) is the baseline cumulative intensity function and ψ(z) is the link function. In the proportional intensity model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where λ0(t) is the baseline intensity function.},
	number = {3-4},
	urldate = {2021-03-12},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shih, Li-Hsing and Leemis, Lawrence M.},
	month = jan,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659308811457},
	keywords = {simulation, Covariates, Nonhomogeneous Poisson processes, Simulation, Variate generation},
	pages = {165--186},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/ZF5F8V29/Shih and Leemis - 1993 - Variate generation for a nonhomogeneous poisson pr.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/H6L3K4D2/00949659308811457.html:text/html},
}

@article{ngwa_generating_2019,
	title = {Generating survival times with time-varying covariates using the {Lambert} {W} {Function}},
	volume = {0},
	issn = {0361-0918},
	url = {https://doi.org/10.1080/03610918.2019.1648822},
	doi = {10.1080/03610918.2019.1648822},
	abstract = {Simulation studies provide an important statistical tool in evaluating survival methods, requiring an appropriate data-generating process to simulate data for an underlying statistical model. Many studies with time-to-event outcomes use the Cox proportional hazard model. While methods for simulating such data with time-invariant predictors have been described, methods for simulating data with time-varying covariates are sorely needed. Here, we describe an approach for generating data for the Cox proportional hazard model with time-varying covariates when event times follow an Exponential or Weibull distribution. For each distribution, we derive a closed-form expression to generate survival times and link the time-varying covariates with the hazard function. We consider a continuous time-varying covariate measured at regular intervals over time, as well as time-invariant covariates, in generating time-to-event data under a number of scenarios. Our results suggest this method can lead to simulation studies with reliable and robust estimation of the association parameter in Cox-Weibull and Cox-Exponential models.},
	number = {0},
	urldate = {2021-03-12},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Ngwa, Julius S. and Cabral, Howard J. and Cheng, Debbie M. and Gagnon, David R. and LaValley, Michael P. and Cupples, L. Adrienne},
	month = aug,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610918.2019.1648822},
	keywords = {simulation, Lambert W Function, Linear Mixed effects model, Longitudinal and survival data, Time-varying covariates, Two step approach},
	pages = {1--19},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/XNL3F56B/Ngwa et al. - 2019 - Generating survival times with time-varying covari.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/X3HELS3V/03610918.2019.html:text/html},
}

@article{harden_simulating_2019,
	title = {Simulating {Duration} {Data} for the {Cox} {Model}},
	volume = {7},
	issn = {2049-8470, 2049-8489},
	url = {https://www.cambridge.org/core/journals/political-science-research-and-methods/article/simulating-duration-data-for-the-cox-model/1945D7548766E76FB31C6C833976822E},
	doi = {10.1017/psrm.2018.19},
	abstract = {The Cox proportional hazards model is a popular method for duration analysis that is frequently the subject of simulation studies. However, no standard method exists for simulating durations directly from its data generating process because it does not assume a distributional form for the baseline hazard function. Instead, simulation studies typically rely on parametric survival distributions, which contradicts the primary motivation for employing the Cox model. We propose a method that generates a baseline hazard function at random by fitting a cubic spline to randomly drawn points. Durations drawn from this function match the Cox model’s inherent flexibility and improve the simulation’s generalizability. The method can be extended to include time-varying covariates and non-proportional hazards.},
	language = {en},
	number = {4},
	urldate = {2021-03-16},
	journal = {Political Science Research and Methods},
	author = {Harden, Jeffrey J. and Kropko, Jonathan},
	month = oct,
	year = {2019},
	note = {Publisher: Cambridge University Press},
	pages = {921--928},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/HIAJS4T7/Harden and Kropko - 2019 - Simulating Duration Data for the Cox Model.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/XTNVSMPU/1945D7548766E76FB31C6C833976822E.html:text/html},
}

@article{kalbfleisch_non-parametric_1978,
	title = {Non-{Parametric} {Bayesian} {Analysis} of {Survival} {Time} {Data}},
	volume = {40},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984758},
	abstract = {A Bayesian analysis of the semi-parametric regression and life model of Cox (1972) is given. The cumulative hazard function is modelled as a gamma process. Both estimation of the regression parameters and of the underlying survival distribution are considered. The results are compared to the results obtained by other approaches.},
	number = {2},
	urldate = {2021-03-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Kalbfleisch, John D.},
	year = {1978},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {214--221},
}

@article{kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2281868},
	doi = {10.2307/2281868},
	abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 ≤ t$_{\textrm{1}}$' ≤ t$_{\textrm{2}}$' ≤ ⋯ ≤ t$_{\textrm{N}}$'. Then {\textless}tex-math{\textgreater}\${\textbackslash}hat\{P\}(t) = {\textbackslash}prod\_r {\textbackslash}lbrack(N - r)/(N - r + 1){\textbackslash}rbrack\${\textless}/tex-math{\textgreater}, where r assumes those values for which t$_{\textrm{r}}$' ≤ t and for which t$_{\textrm{r}}$' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
	number = {282},
	urldate = {2021-03-26},
	journal = {Journal of the American Statistical Association},
	author = {Kaplan, E. L. and Meier, Paul},
	year = {1958},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {457--481},
	file = {JSTOR Full Text PDF:/Users/alexwjung/Google Drive/library/storage/ZV66L7YN/Kaplan and Meier - 1958 - Nonparametric Estimation from Incomplete Observati.pdf:application/pdf},
}

@article{breslow_covariance_1974,
	title = {Covariance analysis of censored survival data},
	volume = {30},
	issn = {0006-341X},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Breslow, N.},
	month = mar,
	year = {1974},
	pmid = {4813387},
	keywords = {Humans, Age Factors, Child, Preschool, Dactinomycin, Leukemia, Lymphoid, Leukocyte Count, Mercaptopurine, Methotrexate, Models, Biological, Nitrogen Mustard Compounds, Prognosis, Regression Analysis, Remission, Spontaneous, Statistics as Topic, Time Factors},
	pages = {89--99},
}

@article{crowley_note_1974,
	title = {A {Note} on {Some} {Recent} {Likelihoods} {Leading} to the {Log} {Rank} {Test}},
	volume = {61},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334736},
	doi = {10.2307/2334736},
	abstract = {A discussion is given of the derivation by Peto (1972) of the log rank test as the locally most powerful rank invariant test against certain Lehmann-type alternatives in the two-sample problem with censored data. The random censorship model is used to show that the test with this local optimality property may depend on the `censoring mechanism, even if censoring is applied equally to both groups. An explanation is offered of Peto's likelihood and of a special case of the conditional likelihood of Cox (1972). Reference is made to asymptotic relative efficiency results which demonstrate that the log rank test may not be fully efficient when censoring is not equal.},
	number = {3},
	urldate = {2021-03-26},
	journal = {Biometrika},
	author = {Crowley, John},
	year = {1974},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {533--538},
	file = {JSTOR Full Text PDF:/Users/alexwjung/Google Drive/library/storage/ZS799Z5C/Crowley - 1974 - A Note on Some Recent Likelihoods Leading to the L.pdf:application/pdf},
}

@book{aalen_survival_2008,
	address = {New York},
	series = {Statistics for {Biology} and {Health}},
	title = {Survival and {Event} {History} {Analysis}: {A} {Process} {Point} of {View}},
	isbn = {978-0-387-20287-7},
	shorttitle = {Survival and {Event} {History} {Analysis}},
	url = {https://www.springer.com/gp/book/9780387202877},
	abstract = {Time-to-event data are ubiquitous in fields such as medicine, biology, demography, sociology, economics and reliability theory. Recently, a need to analyze more complex event histories has emerged. Examples are individuals that move among several states, frailty that makes some units fail before others, internal time-dependent covariates, and the estimation of causal effects from observational data. The aim of this book is to bridge the gap between standard textbook models and a range of models where the dynamic structure of the data manifests itself fully. The common denominator of such models is stochastic processes. The authors show how counting processes, martingales, and stochastic integrals fit very nicely with censored data. Beginning with standard analyses such as Kaplan-Meier plots and Cox regression, the presentation progresses to the additive hazard model and recurrent event data. Stochastic processes are also used as natural models for individual frailty; they allow sensible interpretations of a number of surprising artifacts seen in population data. The stochastic process framework is naturally connected to causality. The authors show how dynamic path analyses can incorporate many modern causality ideas in a framework that takes the time aspect seriously. To make the material accessible to the reader, a large number of practical examples, mainly from medicine, are developed in detail. Stochastic processes are introduced in an intuitive and non-technical manner. The book is aimed at investigators who use event history methods and want a better understanding of the statistical concepts. It is suitable as a textbook for graduate courses in statistics and biostatistics. Odd O. Aalen is professor of medical statistics at the University of Oslo, Norway. His Ph.D. from the University of California, Berkeley in 1975 introduced counting processes and martingales in event history analysis. He has also contributed to numerous other areas of event history analysis, such as additive hazards regression, frailty, and causality through dynamic modeling. Ørnulf Borgan is professor of statistics at the University of Oslo, Norway. Since his Ph.D. in 1984 he has contributed extensively to event history analysis. He is co-author of the monograph Statistical Models Based on Counting Processes, and is editor of Scandinavian Journal of Statistics. Håkon K. Gjessing is professor of medical statistics at the Norwegian Institute of Public Health and the University of Bergen, Norway. Since his Ph.D. in probability in 1995, he has worked on a broad range of theoretical and applied problems in biostatistics.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Aalen, Odd and Borgan, Ornulf and Gjessing, Hakon},
	year = {2008},
	doi = {10.1007/978-0-387-68560-1},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/W4PYXWRH/9780387202877.html:text/html},
}

@book{ibrahim_bayesian_2001,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Bayesian {Survival} {Analysis}},
	isbn = {978-0-387-95277-2},
	url = {https://www.springer.com/gp/book/9780387952772},
	abstract = {Survival analysis arises in many fields of study including medicine, biology, engineering, public health, epidemiology, and economics. This book provides a comprehensive treatment of Bayesian survival analysis.Several topics are addressed, including parametric models, semiparametric models based on prior processes, proportional and non-proportional hazards models, frailty models, cure rate models, model selection and comparison, joint models for longitudinal and survival data, models with time varying covariates, missing covariate data, design and monitoring of clinical trials, accelerated failure time models, models for mulitivariate survival data, and special types of hierarchial survival models. Also various censoring schemes are examined including right and interval censored data. Several additional topics are discussed, including noninformative and informative prior specificiations, computing posterior qualities of interest, Bayesian hypothesis testing, variable selection, model selection with nonnested models, model checking techniques using Bayesian diagnostic methods, and Markov chain Monte Carlo (MCMC) algorithms for sampling from the posteiror and predictive distributions.The book presents a balance between theory and applications, and for each class of models discussed, detailed examples and analyses from case studies are presented whenever possible. The applications are all essentially from the health sciences, including cancer, AIDS, and the environment. The book is intended as a graduate textbook or a reference book for a one semester course at the advanced masters or Ph.D. level. This book would be most suitable for second or third year graduate students in statistics or biostatistics. It would also serve as a useful reference book for applied or theoretical researchers as well as practitioners.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Sinha, Debajyoti},
	year = {2001},
	doi = {10.1007/978-1-4757-3447-8},
}

@book{andersen_statistical_1993,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Statistical {Models} {Based} on {Counting} {Processes}},
	isbn = {978-0-387-94519-4},
	url = {https://www.springer.com/gp/book/9780387945194},
	abstract = {Modern survival analysis and more general event history analysis may be effectively handled in the mathematical framework of counting processes, stochastic integration, martingale central limit theory and product integration. This book presents this theory, which has been the subject of an intense research activity during the past one-and-a- half decades. The exposition of the theory is integrated with careful presentation of many practical examples, almost exclusively from the authors' own experience, with detailed numerical and graphical illustrations. Statistical Models Based on Counting Processes may be viewed as a research monograph for mathematical statisticians and biostatisticians, although almost all methods are given in concrete detail to be used in practice by other mathematically oriented researchers studying event histories (demographers, econometricians, epidemiologists, actuarial mathematicians, reliabilty engineers and biologists). Much of the material has so far only been available in the journal literature (if at all), and so a wide variety of researchers will find this an invaluable survey of the subject."This book is a masterful account of the counting process approach...is certain to be the standard reference for the area, and should be on the bookshelf of anyone interested in event-history analysis." International Statistical Institute Short Book Reviews "...this impressive reference, which contains a a wealth of powerful mathematics, practical examples, and analytic insights, as well as a complete integration of historical developments and recent advances in event history analysis." Journal of the American Statistical Association},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Andersen, Per Kragh and Borgan, Ornulf and Gill, Richard D. and Keiding, Niels},
	year = {1993},
	doi = {10.1007/978-1-4612-4348-9},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/JUUZIGWI/9780387945194.html:text/html},
}

@article{lee_threshold_2006,
	title = {Threshold {Regression} for {Survival} {Analysis}: {Modeling} {Event} {Times} by a {Stochastic} {Process} {Reaching} a {Boundary}},
	volume = {21},
	issn = {0883-4237},
	shorttitle = {Threshold {Regression} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/0708.0346},
	doi = {10.1214/088342306000000330},
	abstract = {Many researchers have investigated ﬁrst hitting times as models for survival data. First hitting times arise naturally in many types of stochastic processes, ranging from Wiener processes to Markov chains. In a survival context, the state of the underlying process represents the strength of an item or the health of an individual. The item fails or the individual experiences a clinical endpoint when the process reaches an adverse threshold state for the ﬁrst time. The time scale can be calendar time or some other operational measure of degradation or disease progression. In many applications, the process is latent (i.e., unobservable). Threshold regression refers to ﬁrst-hitting-time models with regression structures that accommodate covariate data. The parameters of the process, threshold state and time scale may depend on the covariates. This paper reviews aspects of this topic and discusses fruitful avenues for future research.},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Statistical Science},
	author = {Lee, Mei-Ling Ting and Whitmore, G. A.},
	month = nov,
	year = {2006},
	note = {arXiv: 0708.0346},
	keywords = {Statistics - Methodology},
	file = {Lee and Whitmore - 2006 - Threshold Regression for Survival Analysis Modeli.pdf:/Users/alexwjung/Google Drive/library/storage/EHH9EBPL/Lee and Whitmore - 2006 - Threshold Regression for Survival Analysis Modeli.pdf:application/pdf},
}

@article{nan_asymptotic_2009,
	title = {Asymptotic {Theory} for the {Semiparametric} {Accelerated} {Failure} {Time} {Model} with {Missing} {Data}},
	volume = {37},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/30243708},
	abstract = {We consider a class of doubly weighted rank-based estimating methods for the transformation (or accelerated failure time) model with missing data as arise, for example, in case-cohort studies. The weights considered may not be predictable as required in a martingale stochastic process formulation. We treat the general problem as a semiparametric estimating equation problem and provide proofs of asymptotic properties for the weighted estimators, with either true weights or estimated weights, by using empirical process theory where martingale theory may fail. Simulations show that the outcome-dependent weighted method works well for finite samples in case-cohort studies and improves efficiency compared to methods based on predictable weights. Further, it is seen that the method is even more efficient when estimated weights are used, as is commonly the case in the missing data literature. The Gehan censored data Wilcoxon weights are found to be surprisingly efficient in a wide class of problems.},
	number = {5A},
	urldate = {2021-03-26},
	journal = {The Annals of Statistics},
	author = {Nan, Bin and Kalbfleisch, John D. and Yu, Menggang},
	year = {2009},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2351--2376},
}

@book{kalbfleisch_statistical_2011,
	title = {The {Statistical} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-118-03123-0},
	abstract = {* Contains additional discussion and examples on left truncation as well as material on more general censoring and truncation patterns. * Introduces the martingale and counting process formulation swil lbe in a new chapter. * Develops multivariate failure time data in a separate chapter and extends the material on Markov and semi Markov formulations. * Presents new examples and applications of data analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Kalbfleisch, John D. and Prentice, Ross L.},
	month = jan,
	year = {2011},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{witten_survival_2010,
	title = {Survival analysis with high-dimensional covariates},
	volume = {19},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806549/},
	doi = {10.1177/0962280209105024},
	abstract = {In recent years, breakthroughs in biomedical technology have led to a wealth of data in which the number of features (for instance, genes on which expression measurements are available) exceeds the number of observations (e.g. patients). Sometimes survival outcomes are also available for those same observations. In this case, one might be interested in (a) identifying features that are associated with survival (in a univariate sense), and (b) developing a multivariate model for the relationship between the features and survival that can be used to predict survival in a new observation. Due to the high dimensionality of this data, most classical statistical methods for survival analysis cannot be applied directly. Here, we review a number of methods from the literature that address these two problems.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Statistical methods in medical research},
	author = {Witten, Daniela M and Tibshirani, Robert},
	month = feb,
	year = {2010},
	pmid = {19654171},
	pmcid = {PMC4806549},
	pages = {29--51},
	file = {PubMed Central Full Text PDF:/Users/alexwjung/Google Drive/library/storage/W5ESYY5E/Witten and Tibshirani - 2010 - Survival analysis with high-dimensional covariates.pdf:application/pdf},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	language = {eng},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
}

@article{simon_regularization_2011,
	title = {Regularization {Paths} for {Cox}'s {Proportional} {Hazards} {Model} via {Coordinate} {Descent}},
	volume = {39},
	issn = {1548-7660},
	doi = {10.18637/jss.v039.i05},
	abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
	language = {eng},
	number = {5},
	journal = {Journal of Statistical Software},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	month = mar,
	year = {2011},
	pmid = {27065756},
	pmcid = {PMC4824408},
	keywords = {Cox model, elastic net, lasso, survival},
	pages = {1--13},
	file = {Full Text:/Users/alexwjung/Google Drive/library/storage/3ET4BVZ5/Simon et al. - 2011 - Regularization Paths for Cox's Proportional Hazard.pdf:application/pdf},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2021-03-28},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/MQ8CZPWZ/Ishwaran et al. - 2008 - Random survival forests.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/CM69P6RB/08-AOAS169.html:text/html},
}

@article{tibshirani_lasso_1997,
	title = {The lasso method for variable selection in the {Cox} model},
	volume = {16},
	issn = {0277-6715},
	doi = {10.1002/(sici)1097-0258(19970228)16:4<385::aid-sim380>3.0.co;2-3},
	abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
	language = {eng},
	number = {4},
	journal = {Statistics in Medicine},
	author = {Tibshirani, R.},
	month = feb,
	year = {1997},
	pmid = {9044528},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Karnofsky Performance Status, Likelihood Functions, Liver Cirrhosis, Lung Neoplasms, Randomized Controlled Trials as Topic},
	pages = {385--395},
	file = {Submitted Version:/Users/alexwjung/Google Drive/library/storage/D84HQVRE/Tibshirani - 1997 - The lasso method for variable selection in the Cox.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288},
}

@article{yang_cocktail_2013,
	title = {A cocktail algorithm for solving the elastic net penalized {Cox}’s regression in high dimensions},
	volume = {6},
	number = {2},
	journal = {Statistics and its Interface},
	author = {Yang, Yi and Zou, Hui},
	year = {2013},
	note = {Publisher: International Press of Boston},
	pages = {167--173},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Bootstrap {Methods}},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full},
	doi = {10.1214/aos/1176344552},
	abstract = {We discuss the following problem: given a random sample \${\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ from an unknown probability distribution \$F\$, estimate the sampling distribution of some prespecified random variable \$R({\textbackslash}mathbf\{X\}, F)\$, on the basis of the observed data \${\textbackslash}mathbf\{x\}\$. (Standard jackknife theory gives an approximate mean and variance in the case \$R({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta\$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	number = {1},
	urldate = {2021-03-28},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G05, 62G15, 62H30, 62J05, bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
	pages = {1--26},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/9L22396T/Efron - 1979 - Bootstrap Methods Another Look at the Jackknife.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/BKCXU2AJ/1176344552.html:text/html},
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and {Its} {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/3085904},
	abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
	number = {456},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1348--1360},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214508000000337},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/FHCAJUSR/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/6TQYM6FF/016214508000000337.html:text/html},
}

@article{qian_fast_2019,
	title = {A fast and flexible algorithm for solving the lasso in large-scale and ultrahigh-dimensional problems},
	journal = {BioRxiv},
	author = {Qian, Junyang and Du, Wenfei and Tanigawa, Yosuke and Aguirre, Matthew and Tibshirani, Robert and Rivas, Manuel A and Hastie, Trevor},
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {630079},
}

@article{williamson_factors_2020,
	title = {Factors associated with {COVID}-19-related death using {OpenSAFELY}},
	volume = {584},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2521-4},
	doi = {10.1038/s41586-020-2521-4},
	abstract = {Coronavirus disease 2019 (COVID-19) has rapidly affected mortality worldwide1. There is unprecedented urgency to understand who is most at risk of severe outcomes, and this requires new approaches for the timely analysis of large datasets. Working on behalf of NHS England, we created OpenSAFELY—a secure health analytics platform that covers 40\% of all patients in England and holds patient data within the existing data centre of a major vendor of primary care electronic health records. Here we used OpenSAFELY to examine factors associated with COVID-19-related death. Primary care records of 17,278,392 adults were pseudonymously linked to 10,926 COVID-19-related deaths. COVID-19-related death was associated with: being male (hazard ratio (HR) 1.59 (95\% confidence interval 1.53–1.65)); greater age and deprivation (both with a strong gradient); diabetes; severe asthma; and various other medical conditions. Compared with people of white ethnicity, Black and South Asian people were at higher risk, even after adjustment for other factors (HR 1.48 (1.29–1.69) and 1.45 (1.32–1.58), respectively). We have quantified a range of clinical factors associated with COVID-19-related death in one of the largest cohort studies on this topic so far. More patient records are rapidly being added to OpenSAFELY, we will update and extend our results regularly.},
	language = {en},
	number = {7821},
	urldate = {2021-03-28},
	journal = {Nature},
	author = {Williamson, Elizabeth J. and Walker, Alex J. and Bhaskaran, Krishnan and Bacon, Seb and Bates, Chris and Morton, Caroline E. and Curtis, Helen J. and Mehrkar, Amir and Evans, David and Inglesby, Peter and Cockburn, Jonathan and McDonald, Helen I. and MacKenna, Brian and Tomlinson, Laurie and Douglas, Ian J. and Rentsch, Christopher T. and Mathur, Rohini and Wong, Angel Y. S. and Grieve, Richard and Harrison, David and Forbes, Harriet and Schultze, Anna and Croker, Richard and Parry, John and Hester, Frank and Harper, Sam and Perera, Rafael and Evans, Stephen J. W. and Smeeth, Liam and Goldacre, Ben},
	month = aug,
	year = {2020},
	note = {Number: 7821
Publisher: Nature Publishing Group},
	pages = {430--436},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/D2EECGUC/Williamson et al. - 2020 - Factors associated with COVID-19-related death usi.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/MKV8QRY4/s41586-020-2521-4.html:text/html},
}

@article{murray_flexible_2016,
	title = {Flexible {Bayesian} {Survival} {Modeling} with {Semiparametric} {Time}-{Dependent} and {Shape}-{Restricted} {Covariate} {Effects}},
	volume = {11},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-2/Flexible-Bayesian-Survival-Modeling-with-Semiparametric-Time-Dependent-and-Shape/10.1214/15-BA954.full},
	doi = {10.1214/15-BA954},
	abstract = {Presently, there are few options with available software to perform a fully Bayesian analysis of time-to-event data wherein the hazard is estimated semi- or non-parametrically. One option is the piecewise exponential model, which requires an often unrealistic assumption that the hazard is piecewise constant over time. The primary aim of this paper is to construct a tractable semiparametric alternative to the piecewise exponential model that assumes the hazard is continuous, and to provide modifiable, user-friendly software that allows the use of these methods in a variety of settings. To accomplish this aim, we use a novel model formulation for the log-hazard based on a low-rank thin plate linear spline that readily facilitates adjustment for covariates with time-dependent and proportional hazards effects, possibly subject to shape restrictions. We investigate the performance of our model choices via simulation. We then analyze colorectal cancer data from a clinical trial comparing the effectiveness of two novel treatment regimes relative to the standard of care for overall survival. We estimate a time-dependent hazard ratio for each novel regime relative to the standard of care while adjusting for the effect of aspartate transaminase, a biomarker of liver function, that is subject to a non-decreasing shape restriction.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Murray, Thomas A. and Hobbs, Brian P. and Sargent, Daniel J. and Carlin, Bradley P.},
	month = jun,
	year = {2016},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Survival analysis, 62F15, 62F30, 62N86, Bayesian methods, colorectal cancer, penalized splines, semiparametric methods, shape-restricted effects, time-dependent effects},
	pages = {381--402},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/D9XZBZHU/Murray et al. - 2016 - Flexible Bayesian Survival Modeling with Semiparam.pdf:application/pdf},
}

@article{sharef_bayesian_2010,
	title = {Bayesian adaptive {B}-spline estimation in proportional hazards frailty models},
	volume = {4},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-4/issue-none/Bayesian-adaptive-B-spline-estimation-in-proportional-hazards-frailty-models/10.1214/10-EJS566.full},
	doi = {10.1214/10-EJS566},
	abstract = {Frailty models derived from the proportional hazards regression model are frequently used to analyze clustered right-censored survival data. We propose a semiparametric Bayesian methodology for this purpose, modeling both the unknown baseline hazard and density of the random effects using mixtures of B-splines. The posterior distributions for all regression coefficients and spline parameters are obtained using Markov Chain Monte Carlo (MCMC). The methodology permits the use of weighted mixtures of parametric and nonparametric components in modeling the hazard function and frailty distribution; in addition, the spline knots may also be selected adaptively using reversible-jump MCMC. Simulations indicate that the method produces smooth and accurate posterior hazard and frailty density estimates. The Bayesian approach not only produces point estimators that outperform existing approaches in certain circumstances, but also offers a wealth of information about the parameters of interest in the form of MCMC samples from the joint posterior probability distribution. We illustrate the adaptability of the method with data from a study of congestive heart failure.},
	number = {none},
	urldate = {2021-03-29},
	journal = {Electronic Journal of Statistics},
	author = {Sharef, Emmanuel and Strawderman, Robert L. and Ruppert, David and Cowen, Mark and Halasyamani, Lakshmi},
	month = jan,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Survival analysis, frailty distribution, Hazard regression, heart failure, knot selection, random effect density, re-hospitalization, reversible-jump MCMC},
	pages = {606--642},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/2WFCJ2XJ/Sharef et al. - 2010 - Bayesian adaptive B-spline estimation in proportio.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/PPTNE33F/10-EJS566.html:text/html},
}

@article{alsefri_bayesian_2020,
	title = {Bayesian joint modelling of longitudinal and time to event data: a methodological review},
	volume = {20},
	issn = {1471-2288},
	shorttitle = {Bayesian joint modelling of longitudinal and time to event data},
	url = {https://doi.org/10.1186/s12874-020-00976-2},
	doi = {10.1186/s12874-020-00976-2},
	abstract = {In clinical research, there is an increasing interest in joint modelling of longitudinal and time-to-event data, since it reduces bias in parameter estimation and increases the efficiency of statistical inference. Inference and prediction from frequentist approaches of joint models have been extensively reviewed, and due to the recent popularity of data-driven Bayesian approaches, a review on current Bayesian estimation of joint model is useful to draw recommendations for future researches.},
	number = {1},
	urldate = {2021-03-29},
	journal = {BMC Medical Research Methodology},
	author = {Alsefri, Maha and Sudell, Maria and García-Fiñana, Marta and Kolamunnage-Dona, Ruwanthi},
	month = apr,
	year = {2020},
	keywords = {Bayesian estimation, Dynamic prediction, Joint models, Longitudinal outcomes, Time-to-event},
	pages = {94},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/4WN5Y46P/Alsefri et al. - 2020 - Bayesian joint modelling of longitudinal and time .pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/M6MYJ8G4/s12874-020-00976-2.html:text/html},
}

@article{alvares_bayesian_2021,
	title = {Bayesian survival analysis with {BUGS}},
	journal = {Statistics in Medicine},
	author = {Alvares, Danilo and Lázaro, Elena and Gómez-Rubio, Virgilio and Armero, Carmen},
	year = {2021},
	note = {Publisher: Wiley Online Library},
}

@article{hennerfeind_geoadditive_2006,
	title = {Geoadditive {Survival} {Models}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000348},
	doi = {10.1198/016214506000000348},
	abstract = {Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoadditive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting.},
	number = {475},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Hennerfeind, Andrea and Brezger, Andreas and Fahrmeir, Ludwig},
	month = sep,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000000348},
	keywords = {Bayesian hazard rate model, Markov chain Monte Carlo, Markov random field, Penalized spline, Semiparametric modeling, Spatial survival data},
	pages = {1065--1075},
	file = {Snapshot:/Users/alexwjung/Google Drive/library/storage/5WEB4GE6/016214506000000348.html:text/html;Submitted Version:/Users/alexwjung/Google Drive/library/storage/89ZNMRJM/Hennerfeind et al. - 2006 - Geoadditive Survival Models.pdf:application/pdf},
}

@article{cai_mixed_2002,
	title = {Mixed {Model}-{Based} {Hazard} {Estimation}},
	volume = {11},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/1391161},
	abstract = {This article proposes a new method for estimation of the hazard function from a set of censored failure time data, with a view to extending the general approach to more complicated models. The approach is based on a mixed model representation of penalized spline hazard estimators. One payoff is the automation of the smoothing parameter choice through restricted maximum likelihood. Another is the option to use standard mixed model software for automatic hazard estimation.},
	number = {4},
	urldate = {2021-03-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cai, T. and Hyndman, R. J. and Wand, M. P.},
	year = {2002},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {784--798},
}

@article{muller_nonparametric_2004,
	title = {Nonparametric {Bayesian} {Data} {Analysis}},
	volume = {19},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Nonparametric-Bayesian-Data-Analysis/10.1214/088342304000000017.full},
	doi = {10.1214/088342304000000017},
	abstract = {We review the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem we review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models.},
	number = {1},
	urldate = {2021-03-29},
	journal = {Statistical Science},
	author = {Müller, Peter and Quintana, Fernando A.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Survival analysis, Density estimation, Dirichlet process, Pólya tree, random probability model (RPM), regression},
	pages = {95--110},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/N92GVT2Y/Müller and Quintana - 2004 - Nonparametric Bayesian Data Analysis.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/XNBIKUA7/088342304000000017.html:text/html},
}

@article{muller_bayesian_2013,
	title = {Bayesian {Nonparametric} {Inference} – {Why} and {How}},
	volume = {8},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-8/issue-2/Bayesian-Nonparametric-Inference--Why-and-How/10.1214/13-BA811.full},
	doi = {10.1214/13-BA811},
	abstract = {We review inference under models with nonparametric Bayesian (BNP) priors. The discussion follows a set of examples for some common inference problems. The examples are chosen to highlight problems that are challenging for standard parametric inference. We discuss inference for density estimation, clustering, regression and for mixed effects models with random effects distributions. While we focus on arguing for the need for the flexibility of BNP models, we also review some of the more commonly used BNP models, thus hopefully answering a bit of both questions, why and how to use BNP. This review was sponsored by the Bayesian Nonparametrics Section of ISBA (ISBA/BNP). The authors thank the section officers for the support and encouragement.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Müller, Peter and Mitra, Riten},
	month = jun,
	year = {2013},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Dirichlet process, dependent Dirichlet process, nonparametric models, Polya tree},
	pages = {269--302},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/PERYPIHH/Müller and Mitra - 2013 - Bayesian Nonparametric Inference – Why and How.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/VLFKC9M5/13-BA811.html:text/html},
}

@article{sinha_semiparametric_1993,
	title = {Semiparametric {Bayesian} {Analysis} of {Multiple} {Event} {Time} {Data}},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290789},
	doi = {10.2307/2290789},
	abstract = {Multiple event time data (e.g., carcinogenic growths in different times and locations, multiple attacks of cardiac arrest) arise in various medical studies. A Bayesian analysis of such data based on proportional intensity model of multiple event time data is presented in this paper. The Bayesian structure is somewhat analogous to that used by Kalbfleisch in a proportional hazard model. An unobserved random frailty component is used in the proportional intensity model to take care of heterogeneity among the intensity processes in different subjects. The Monte Carlo method of sampling from multivariate distributions, the so-called Gibbs sampler, is used to sample from the joint posterior distribution of the unknown parameters. The methodology developed here is exemplified with the well-known data set on rat tumors of Gail, Santner, and Brown.},
	number = {423},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {979--983},
}

@article{qiou_multivariate_1999,
	title = {Multivariate survival analysis with positive stable frailties},
	volume = {55},
	issn = {0006-341X},
	doi = {10.1111/j.0006-341x.1999.00637.x},
	abstract = {In this paper, we describe Bayesian modeling of dependent multivariate survival data using positive stable frailty distributions. A flexible baseline hazard formulation using a piecewise exponential model with a correlated prior process is used. The estimation of the stable law parameter together with the parameters of the (conditional) proportional hazards model is facilitated by a modified Gibbs sampling procedure. The methodology is illustrated on kidney infection data (McGilchrist and Aisbett, 1991).},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Qiou, Z. and Ravishanker, N. and Dey, D. K.},
	month = jun,
	year = {1999},
	pmid = {11318227},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Algorithms, Bayes Theorem, Biometry, Female, Kidney Diseases, Male, Multivariate Analysis},
	pages = {637--644},
}

@article{sinha_semiparametric_1997,
	title = {Semiparametric {Bayesian} {Analysis} of {Survival} {Data}},
	volume = {92},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2965586},
	doi = {10.2307/2965586},
	abstract = {This review article investigates the potential of Bayes methods for the analysis of survival data using semiparametric models based on either the hazard or the intensity function. The nonparametric part of every model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, including sampling-based algorithms, are used to find posterior estimates of several quantities of interest even when dealing with complex models and unusual data structures. The methodologies developed herein are motivated and aimed at analyzing some common types of survival data from different medical studies; here we focus on univariate survival data in the presence of fixed and time-dependent covariates, multiple event-time data for repeated nonfatal events, and multivariate survival data (subjects are related; e.g., families or litters), each patient with interval-censored infection time and interval-censored disease occurrence time in tandem [e.g., patients with acquired immunodeficiency syndrome (AIDS) and other infectious diseases with long incubation times]. Bayesian exploratory data analysis (EDA) methods and diagnostics for model selection and model assessment are considered for each case. Special attention is given to tests of the parametric modeling assumptions and to censoring.},
	number = {439},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti and Dey, Dipak K.},
	year = {1997},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1195--1212},
}

@incollection{laud_bayesian_1998,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Nonparametric} and {Covariate} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-4612-1732-9},
	url = {https://doi.org/10.1007/978-1-4612-1732-9_11},
	abstract = {A Bayesian analysis of the semi-parametric regression model of Cox (1972) is given. The cumulative hazard function is modelled as a beta process. The posterior distribution of the regression parameters and the survival function are obtained using a combination of recent Monte Carlo methods. An illustrative analysis within the context of survival time data is given.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {Practical {Nonparametric} and {Semiparametric} {Bayesian} {Statistics}},
	publisher = {Springer},
	author = {Laud, Purushottam W. and Damien, Paul and Smith, Adrian F. M.},
	editor = {Dey, Dipak and Müller, Peter and Sinha, Debajyoti},
	year = {1998},
	doi = {10.1007/978-1-4612-1732-9_11},
	keywords = {Cumulative Hazard, Frailty Model, Hazard Rate, Markov Chain Monte Carlo Method, Posterior Distribution},
	pages = {213--225},
	file = {Submitted Version:/Users/alexwjung/Google Drive/library/storage/IKLEXZTT/Laud et al. - 1998 - Bayesian Nonparametric and Covariate Analysis of F.pdf:application/pdf},
}

@article{nietobarajas_markov_2002,
	title = {Markov {Beta} and {Gamma} {Processes} for {Modelling} {Hazard} {Rates}},
	volume = {29},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00298},
	doi = {https://doi.org/10.1111/1467-9469.00298},
	abstract = {This paper generalizes the discrete time independent increment beta process of Hjort (1990), for modelling discrete failure times, and also generalizes the independent gamma process for modelling piecewise constant hazard rates (Walker and Mallick, 1997). The generalizations are from independent increment to Markov increment prior processes allowing the modelling of smoothness. We derive posterior distributions and undertake a full Bayesian analysis.},
	language = {en},
	number = {3},
	urldate = {2021-03-29},
	journal = {Scandinavian Journal of Statistics},
	author = {Nieto‐Barajas, Luis E. and Walker, Stephen G.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9469.00298},
	keywords = {survival analysis, Bayes non-parametrics, beta process, gamma process, Markov process, stationary process},
	pages = {413--424},
}

@article{royston_flexible_2002,
	title = {Flexible parametric proportional-hazards and proportional-odds models for censored survival data, with application to prognostic modelling and estimation of treatment effects},
	volume = {21},
	issn = {0277-6715},
	doi = {10.1002/sim.1203},
	abstract = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, non-proportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer.},
	language = {eng},
	number = {15},
	journal = {Statistics in Medicine},
	author = {Royston, Patrick and Parmar, Mahesh K. B.},
	month = aug,
	year = {2002},
	pmid = {12210632},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Models, Biological, Prognosis, Female, Antineoplastic Agents, Breast Neoplasms, Carcinoma, Transitional Cell, Treatment Outcome, Urinary Bladder Neoplasms},
	pages = {2175--2197},
}

@article{teh_consistency_2016,
	title = {Consistency and fluctuations for stochastic gradient {Langevin} dynamics},
	volume = {17},
	journal = {Journal of Machine Learning Research},
	author = {Teh, Yee Whye and Thiery, Alexandre H and Vollmer, Sebastian J},
	year = {2016},
	note = {Publisher: Journal of Machine Learning Research},
}

@inproceedings{betancourt_fundamental_2015,
	title = {The fundamental incompatibility of scalable {Hamiltonian} {Monte} {Carlo} and naive data subsampling},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Betancourt, Michael},
	year = {2015},
	pages = {533--540},
}

@inproceedings{bardenet_adaptive_2014,
	title = {An adaptive subsampling approach for mcmc inference in large datasets},
	booktitle = {Proceedings of {The} 31st {International} {Conference} on {Machine} {Learning}},
	author = {Bardenet, Rémi and Doucet, Arnaud and Holmes, Chris},
	year = {2014},
	pages = {405--413},
}

@article{quiroz_speeding_2019,
	title = {Speeding {Up} {MCMC} by {Efficient} {Data} {Subsampling}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2018.1448827},
	doi = {10.1080/01621459.2018.1448827},
	abstract = {We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online.},
	number = {526},
	urldate = {2021-03-30},
	journal = {Journal of the American Statistical Association},
	author = {Quiroz, Matias and Kohn, Robert and Villani, Mattias and Tran, Minh-Ngoc},
	month = apr,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2018.1448827},
	keywords = {Bayesian inference, Big Data, Block pseudo-marginal, Correlated pseudo-marginal, Estimated likelihood, Survey sampling},
	pages = {831--843},
	file = {Full Text PDF:/Users/alexwjung/Google Drive/library/storage/23JTC69E/Quiroz et al. - 2019 - Speeding Up MCMC by Efficient Data Subsampling.pdf:application/pdf;Snapshot:/Users/alexwjung/Google Drive/library/storage/NZRLXJW6/01621459.2018.html:text/html},
}

@article{therneau_package_2015,
	title = {Package ‘survival’},
	volume = {128},
	number = {10},
	journal = {R Top Doc},
	author = {Therneau, Terry M and Lumley, Thomas},
	year = {2015},
	pages = {28--33},
}

@article{arjas_filtering_1992,
	title = {Filtering the histories of a partially observed marked point process},
	volume = {40},
	number = {2},
	journal = {Stochastic processes and their applications},
	author = {Arjas, Elja and Haara, Pentti and Norros, Ikka},
	year = {1992},
	note = {Publisher: Elsevier},
	pages = {225--250},
}

@article{arjas_marked_1984,
	title = {A marked point process approach to censored failure data with complicated covariates},
	journal = {Scandinavian Journal of Statistics},
	author = {Arjas, Elja and Haara, Pentti},
	year = {1984},
	note = {Publisher: JSTOR},
	pages = {193--209},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki and {others}},
	year = {2017},
	note = {Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	pages = {5018--5051},
}

@article{nelson_theory_1972,
	title = {Theory and applications of hazard plotting for censored failure data},
	volume = {14},
	number = {4},
	journal = {Technometrics},
	author = {Nelson, Wayne},
	year = {1972},
	note = {Publisher: Taylor \& Francis},
	pages = {945--966},
}

@article{aalen_nonparametric_1978,
	title = {Nonparametric inference for a family of counting processes},
	journal = {The Annals of Statistics},
	author = {Aalen, Odd},
	year = {1978},
	note = {Publisher: JSTOR},
	pages = {701--726},
}

@inproceedings{ranganath_black_2014,
	title = {Black box variational inference},
	booktitle = {Artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
	year = {2014},
	pages = {814--822},
}

@book{wainwright_graphical_2008,
	title = {Graphical models, exponential families, and variational inference},
	publisher = {Now Publishers Inc},
	author = {Wainwright, Martin J and Jordan, Michael Irwin},
	year = {2008},
}

@article{mohamed_monte_2019,
	title = {Monte carlo gradient estimation in machine learning},
	journal = {arXiv preprint arXiv:1906.10652},
	author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
	year = {2019},
}

@article{laurie_surgical_1989,
	title = {Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. {The} {North} {Central} {Cancer} {Treatment} {Group} and the {Mayo} {Clinic}.},
	volume = {7},
	number = {10},
	journal = {Journal of Clinical Oncology},
	author = {Laurie, John A and Moertel, Charles G and Fleming, Thomas R and Wieand, Harry S and Leigh, John E and Rubin, Jebal and McCormack, Greg W and Gerstner, James B and Krook, James E and Malliard, James},
	year = {1989},
	pages = {1447--1456},
}

@article{loprinzi_prospective_1994,
	title = {Prospective evaluation of prognostic variables from patient-completed questionnaires. {North} {Central} {Cancer} {Treatment} {Group}.},
	volume = {12},
	number = {3},
	journal = {Journal of Clinical Oncology},
	author = {Loprinzi, Charles Lawrence and Laurie, John A and Wieand, H Sam and Krook, James E and Novotny, Paul J and Kugler, John W and Bartel, Joan and Law, Marlys and Bateman, Marilyn and Klatt, Nancy E},
	year = {1994},
	pages = {601--607},
}

@article{crowley_covariance_1977,
	title = {Covariance analysis of heart transplant survival data},
	volume = {72},
	number = {357},
	journal = {Journal of the American Statistical Association},
	author = {Crowley, John and Hu, Marie},
	year = {1977},
	note = {Publisher: Taylor \& Francis},
	pages = {27--36},
}

@article{allen_nonalcoholic_2018,
	title = {Nonalcoholic fatty liver disease incidence and impact on metabolic burden and death: a 20 year-community study},
	volume = {67},
	number = {5},
	journal = {Hepatology},
	author = {Allen, Alina M and Therneau, Terry M and Larson, Joseph J and Coward, Alexandra and Somers, Virend K and Kamath, Patrick S},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1726--1736},
}

@article{muhammad_epic-survival_2021,
	title = {{EPIC}-{Survival}: {End}-to-end {Part} {Inferred} {Clustering} for {Survival} {Analysis}, {Featuring} {Prognostic} {Stratification} {Boosting}},
	shorttitle = {{EPIC}-{Survival}},
	url = {http://arxiv.org/abs/2101.11085},
	abstract = {Histopathology-based survival modelling has two major hurdles. Firstly, a well-performing survival model has minimal clinical application if it does not contribute to the stratiﬁcation of a cancer patient cohort into different risk groups, preferably driven by histologic morphologies. In the clinical setting, individuals are not given speciﬁc prognostic predictions, but are rather predicted to lie within a risk group which has a general survival trend. Thus, It is imperative that a survival model produces well-stratiﬁed risk groups. Secondly, until now, survival modelling was done in a two-stage approach (encoding and aggregation). The massive amount of pixels in digitized whole slide images were never utilized to their fullest extent due to technological constraints on data processing, forcing decoupled learning. EPIC-Survival bridges encoding and aggregation into an end-to-end survival modelling approach, while introducing stratiﬁcation boosting to encourage the model to not only optimize ranking, but also to discriminate between risk groups. In this study we show that EPIC-Survival performs better than other approaches in modelling intrahepatic cholangiocarcinoma, a historically difﬁcult cancer to model. Further, we show that stratiﬁcation boosting improves further improves model performance, resulting in a concordance-index of 0.880 on a held-out test set. Finally, we were able to identify speciﬁc histologic differences, not commonly sought out in ICC, between low and high risk groups.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2101.11085 [cs]},
	author = {Muhammad, Hassan and Xie, Chensu and Sigel, Carlie S. and Doukas, Michael and Alpert, Lindsay and Fuchs, Thomas J.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.11085},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Muhammad et al. - 2021 - EPIC-Survival End-to-end Part Inferred Clustering.pdf:/Users/alexwjung/Google Drive/library/storage/5KFMIIAC/Muhammad et al. - 2021 - EPIC-Survival End-to-end Part Inferred Clustering.pdf:application/pdf},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Publisher: Wiley Online Library},
	pages = {389--402},
}

@article{gelman_bayesian_2020,
	title = {Bayesian workflow},
	journal = {arXiv preprint arXiv:2011.01808},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	year = {2020},
}

@inproceedings{yao_yes_2018,
	title = {Yes, but did it work?: {Evaluating} variational inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	year = {2018},
	pages = {5581--5590},
}

@article{virani_heart_2021,
	title = {Heart disease and stroke statistics—2021 update: a report from the {American} {Heart} {Association}},
	volume = {143},
	number = {8},
	journal = {Circulation},
	author = {Virani, Salim S and Alonso, Alvaro and Aparicio, Hugo J and Benjamin, Emelia J and Bittencourt, Marcio S and Callaway, Clifton W and Carson, April P and Chamberlain, Alanna M and Cheng, Susan and Delling, Francesca N and {others}},
	year = {2021},
	note = {Publisher: Am Heart Assoc},
	pages = {e254--e743},
}

@article{kannel_lessons_1976,
	title = {Some lessons in cardiovascular epidemiology from {Framingham}},
	volume = {37},
	number = {2},
	journal = {The American journal of cardiology},
	author = {Kannel, William B},
	year = {1976},
	note = {Publisher: Elsevier},
	pages = {269--282},
}

@article{dawber_epidemiological_1951,
	title = {Epidemiological approaches to heart disease: the {Framingham} {Study}},
	volume = {41},
	number = {3},
	journal = {American Journal of Public Health and the Nations Health},
	author = {Dawber, Thomas R and Meadors, Gilcin F and Moore Jr, Felix E},
	year = {1951},
	note = {Publisher: American Public Health Association},
	pages = {279--286},
}

@article{mahmood_framingham_2014,
	title = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease: a historical perspective},
	volume = {383},
	number = {9921},
	journal = {The lancet},
	author = {Mahmood, Syed S and Levy, Daniel and Vasan, Ramachandran S and Wang, Thomas J},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {999--1008},
}

@article{sudlow_uk_2015,
	title = {{UK} biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age},
	volume = {12},
	number = {3},
	journal = {Plos med},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and {others}},
	year = {2015},
	note = {Publisher: Public Library of Science},
	pages = {e1001779},
}

@article{carey_prevention_2018,
	title = {Prevention, detection, evaluation, and management of high blood pressure in adults: synopsis of the 2017 {American} {College} of {Cardiology}/{American} {Heart} {Association} {Hypertension} {Guideline}},
	volume = {168},
	number = {5},
	journal = {Annals of internal medicine},
	author = {Carey, Robert M and Whelton, Paul K},
	year = {2018},
	note = {Publisher: American College of Physicians},
	pages = {351--358},
}

@article{bramer_international_1988,
	title = {International statistical classification of diseases and related health problems. {Tenth} revision.},
	volume = {41},
	number = {1},
	journal = {World health statistics quarterly. Rapport trimestriel de statistiques sanitaires mondiales},
	author = {Brämer, Gerlind R},
	year = {1988},
	pages = {32--36},
}

@article{breslow_covariance_1974-1,
	title = {Covariance analysis of censored survival data},
	journal = {Biometrics},
	author = {Breslow, Norman},
	year = {1974},
	note = {Publisher: JSTOR},
	pages = {89--99},
}

@article{fine_proportional_1999,
	title = {A proportional hazards model for the subdistribution of a competing risk},
	volume = {94},
	number = {446},
	journal = {Journal of the American statistical association},
	author = {Fine, Jason P and Gray, Robert J},
	year = {1999},
	note = {Publisher: Taylor \& Francis},
	pages = {496--509},
}

@article{segall_heart_2014,
	title = {Heart failure in patients with chronic kidney disease: a systematic integrative review},
	volume = {2014},
	journal = {BioMed research international},
	author = {Segall, Liviu and Nistor, Ionut and Covic, Adrian},
	year = {2014},
	note = {Publisher: Hindawi},
}

@article{hastie_glmnet_2014,
	title = {Glmnet vignette},
	volume = {9},
	number = {2016},
	journal = {Retrieved June},
	author = {Hastie, Trevor and Qian, Junyang},
	year = {2014},
	pages = {1--30},
}

@book{klein_survival_2006,
	title = {Survival analysis: techniques for censored and truncated data},
	publisher = {Springer Science \& Business Media},
	author = {Klein, John P and Moeschberger, Melvin L},
	year = {2006},
}
