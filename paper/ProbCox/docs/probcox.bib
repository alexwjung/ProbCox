
@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difﬁcult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to ﬁrst posit a family of densities and then to ﬁnd the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-ﬁeld variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	language = {en},
	number = {518},
	urldate = {2020-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, theory},
	pages = {859--877},
}

@article{mittal_high-dimensional_2014,
	title = {High-dimensional, massive sample-size {Cox} proportional hazards regression for survival analysis},
	volume = {15},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxt043},
	doi = {10.1093/biostatistics/kxt043},
	abstract = {Survival analysis endures as an old, yet active research field with applications that spread across many domains. Continuing improvements in data acquisition techniques pose constant challenges in applying existing survival analysis methods to these emerging data sets. In this paper, we present tools for fitting regularized Cox survival analysis models on high-dimensional, massive sample-size (HDMSS) data using a variant of the cyclic coordinate descent optimization technique tailored for the sparsity that HDMSS data often present. Experiments on two real data examples demonstrate that efficient analyses of HDMSS data using these tools result in improved predictive performance and calibration.},
	language = {en},
	number = {2},
	urldate = {2020-11-20},
	journal = {Biostatistics},
	author = {Mittal, S. and Madigan, D. and Burd, R. S. and Suchard, M. A.},
	month = apr,
	year = {2014},
	keywords = {theory},
	pages = {207--221},
}

@article{tarkhan_bigsurvsgd_2020,
	title = {{BigSurvSGD}: {Big} {Survival} {Data} {Analysis} via {Stochastic} {Gradient} {Descent}},
	shorttitle = {{BigSurvSGD}},
	url = {http://arxiv.org/abs/2003.00116},
	abstract = {In many biomedical applications, outcome is measured as a “time-to-event” (eg. disease progression or death). To assess the connection between features of a patient and this outcome, it is common to assume a proportional hazards model, and ﬁt a proportional hazards regression (or Cox regression). To ﬁt this model, a log-concave objective function known as the “partial likelihood” is maximized. For moderate-sized datasets, an eﬃcient Newton-Raphson algorithm that leverages the structure of the objective can be employed. However, in large datasets this approach has two issues: 1) The computational tricks that leverage structure can also lead to computational instability; 2) The objective does not naturally decouple: Thus, if the dataset does not ﬁt in memory, the model can be very computationally expensive to ﬁt. This additionally means that the objective is not directly amenable to stochastic gradient-based optimization methods. To overcome these issues, we propose a simple, new framing of proportional hazards regression: This results in an objective function that is amenable to stochastic gradient descent. We show that this simple modiﬁcation allows us to eﬃciently ﬁt survival models with very large datasets. This also facilitates training complex, eg. neural-network-based, models with survival data.},
	language = {en},
	urldate = {2020-11-20},
	journal = {arXiv:2003.00116 [math, stat]},
	author = {Tarkhan, Aliasghar and Simon, Noah},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.00116},
	keywords = {Mathematics - Statistics Theory, theory},
}

@article{ngwa_generating_2019,
	title = {Generating survival times with time-varying covariates using the {Lambert} {W} {Function}},
	volume = {0},
	issn = {0361-0918},
	url = {https://doi.org/10.1080/03610918.2019.1648822},
	doi = {10.1080/03610918.2019.1648822},
	abstract = {Simulation studies provide an important statistical tool in evaluating survival methods, requiring an appropriate data-generating process to simulate data for an underlying statistical model. Many studies with time-to-event outcomes use the Cox proportional hazard model. While methods for simulating such data with time-invariant predictors have been described, methods for simulating data with time-varying covariates are sorely needed. Here, we describe an approach for generating data for the Cox proportional hazard model with time-varying covariates when event times follow an Exponential or Weibull distribution. For each distribution, we derive a closed-form expression to generate survival times and link the time-varying covariates with the hazard function. We consider a continuous time-varying covariate measured at regular intervals over time, as well as time-invariant covariates, in generating time-to-event data under a number of scenarios. Our results suggest this method can lead to simulation studies with reliable and robust estimation of the association parameter in Cox-Weibull and Cox-Exponential models.},
	number = {0},
	urldate = {2021-03-12},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Ngwa, Julius S. and Cabral, Howard J. and Cheng, Debbie M. and Gagnon, David R. and LaValley, Michael P. and Cupples, L. Adrienne},
	month = aug,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610918.2019.1648822},
	keywords = {Lambert W Function, Linear Mixed effects model, Longitudinal and survival data, simulation, Time-varying covariates, Two step approach},
	pages = {1--19},
}

@article{shih_variate_1993,
	title = {Variate generation for a nonhomogeneous poisson process with time dependent covariates},
	volume = {44},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659308811457},
	doi = {10.1080/00949659308811457},
	abstract = {Algorithms are developed for generating a sequence of event times from a nonhomogeneous Poisson process that is influenced by the values of covariates that vary with time. Closed form expressions for random variate generation are shown for several baseline intensity and link functions. Two specific models linking the baseline process to the general model are considered: the accelerated time model and the proportional intensity model. In the accelerated time model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where z is a covariate vector, ⋀0(t) is the baseline cumulative intensity function and ψ(z) is the link function. In the proportional intensity model, the cumulative intensity function of a nonhomogeneous Poisson process under covariate effects is , where λ0(t) is the baseline intensity function.},
	number = {3-4},
	urldate = {2021-03-12},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shih, Li-Hsing and Leemis, Lawrence M.},
	month = jan,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659308811457},
	keywords = {simulation, Covariates, Nonhomogeneous Poisson processes, Simulation, Variate generation},
	pages = {165--186},
}

@article{leemis_variate_1990,
	title = {Variate generation for accelerated life and proportional hazards models with time dependent covariates},
	volume = {10},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/0167715290900529},
	doi = {10.1016/0167-7152(90)90052-9},
	abstract = {Variate generation algorithms for lifetimes when survival models incorporate time dependent covariates are presented. These algorithms are closed form for special cases of the function that links the covariate values to the survivor distribution. These algorithms are illustrated by several examples.},
	language = {en},
	number = {4},
	urldate = {2021-03-12},
	journal = {Statistics \& Probability Letters},
	author = {Leemis, Lawrence M. and Shih, Li-Hsing and Reynertson, Kurt},
	month = sep,
	year = {1990},
	keywords = {simulation, Accelerated life model, Monte Carlo simulation, proportional hazards model, time dependent covariates, variate generation},
	pages = {335--339},
}

@article{zhou_understanding_2001,
	title = {Understanding the {Cox} {Regression} {Models} with {Time}-{Change} {Covariates}},
	volume = {55},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2686004},
	abstract = {The Cox regression model is a cornerstone of modern survival analysis and is widely used in many other fields as well. But the Cox models with time-change covariates are not easy to understand or visualize. We therefore offer a simple and easy-to-understand interpretation of the (arbitrary) baseline hazard and time-change covariate. This interpretation also provides a way to simulate variables that follow a Cox model with arbitrary baseline hazard and time-change covariate.},
	number = {2},
	urldate = {2021-03-12},
	journal = {The American Statistician},
	author = {Zhou, Mai},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {simulation},
	pages = {153--155},
}

@article{bender_generating_2005,
	title = {Generating survival times to simulate {Cox} proportional hazards models},
	volume = {24},
	issn = {0277-6715},
	doi = {10.1002/sim.2059},
	abstract = {Simulation studies present an important statistical tool to investigate the performance, properties and adequacy of statistical models in pre-specified situations. One of the most important statistical models in medical research is the proportional hazards model of Cox. In this paper, techniques to generate survival times for simulation studies regarding Cox proportional hazards models are presented. A general formula describing the relation between the hazard and the corresponding survival time of the Cox model is derived, which is useful in simulation studies. It is shown how the exponential, the Weibull and the Gompertz distribution can be applied to generate appropriate survival times for simulation studies. Additionally, the general relation between hazard and survival time can be used to develop own distributions for special situations and to handle flexibly parameterized proportional hazards models. The use of distributions other than the exponential distribution is indispensable to investigate the characteristics of the Cox proportional hazards model, especially in non-standard situations, where the partial likelihood depends on the baseline hazard. A simulation study investigating the effect of measurement errors in the German Uranium Miners Cohort Study is considered to illustrate the proposed simulation techniques and to emphasize the importance of a careful modelling of the baseline hazard in Cox models.},
	language = {eng},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
	month = jun,
	year = {2005},
	pmid = {15724232},
	keywords = {simulation, Cohort Studies, Computer Simulation, Germany, East, Humans, Mining, Neoplasms, Proportional Hazards Models, Radiation, Radon, Survival Analysis},
	pages = {1713--1723},
}

@article{kvamme_time--event_2019,
	title = {Time-to-{Event} {Prediction} with {Neural} {Networks} and {Cox} {Regression}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-424.html},
	number = {129},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kvamme, Havard and Borgan, Ornulf and Scheel, Ida},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, theory},
	pages = {1--30},
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	number = {4},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	keywords = {theory},
	pages = {1303--1347},
}

@article{millett_sex_2018,
	title = {Sex differences in risk factors for myocardial infarction: cohort study of {UK} {Biobank} participants},
	issn = {0959-8138, 1756-1833},
	shorttitle = {Sex differences in risk factors for myocardial infarction},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.k4247},
	doi = {10.1136/bmj.k4247},
	abstract = {Objectives To investigate sex differences in risk factors for incident myocardial infarction (MI) and whether they vary with age. Design Prospective population based study. Setting UK Biobank. Participants 471 998 participants (56\% women; mean age 56.2) with no history of cardiovascular disease. Main outcome measure Incident (fatal and non-fatal) MI. Results 5081 participants (1463 (28.8\%) of whom were women) had MI over seven years’ mean follow-up, resulting in an incidence per 10 000 person years of 7.76 (95\% confidence interval 7.37 to 8.16) for women and 24.35 (23.57 to 25.16) for men. Higher blood pressure indices, smoking intensity, body mass index, and the presence of diabetes were associated with an increased risk of MI in men and women, but associations were attenuated with age. In women, systolic blood pressure and hypertension, smoking status and intensity, and diabetes were associated with higher hazard ratios for MI compared with men: ratio of hazard ratios 1.09 (95\% confidence interval 1.02 to 1.16) for systolic blood pressure, 1.55 (1.32 to 1.83) for current smoking, 2.91 (1.56 to 5.45) for type 1 diabetes, and 1.47 (1.16 to 1.87) for type 2 diabetes. There was no evidence that any of these ratios of hazard ratios decreased with age (P{\textgreater}0.2). With the exception of type 1 diabetes, the incidence of MI was higher in men than in women for all risk factors.},
	language = {en},
	urldate = {2021-03-10},
	journal = {BMJ},
	author = {Millett, Elizabeth R C and Peters, Sanne A E and Woodward, Mark},
	month = nov,
	year = {2018},
	keywords = {application},
	pages = {k4247},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2985181},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	keywords = {theory},
	pages = {187--220},
}

@article{cox_partial_1975,
	title = {Partial {Likelihood}},
	volume = {62},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335362},
	doi = {10.2307/2335362},
	abstract = {A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and inference in stochastic processes. It is shown that the usual large-sample properties of maximum likelihood estimates and tests apply when partial likelihood is used.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Cox, D. R.},
	year = {1975},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {269--276},
}

@article{hjort_nonparametric_1990,
	title = {Nonparametric {Bayes} {Estimators} {Based} on {Beta} {Processes} in {Models} for {Life} {History} {Data}},
	volume = {18},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242052},
	abstract = {Several authors have constructed nonparametric Bayes estimators for a cumulative distribution function based on (possibly right-censored) data. The prior distributions have, for example, been Dirichlet processes or, more generally, processes neutral to the right. The present article studies the related problem of finding Bayes estimators for cumulative hazard rates and related quantities, w.r.t. prior distributions that correspond to cumulative hazard rate processes with nonnegative independent increments. A particular class of prior processes, termed beta processes, is introduced and is shown to constitute a conjugate class. To arrive at these, a nonparametric time-discrete framework for survival data, which has some independent interest, is studied first. An important bonus of the approach based on cumulative hazards is that more complicated models for life history data than the simple life table situation can be treated, for example, time-inhomogeneous Markov chains. We find posterior distributions and derive Bayes estimators in such models and also present a semiparametric Bayesian analysis of the Cox regression model. The Bayes estimators are easy to interpret and easy to compute. In the limiting case of a vague prior the Bayes solution for a cumulative hazard is the Nelson-Aalen estimator and the Bayes solution for a survival probability is the Kaplan-Meier estimator.},
	number = {3},
	urldate = {2021-03-11},
	journal = {The Annals of Statistics},
	author = {Hjort, Nils Lid},
	year = {1990},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {theory},
	pages = {1259--1294},
}

@article{yusuf_modifiable_2020,
	title = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE}): a prospective cohort study},
	volume = {395},
	issn = {01406736},
	shorttitle = {Modifiable risk factors, cardiovascular disease, and mortality in 155 722 individuals from 21 high-income, middle-income, and low-income countries ({PURE})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673619320082},
	doi = {10.1016/S0140-6736(19)32008-2},
	abstract = {Background Global estimates of the effect of common modifiable risk factors on cardiovascular disease and mortality are largely based on data from separate studies, using different methodologies. The Prospective Urban Rural Epidemiology (PURE) study overcomes these limitations by using similar methods to prospectively measure the effect of modifiable risk factors on cardiovascular disease and mortality across 21 countries (spanning five continents) grouped by different economic levels.},
	language = {en},
	number = {10226},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Yusuf, Salim and Joseph, Philip and Rangarajan, Sumathy and Islam, Shofiqul and Mente, Andrew and Hystad, Perry and Brauer, Michael and Kutty, Vellappillil Raman and Gupta, Rajeev and Wielgosz, Andreas and AlHabib, Khalid F and Dans, Antonio and Lopez-Jaramillo, Patricio and Avezum, Alvaro and Lanas, Fernando and Oguz, Aytekin and Kruger, Iolanthe M and Diaz, Rafael and Yusoff, Khalid and Mony, Prem and Chifamba, Jephat and Yeates, Karen and Kelishadi, Roya and Yusufali, Afzalhussein and Khatib, Rasha and Rahman, Omar and Zatonska, Katarzyna and Iqbal, Romaina and Wei, Li and Bo, Hu and Rosengren, Annika and Kaur, Manmeet and Mohan, Viswanathan and Lear, Scott A and Teo, Koon K and Leong, Darryl and O'Donnell, Martin and McKee, Martin and Dagenais, Gilles},
	month = mar,
	year = {2020},
	keywords = {application},
	pages = {795--808},
}

@article{kalbfleisch_marginal_1973,
	title = {Marginal {Likelihoods} {Based} on {Cox}'s {Regression} and {Life} {Model}},
	volume = {60},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334538},
	doi = {10.2307/2334538},
	abstract = {Marginal likelihoods are obtained for the regression parameters in the model presented by Cox (1972). If no ties occur in the recording of failure time data the results of Cox are given a straightforward justification. If ties occur in the data, results different from those suggested by Cox are obtained. Some Monte-Carlo comparisons of these competing results are made. A discrete model is developed for grouped data from Cox's model and estimates of the survivor function are given for both continuous and grouped data.},
	number = {2},
	urldate = {2021-03-11},
	journal = {Biometrika},
	author = {Kalbfleisch, J. D. and Prentice, R. L.},
	year = {1973},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {theory},
	pages = {267--278},
}

@article{gustafson_large_1997,
	title = {Large {Hierarchical} {Bayesian} {Analysis} of {Multivariate} {Survival} {Data}},
	volume = {53},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533110},
	doi = {10.2307/2533110},
	abstract = {Failure times that are grouped according to shared environments arise commonly in statistical practice. That is, multiple responses may be observed for each of many units. For instance, the units might be patients or centers in a clinical trial setting. Bayesian hierarchical models are appropriate for data analysis in this context. At the first stage of the model, survival times can be modelled via the Cox partial likelihood, using a justification due to Kalbfleisch (1978, Journal of the Royal Statistical Society, Series B 40, 214-221). Thus, questionable parametric assumptions are avoided. Conventional wisdom dictates that it is comparatively safe to make parametric assumptions at subsequent stages. Thus, unit-specific parameters are modelled parametrically. The posterior distribution of parameters given observed data is examined using Markov chain Monte Carlo methods. Specifically, the hybrid Monte Carlo method, as described by Neal (1993a, in Advances in Neural Information Processing 5, 475-482; 1993b, Probabilistic inference using Markov chain Monte Carlo methods), is utilized.},
	number = {1},
	urldate = {2021-03-11},
	journal = {Biometrics},
	author = {Gustafson, Paul},
	year = {1997},
	note = {Publisher: [Wiley, International Biometric Society]},
	keywords = {theory},
	pages = {230--242},
}

@article{austin_generating_2012,
	title = {Generating survival times to simulate {Cox} proportional hazards models with time-varying covariates},
	volume = {31},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3546387/},
	doi = {10.1002/sim.5452},
	abstract = {Simulations and Monte Carlo methods serve an important role in modern statistical research. They allow for an examination of the performance of statistical procedures in settings in which analytic and mathematical derivations may not be feasible. A key element in any statistical simulation is the existence of an appropriate data-generating process: one must be able to simulate data from a specified statistical model. We describe data-generating processes for the Cox proportional hazards model with time-varying covariates when event times follow an exponential, Weibull, or Gompertz distribution. We consider three types of time-varying covariates: first, a dichotomous time-varying covariate that can change at most once from untreated to treated (e.g., organ transplant); second, a continuous time-varying covariate such as cumulative exposure at a constant dose to radiation or to a pharmaceutical agent used for a chronic condition; third, a dichotomous time-varying covariate with a subject being able to move repeatedly between treatment states (e.g., current compliance or use of a medication). In each setting, we derive closed-form expressions that allow one to simulate survival times so that survival times are related to a vector of fixed or time-invariant covariates and to a single time-varying covariate. We illustrate the utility of our closed-form expressions for simulating event times by using Monte Carlo simulations to estimate the statistical power to detect as statistically significant the effect of different types of binary time-varying covariates. This is compared with the statistical power to detect as statistically significant a binary time-invariant covariate. Copyright © 2012 John Wiley \& Sons, Ltd.},
	number = {29},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C},
	month = dec,
	year = {2012},
	pmid = {22763916},
	pmcid = {PMC3546387},
	keywords = {simulation},
	pages = {3946--3958},
}

@article{dagostino_ralph_b_general_2008,
	title = {General {Cardiovascular} {Risk} {Profile} for {Use} in {Primary} {Care}},
	volume = {117},
	url = {https://www.ahajournals.org/doi/10.1161/circulationaha.107.699579},
	doi = {10.1161/CIRCULATIONAHA.107.699579},
	abstract = {Background— Separate multivariable risk algorithms are commonly used to assess risk of specific atherosclerotic cardiovascular disease (CVD) events, ie, coronary heart disease, cerebrovascular disease, peripheral vascular disease, and heart failure. The present report presents a single multivariable risk function that predicts risk of developing all CVD and of its constituents.Methods and Results— We used Cox proportional-hazards regression to evaluate the risk of developing a first CVD event in 8491 Framingham study participants (mean age, 49 years; 4522 women) who attended a routine examination between 30 and 74 years of age and were free of CVD. Sex-specific multivariable risk functions (“general CVD” algorithms) were derived that incorporated age, total and high-density lipoprotein cholesterol, systolic blood pressure, treatment for hypertension, smoking, and diabetes status. We assessed the performance of the general CVD algorithms for predicting individual CVD events (coronary heart disease, stroke, peripheral artery disease, or heart failure). Over 12 years of follow-up, 1174 participants (456 women) developed a first CVD event. All traditional risk factors evaluated predicted CVD risk (multivariable-adjusted P{\textless}0.0001). The general CVD algorithm demonstrated good discrimination (C statistic, 0.763 [men] and 0.793 [women]) and calibration. Simple adjustments to the general CVD risk algorithms allowed estimation of the risks of each CVD component. Two simple risk scores are presented, 1 based on all traditional risk factors and the other based on non–laboratory-based predictors.Conclusions— A sex-specific multivariable risk factor algorithm can be conveniently used to assess general CVD risk and risk of individual CVD events (coronary, cerebrovascular, and peripheral arterial disease and heart failure). The estimated absolute CVD event rates can be used to quantify risk and to guide preventive care.},
	number = {6},
	urldate = {2021-03-10},
	journal = {Circulation},
	author = {{D’Agostino Ralph B.} and {Vasan Ramachandran S.} and {Pencina Michael J.} and {Wolf Philip A.} and {Cobain Mark} and {Massaro Joseph M.} and {Kannel William B.}},
	month = feb,
	year = {2008},
	note = {Publisher: American Heart Association},
	keywords = {application},
	pages = {743--753},
}

@article{li_fast_2020,
	title = {Fast {Lasso} method for large-scale and ultrahigh-dimensional {Cox} model with applications to {UK} {Biobank}},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxaa038},
	doi = {10.1093/biostatistics/kxaa038},
	abstract = {We develop a scalable and highly efficient algorithm to fit a Cox proportional hazard model by maximizing the \$L{\textasciicircum}1\$-regularized (Lasso) partial likelihood function, based on the Batch Screening Iterative Lasso (BASIL) method developed in Qian and others (2019). Our algorithm is particularly suitable for large-scale and high-dimensional data that do not fit in the memory. The output of our algorithm is the full Lasso path, the parameter estimates at all predefined regularization parameters, as well as their validation accuracy measured using the concordance index (C-index) or the validation deviance. To demonstrate the effectiveness of our algorithm, we analyze a large genotype-survival time dataset across 306 disease outcomes from the UK Biobank (Sudlow and others, 2015). We provide a publicly available implementation of the proposed approach for genetics data on top of the PLINK2 package and name it snpnet-Cox.},
	number = {kxaa038},
	urldate = {2021-03-11},
	journal = {Biostatistics},
	author = {Li, Ruilin and Chang, Christopher and Justesen, Johanne M and Tanigawa, Yosuke and Qiang, Junyang and Hastie, Trevor and Rivas, Manuel A and Tibshirani, Robert},
	month = sep,
	year = {2020},
	keywords = {theory},
}

@article{mortensen_elevated_2020,
	title = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years: a contemporary primary prevention cohort},
	volume = {396},
	issn = {01406736},
	shorttitle = {Elevated {LDL} cholesterol and increased risk of myocardial infarction and atherosclerotic cardiovascular disease in individuals aged 70–100 years},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673620322339},
	doi = {10.1016/S0140-6736(20)32233-9},
	abstract = {Background Findings of historical studies suggest that elevated LDL cholesterol is not associated with increased risk of myocardial infarction and atherosclerotic cardiovascular disease in patients older than 70 years. We aimed to test this hypothesis in a contemporary population of individuals aged 70–100 years.},
	language = {en},
	number = {10263},
	urldate = {2021-03-10},
	journal = {The Lancet},
	author = {Mortensen, Martin Bødtker and Nordestgaard, Børge Grønne},
	month = nov,
	year = {2020},
	keywords = {application},
	pages = {1644--1652},
}

@article{hendry_data_2014,
	title = {Data generation for the {Cox} proportional hazards model with time-dependent covariates: a method for medical researchers},
	volume = {33},
	issn = {02776715},
	shorttitle = {Data generation for the {Cox} proportional hazards model with time-dependent covariates},
	url = {http://doi.wiley.com/10.1002/sim.5945},
	doi = {10.1002/sim.5945},
	abstract = {The proliferation of longitudinal studies has increased the importance of statistical methods for time-to-event data that can incorporate time-dependent covariates. The Cox proportional hazards model is one such method that is widely used. As more extensions of the Cox model with time-dependent covariates are developed, simulations studies will grow in importance as well. An essential starting point for simulation studies of time-to-event models is the ability to produce simulated survival times from a known data generating process. This paper develops a method for the generation of survival times that follow a Cox proportional hazards model with time-dependent covariates. The method presented relies on a simple transformation of random variables generated according to a truncated piecewise exponential distribution, and allows practitioners great ﬂexibility and control over both the number of time-dependent covariates and the number of time periods in the duration of follow-up measurement. Within this framework, an additional argument is suggested that allows researchers to generate time-to-event data in which covariates change at integer-valued steps of the time scale. The purpose of this approach is to produce data for simulation experiments that mimic the types of data structures applied researchers encounter when using longitudinal biomedical data. Validity is assessed in a set of simulation experiments and results indicate that the proposed procedure performs well in producing data that conform to the assumptions of the Cox proportional hazards model.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics in Medicine},
	author = {Hendry, David J.},
	month = feb,
	year = {2014},
	keywords = {simulation},
	pages = {436--454},
}

@article{andersen_coxs_1982,
	title = {Cox's {Regression} {Model} for {Counting} {Processes}: {A} {Large} {Sample} {Study}},
	volume = {10},
	issn = {0090-5364},
	shorttitle = {Cox's {Regression} {Model} for {Counting} {Processes}},
	url = {http://projecteuclid.org/euclid.aos/1176345976},
	doi = {10.1214/aos/1176345976},
	language = {en},
	number = {4},
	urldate = {2021-03-10},
	journal = {The Annals of Statistics},
	author = {Andersen, P. K. and Gill, R. D.},
	month = dec,
	year = {1982},
	keywords = {theory},
	pages = {1100--1120},
}

@article{sylvestre_comparison_2008,
	title = {Comparison of algorithms to generate event times conditional on time-dependent covariates},
	volume = {27},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3092},
	doi = {https://doi.org/10.1002/sim.3092},
	abstract = {The Cox proportional hazards model with time-dependent covariates (TDC) is now a part of the standard statistical analysis toolbox in medical research. As new methods involving more complex modeling of time-dependent variables are developed, simulations could often be used to systematically assess the performance of these models. Yet, generating event times conditional on TDC requires well-designed and efficient algorithms. We compare two classes of such algorithms: permutational algorithms (PAs) and algorithms based on a binomial model. We also propose a modification of the PA to incorporate a rejection sampler. We performed a simulation study to assess the accuracy, stability, and speed of these algorithms in several scenarios. Both classes of algorithms generated data sets that, once analyzed, provided virtually unbiased estimates with comparable variances. In terms of computational efficiency, the PA with the rejection sampler reduced the time necessary to generate data by more than 50 per cent relative to alternative methods. The PAs also allowed more flexibility in the specification of the marginal distributions of event times and required less calibration. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {14},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Sylvestre, Marie-Pierre and Abrahamowicz, Michal},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3092},
	keywords = {simulation, proportional hazards model, algorithms, rejection sampling, simulations, survival analysis, time-dependent covariates},
	pages = {2618--2634},
}

@article{alaa_cardiovascular_2019,
	title = {Cardiovascular disease risk prediction using automated machine learning: {A} prospective study of 423,604 {UK} {Biobank} participants},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Cardiovascular disease risk prediction using automated machine learning},
	url = {https://dx.plos.org/10.1371/journal.pone.0213653},
	doi = {10.1371/journal.pone.0213653},
	language = {en},
	number = {5},
	urldate = {2021-03-10},
	journal = {PLOS ONE},
	author = {Alaa, Ahmed M. and Bolton, Thomas and Di Angelantonio, Emanuele and Rudd, James H. F. and van der Schaar, Mihaela},
	editor = {Aalto-Setala, Katriina},
	month = may,
	year = {2019},
	keywords = {application},
	pages = {e0213653},
}

@article{kucukelbir_automatic_2017,
	title = {Automatic {Differentiation} {Variational} {Inference}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/16-107.html},
	number = {14},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	year = {2017},
	keywords = {theory},
	pages = {1--45},
}

@article{andersen_analysis_2021,
	title = {Analysis of time-to-event for observational studies: {Guidance} to the use of intensity models},
	volume = {40},
	issn = {1097-0258},
	shorttitle = {Analysis of time-to-event for observational studies},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8757},
	doi = {https://doi.org/10.1002/sim.8757},
	abstract = {This paper provides guidance for researchers with some mathematical background on the conduct of time-to-event analysis in observational studies based on intensity (hazard) models. Discussions of basic concepts like time axis, event definition and censoring are given. Hazard models are introduced, with special emphasis on the Cox proportional hazards regression model. We provide check lists that may be useful both when fitting the model and assessing its goodness of fit and when interpreting the results. Special attention is paid to how to avoid problems with immortal time bias by introducing time-dependent covariates. We discuss prediction based on hazard models and difficulties when attempting to draw proper causal conclusions from such models. Finally, we present a series of examples where the methods and check lists are exemplified. Computational details and implementation using the freely available R software are documented in Supplementary Material. The paper was prepared as part of the STRATOS initiative.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Statistics in Medicine},
	author = {Andersen, Per Kragh and Perme, Maja Pohar and Houwelingen, Hans C. van and Cook, Richard J. and Joly, Pierre and Martinussen, Torben and Taylor, Jeremy M. G. and Abrahamowicz, Michal and Therneau, Terry M.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8757},
	keywords = {theory, survival analysis, time-dependent covariates, censoring, Cox regression model, hazard function, immortal time bias, multistate model, prediction, STRATOS initiative},
	pages = {185--211},
}

@article{mackenzie_marginal_2002,
	title = {Marginal and hazard ratio specific random data generation: {Applications} to semi-parametric bootstrapping},
	volume = {12},
	issn = {1573-1375},
	shorttitle = {Marginal and hazard ratio specific random data generation},
	url = {https://doi.org/10.1023/A:1020750810409},
	doi = {10.1023/A:1020750810409},
	abstract = {Cox's partial likelihood for censored time-to-event data can be interpreted as a permutation probability, whereby covariate values are permuted to the observed times-to-event and censoring times. This interpretation facilitates a simple method for jointly generating times-to-event and covariate tuples with considerable flexibility, including time dependence of the hazard ratio and specification of both the marginal time-to-event and covariate distributions. This interpretation also facilitates a method for semi-parametric bootstrapping of hazard ratio estimators.},
	language = {en},
	number = {3},
	urldate = {2021-03-11},
	journal = {Statistics and Computing},
	author = {Mackenzie, Todd and Abrahamowicz, Michal},
	month = jul,
	year = {2002},
	keywords = {simulation},
	pages = {245--252},
}

@article{abrahamowicz_time-dependent_1996,
	title = {Time-{Dependent} {Hazard} {Ratio}: {Modeling} and {Hypothesis} {Testing} with {Application} in {Lupus} {Nephritis}},
	volume = {91},
	issn = {0162-1459},
	shorttitle = {Time-{Dependent} {Hazard} {Ratio}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476711},
	doi = {10.1080/01621459.1996.10476711},
	abstract = {We investigate the association between duration of untreated disease and survival in lupus nephritis, a rare rheumatologic disease. In this case, as in many other studies of survival, a priori considerations suggest that the effect of the predictor on hazard may change with increasing follow-up time. To accommodate such situations, we use regression splines to model the hazard ratio as a flexible function of time. We propose model-based tests of the hypotheses of hazards proportionality and of no association. We evaluate the accuracy of estimation and inference in simulations and also present analysis of a larger medical data set.},
	number = {436},
	urldate = {2021-03-11},
	journal = {Journal of the American Statistical Association},
	author = {Abrahamowicz, Michal and Mackenzie, Todd and Esdaile, John M.},
	month = dec,
	year = {1996},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476711},
	keywords = {simulation, Akaike information criterion, Partial likelihood, Proportional hazards, Regression splines, Survival analysis},
	pages = {1432--1439},
}

@article{harden_simulating_2019,
	title = {Simulating {Duration} {Data} for the {Cox} {Model}},
	volume = {7},
	issn = {2049-8470, 2049-8489},
	url = {https://www.cambridge.org/core/journals/political-science-research-and-methods/article/simulating-duration-data-for-the-cox-model/1945D7548766E76FB31C6C833976822E},
	doi = {10.1017/psrm.2018.19},
	abstract = {The Cox proportional hazards model is a popular method for duration analysis that is frequently the subject of simulation studies. However, no standard method exists for simulating durations directly from its data generating process because it does not assume a distributional form for the baseline hazard function. Instead, simulation studies typically rely on parametric survival distributions, which contradicts the primary motivation for employing the Cox model. We propose a method that generates a baseline hazard function at random by fitting a cubic spline to randomly drawn points. Durations drawn from this function match the Cox model’s inherent flexibility and improve the simulation’s generalizability. The method can be extended to include time-varying covariates and non-proportional hazards.},
	language = {en},
	number = {4},
	urldate = {2021-03-16},
	journal = {Political Science Research and Methods},
	author = {Harden, Jeffrey J. and Kropko, Jonathan},
	month = oct,
	year = {2019},
	note = {Publisher: Cambridge University Press},
	pages = {921--928},
}

@article{quiroz_speeding_2019,
	title = {Speeding {Up} {MCMC} by {Efficient} {Data} {Subsampling}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2018.1448827},
	doi = {10.1080/01621459.2018.1448827},
	abstract = {We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online.},
	number = {526},
	urldate = {2021-03-30},
	journal = {Journal of the American Statistical Association},
	author = {Quiroz, Matias and Kohn, Robert and Villani, Mattias and Tran, Minh-Ngoc},
	month = apr,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2018.1448827},
	keywords = {Bayesian inference, Big Data, Block pseudo-marginal, Correlated pseudo-marginal, Estimated likelihood, Survey sampling},
	pages = {831--843},
}

@article{teh_consistency_2016,
	title = {Consistency and fluctuations for stochastic gradient {Langevin} dynamics},
	volume = {17},
	journal = {Journal of Machine Learning Research},
	author = {Teh, Yee Whye and Thiery, Alexandre H and Vollmer, Sebastian J},
	year = {2016},
	note = {Publisher: Journal of Machine Learning Research},
}

@article{royston_flexible_2002,
	title = {Flexible parametric proportional-hazards and proportional-odds models for censored survival data, with application to prognostic modelling and estimation of treatment effects},
	volume = {21},
	issn = {0277-6715},
	doi = {10.1002/sim.1203},
	abstract = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, non-proportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer.},
	language = {eng},
	number = {15},
	journal = {Statistics in Medicine},
	author = {Royston, Patrick and Parmar, Mahesh K. B.},
	month = aug,
	year = {2002},
	pmid = {12210632},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Antineoplastic Agents, Breast Neoplasms, Carcinoma, Transitional Cell, Female, Models, Biological, Prognosis, Treatment Outcome, Urinary Bladder Neoplasms},
	pages = {2175--2197},
}

@article{nietobarajas_markov_2002,
	title = {Markov {Beta} and {Gamma} {Processes} for {Modelling} {Hazard} {Rates}},
	volume = {29},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00298},
	doi = {https://doi.org/10.1111/1467-9469.00298},
	abstract = {This paper generalizes the discrete time independent increment beta process of Hjort (1990), for modelling discrete failure times, and also generalizes the independent gamma process for modelling piecewise constant hazard rates (Walker and Mallick, 1997). The generalizations are from independent increment to Markov increment prior processes allowing the modelling of smoothness. We derive posterior distributions and undertake a full Bayesian analysis.},
	language = {en},
	number = {3},
	urldate = {2021-03-29},
	journal = {Scandinavian Journal of Statistics},
	author = {Nieto‐Barajas, Luis E. and Walker, Stephen G.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9469.00298},
	keywords = {survival analysis, Bayes non-parametrics, beta process, gamma process, Markov process, stationary process},
	pages = {413--424},
}

@incollection{laud_bayesian_1998,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Nonparametric} and {Covariate} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-4612-1732-9},
	url = {https://doi.org/10.1007/978-1-4612-1732-9_11},
	abstract = {A Bayesian analysis of the semi-parametric regression model of Cox (1972) is given. The cumulative hazard function is modelled as a beta process. The posterior distribution of the regression parameters and the survival function are obtained using a combination of recent Monte Carlo methods. An illustrative analysis within the context of survival time data is given.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {Practical {Nonparametric} and {Semiparametric} {Bayesian} {Statistics}},
	publisher = {Springer},
	author = {Laud, Purushottam W. and Damien, Paul and Smith, Adrian F. M.},
	editor = {Dey, Dipak and Müller, Peter and Sinha, Debajyoti},
	year = {1998},
	doi = {10.1007/978-1-4612-1732-9_11},
	keywords = {Cumulative Hazard, Frailty Model, Hazard Rate, Markov Chain Monte Carlo Method, Posterior Distribution},
	pages = {213--225},
}

@article{sinha_semiparametric_1997,
	title = {Semiparametric {Bayesian} {Analysis} of {Survival} {Data}},
	volume = {92},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2965586},
	doi = {10.2307/2965586},
	abstract = {This review article investigates the potential of Bayes methods for the analysis of survival data using semiparametric models based on either the hazard or the intensity function. The nonparametric part of every model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, including sampling-based algorithms, are used to find posterior estimates of several quantities of interest even when dealing with complex models and unusual data structures. The methodologies developed herein are motivated and aimed at analyzing some common types of survival data from different medical studies; here we focus on univariate survival data in the presence of fixed and time-dependent covariates, multiple event-time data for repeated nonfatal events, and multivariate survival data (subjects are related; e.g., families or litters), each patient with interval-censored infection time and interval-censored disease occurrence time in tandem [e.g., patients with acquired immunodeficiency syndrome (AIDS) and other infectious diseases with long incubation times]. Bayesian exploratory data analysis (EDA) methods and diagnostics for model selection and model assessment are considered for each case. Special attention is given to tests of the parametric modeling assumptions and to censoring.},
	number = {439},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti and Dey, Dipak K.},
	year = {1997},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1195--1212},
}

@article{qiou_multivariate_1999,
	title = {Multivariate survival analysis with positive stable frailties},
	volume = {55},
	issn = {0006-341X},
	doi = {10.1111/j.0006-341x.1999.00637.x},
	abstract = {In this paper, we describe Bayesian modeling of dependent multivariate survival data using positive stable frailty distributions. A flexible baseline hazard formulation using a piecewise exponential model with a correlated prior process is used. The estimation of the stable law parameter together with the parameters of the (conditional) proportional hazards model is facilitated by a modified Gibbs sampling procedure. The methodology is illustrated on kidney infection data (McGilchrist and Aisbett, 1991).},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Qiou, Z. and Ravishanker, N. and Dey, D. K.},
	month = jun,
	year = {1999},
	pmid = {11318227},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Female, Algorithms, Bayes Theorem, Biometry, Kidney Diseases, Male, Multivariate Analysis},
	pages = {637--644},
}

@article{sinha_semiparametric_1993,
	title = {Semiparametric {Bayesian} {Analysis} of {Multiple} {Event} {Time} {Data}},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290789},
	doi = {10.2307/2290789},
	abstract = {Multiple event time data (e.g., carcinogenic growths in different times and locations, multiple attacks of cardiac arrest) arise in various medical studies. A Bayesian analysis of such data based on proportional intensity model of multiple event time data is presented in this paper. The Bayesian structure is somewhat analogous to that used by Kalbfleisch in a proportional hazard model. An unobserved random frailty component is used in the proportional intensity model to take care of heterogeneity among the intensity processes in different subjects. The Monte Carlo method of sampling from multivariate distributions, the so-called Gibbs sampler, is used to sample from the joint posterior distribution of the unknown parameters. The methodology developed here is exemplified with the well-known data set on rat tumors of Gail, Santner, and Brown.},
	number = {423},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {979--983},
}

@article{muller_bayesian_2013,
	title = {Bayesian {Nonparametric} {Inference} – {Why} and {How}},
	volume = {8},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-8/issue-2/Bayesian-Nonparametric-Inference--Why-and-How/10.1214/13-BA811.full},
	doi = {10.1214/13-BA811},
	abstract = {We review inference under models with nonparametric Bayesian (BNP) priors. The discussion follows a set of examples for some common inference problems. The examples are chosen to highlight problems that are challenging for standard parametric inference. We discuss inference for density estimation, clustering, regression and for mixed effects models with random effects distributions. While we focus on arguing for the need for the flexibility of BNP models, we also review some of the more commonly used BNP models, thus hopefully answering a bit of both questions, why and how to use BNP. This review was sponsored by the Bayesian Nonparametrics Section of ISBA (ISBA/BNP). The authors thank the section officers for the support and encouragement.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Müller, Peter and Mitra, Riten},
	month = jun,
	year = {2013},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Dirichlet process, dependent Dirichlet process, nonparametric models, Polya tree},
	pages = {269--302},
}

@article{muller_nonparametric_2004,
	title = {Nonparametric {Bayesian} {Data} {Analysis}},
	volume = {19},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Nonparametric-Bayesian-Data-Analysis/10.1214/088342304000000017.full},
	doi = {10.1214/088342304000000017},
	abstract = {We review the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem we review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models.},
	number = {1},
	urldate = {2021-03-29},
	journal = {Statistical Science},
	author = {Müller, Peter and Quintana, Fernando A.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Survival analysis, Density estimation, Dirichlet process, Pólya tree, random probability model (RPM), regression},
	pages = {95--110},
}

@article{cai_mixed_2002,
	title = {Mixed {Model}-{Based} {Hazard} {Estimation}},
	volume = {11},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/1391161},
	abstract = {This article proposes a new method for estimation of the hazard function from a set of censored failure time data, with a view to extending the general approach to more complicated models. The approach is based on a mixed model representation of penalized spline hazard estimators. One payoff is the automation of the smoothing parameter choice through restricted maximum likelihood. Another is the option to use standard mixed model software for automatic hazard estimation.},
	number = {4},
	urldate = {2021-03-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cai, T. and Hyndman, R. J. and Wand, M. P.},
	year = {2002},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {784--798},
}

@article{hennerfeind_geoadditive_2006,
	title = {Geoadditive {Survival} {Models}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000348},
	doi = {10.1198/016214506000000348},
	abstract = {Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoadditive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting.},
	number = {475},
	urldate = {2021-03-29},
	journal = {Journal of the American Statistical Association},
	author = {Hennerfeind, Andrea and Brezger, Andreas and Fahrmeir, Ludwig},
	month = sep,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000000348},
	keywords = {Bayesian hazard rate model, Markov chain Monte Carlo, Markov random field, Penalized spline, Semiparametric modeling, Spatial survival data},
	pages = {1065--1075},
}

@article{alsefri_bayesian_2020,
	title = {Bayesian joint modelling of longitudinal and time to event data: a methodological review},
	volume = {20},
	issn = {1471-2288},
	shorttitle = {Bayesian joint modelling of longitudinal and time to event data},
	url = {https://doi.org/10.1186/s12874-020-00976-2},
	doi = {10.1186/s12874-020-00976-2},
	abstract = {In clinical research, there is an increasing interest in joint modelling of longitudinal and time-to-event data, since it reduces bias in parameter estimation and increases the efficiency of statistical inference. Inference and prediction from frequentist approaches of joint models have been extensively reviewed, and due to the recent popularity of data-driven Bayesian approaches, a review on current Bayesian estimation of joint model is useful to draw recommendations for future researches.},
	number = {1},
	urldate = {2021-03-29},
	journal = {BMC Medical Research Methodology},
	author = {Alsefri, Maha and Sudell, Maria and García-Fiñana, Marta and Kolamunnage-Dona, Ruwanthi},
	month = apr,
	year = {2020},
	keywords = {Bayesian estimation, Dynamic prediction, Joint models, Longitudinal outcomes, Time-to-event},
	pages = {94},
}

@article{sharef_bayesian_2010,
	title = {Bayesian adaptive {B}-spline estimation in proportional hazards frailty models},
	volume = {4},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-4/issue-none/Bayesian-adaptive-B-spline-estimation-in-proportional-hazards-frailty-models/10.1214/10-EJS566.full},
	doi = {10.1214/10-EJS566},
	abstract = {Frailty models derived from the proportional hazards regression model are frequently used to analyze clustered right-censored survival data. We propose a semiparametric Bayesian methodology for this purpose, modeling both the unknown baseline hazard and density of the random effects using mixtures of B-splines. The posterior distributions for all regression coefficients and spline parameters are obtained using Markov Chain Monte Carlo (MCMC). The methodology permits the use of weighted mixtures of parametric and nonparametric components in modeling the hazard function and frailty distribution; in addition, the spline knots may also be selected adaptively using reversible-jump MCMC. Simulations indicate that the method produces smooth and accurate posterior hazard and frailty density estimates. The Bayesian approach not only produces point estimators that outperform existing approaches in certain circumstances, but also offers a wealth of information about the parameters of interest in the form of MCMC samples from the joint posterior probability distribution. We illustrate the adaptability of the method with data from a study of congestive heart failure.},
	number = {none},
	urldate = {2021-03-29},
	journal = {Electronic Journal of Statistics},
	author = {Sharef, Emmanuel and Strawderman, Robert L. and Ruppert, David and Cowen, Mark and Halasyamani, Lakshmi},
	month = jan,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Survival analysis, frailty distribution, Hazard regression, heart failure, knot selection, random effect density, re-hospitalization, reversible-jump MCMC},
	pages = {606--642},
}

@article{murray_flexible_2016,
	title = {Flexible {Bayesian} {Survival} {Modeling} with {Semiparametric} {Time}-{Dependent} and {Shape}-{Restricted} {Covariate} {Effects}},
	volume = {11},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-2/Flexible-Bayesian-Survival-Modeling-with-Semiparametric-Time-Dependent-and-Shape/10.1214/15-BA954.full},
	doi = {10.1214/15-BA954},
	abstract = {Presently, there are few options with available software to perform a fully Bayesian analysis of time-to-event data wherein the hazard is estimated semi- or non-parametrically. One option is the piecewise exponential model, which requires an often unrealistic assumption that the hazard is piecewise constant over time. The primary aim of this paper is to construct a tractable semiparametric alternative to the piecewise exponential model that assumes the hazard is continuous, and to provide modifiable, user-friendly software that allows the use of these methods in a variety of settings. To accomplish this aim, we use a novel model formulation for the log-hazard based on a low-rank thin plate linear spline that readily facilitates adjustment for covariates with time-dependent and proportional hazards effects, possibly subject to shape restrictions. We investigate the performance of our model choices via simulation. We then analyze colorectal cancer data from a clinical trial comparing the effectiveness of two novel treatment regimes relative to the standard of care for overall survival. We estimate a time-dependent hazard ratio for each novel regime relative to the standard of care while adjusting for the effect of aspartate transaminase, a biomarker of liver function, that is subject to a non-decreasing shape restriction.},
	number = {2},
	urldate = {2021-03-29},
	journal = {Bayesian Analysis},
	author = {Murray, Thomas A. and Hobbs, Brian P. and Sargent, Daniel J. and Carlin, Bradley P.},
	month = jun,
	year = {2016},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Survival analysis, 62F15, 62F30, 62N86, Bayesian methods, colorectal cancer, penalized splines, semiparametric methods, shape-restricted effects, time-dependent effects},
	pages = {381--402},
}

@article{williamson_factors_2020,
	title = {Factors associated with {COVID}-19-related death using {OpenSAFELY}},
	volume = {584},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2521-4},
	doi = {10.1038/s41586-020-2521-4},
	abstract = {Coronavirus disease 2019 (COVID-19) has rapidly affected mortality worldwide1. There is unprecedented urgency to understand who is most at risk of severe outcomes, and this requires new approaches for the timely analysis of large datasets. Working on behalf of NHS England, we created OpenSAFELY—a secure health analytics platform that covers 40\% of all patients in England and holds patient data within the existing data centre of a major vendor of primary care electronic health records. Here we used OpenSAFELY to examine factors associated with COVID-19-related death. Primary care records of 17,278,392 adults were pseudonymously linked to 10,926 COVID-19-related deaths. COVID-19-related death was associated with: being male (hazard ratio (HR) 1.59 (95\% confidence interval 1.53–1.65)); greater age and deprivation (both with a strong gradient); diabetes; severe asthma; and various other medical conditions. Compared with people of white ethnicity, Black and South Asian people were at higher risk, even after adjustment for other factors (HR 1.48 (1.29–1.69) and 1.45 (1.32–1.58), respectively). We have quantified a range of clinical factors associated with COVID-19-related death in one of the largest cohort studies on this topic so far. More patient records are rapidly being added to OpenSAFELY, we will update and extend our results regularly.},
	language = {en},
	number = {7821},
	urldate = {2021-03-28},
	journal = {Nature},
	author = {Williamson, Elizabeth J. and Walker, Alex J. and Bhaskaran, Krishnan and Bacon, Seb and Bates, Chris and Morton, Caroline E. and Curtis, Helen J. and Mehrkar, Amir and Evans, David and Inglesby, Peter and Cockburn, Jonathan and McDonald, Helen I. and MacKenna, Brian and Tomlinson, Laurie and Douglas, Ian J. and Rentsch, Christopher T. and Mathur, Rohini and Wong, Angel Y. S. and Grieve, Richard and Harrison, David and Forbes, Harriet and Schultze, Anna and Croker, Richard and Parry, John and Hester, Frank and Harper, Sam and Perera, Rafael and Evans, Stephen J. W. and Smeeth, Liam and Goldacre, Ben},
	month = aug,
	year = {2020},
	note = {Number: 7821
Publisher: Nature Publishing Group},
	pages = {430--436},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214508000000337},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and {Its} {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/3085904},
	abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
	number = {456},
	urldate = {2021-03-28},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1348--1360},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Bootstrap {Methods}},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full},
	doi = {10.1214/aos/1176344552},
	abstract = {We discuss the following problem: given a random sample \${\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ from an unknown probability distribution \$F\$, estimate the sampling distribution of some prespecified random variable \$R({\textbackslash}mathbf\{X\}, F)\$, on the basis of the observed data \${\textbackslash}mathbf\{x\}\$. (Standard jackknife theory gives an approximate mean and variance in the case \$R({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta\$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	number = {1},
	urldate = {2021-03-28},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G05, 62G15, 62H30, 62J05, bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
	pages = {1--26},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288},
}

@article{tibshirani_lasso_1997,
	title = {The lasso method for variable selection in the {Cox} model},
	volume = {16},
	issn = {0277-6715},
	doi = {10.1002/(sici)1097-0258(19970228)16:4<385::aid-sim380>3.0.co;2-3},
	abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
	language = {eng},
	number = {4},
	journal = {Statistics in Medicine},
	author = {Tibshirani, R.},
	month = feb,
	year = {1997},
	pmid = {9044528},
	keywords = {Humans, Proportional Hazards Models, Survival Analysis, Karnofsky Performance Status, Likelihood Functions, Liver Cirrhosis, Lung Neoplasms, Randomized Controlled Trials as Topic},
	pages = {385--395},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2021-03-28},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
}

@article{simon_regularization_2011,
	title = {Regularization {Paths} for {Cox}'s {Proportional} {Hazards} {Model} via {Coordinate} {Descent}},
	volume = {39},
	issn = {1548-7660},
	doi = {10.18637/jss.v039.i05},
	abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
	language = {eng},
	number = {5},
	journal = {Journal of Statistical Software},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = mar,
	year = {2011},
	pmid = {27065756},
	pmcid = {PMC4824408},
	keywords = {Cox model, elastic net, lasso, survival},
	pages = {1--13},
}

@article{witten_survival_2010,
	title = {Survival analysis with high-dimensional covariates},
	volume = {19},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806549/},
	doi = {10.1177/0962280209105024},
	abstract = {In recent years, breakthroughs in biomedical technology have led to a wealth of data in which the number of features (for instance, genes on which expression measurements are available) exceeds the number of observations (e.g. patients). Sometimes survival outcomes are also available for those same observations. In this case, one might be interested in (a) identifying features that are associated with survival (in a univariate sense), and (b) developing a multivariate model for the relationship between the features and survival that can be used to predict survival in a new observation. Due to the high dimensionality of this data, most classical statistical methods for survival analysis cannot be applied directly. Here, we review a number of methods from the literature that address these two problems.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Statistical methods in medical research},
	author = {Witten, Daniela M and Tibshirani, Robert},
	month = feb,
	year = {2010},
	pmid = {19654171},
	pmcid = {PMC4806549},
	pages = {29--51},
}

@book{andersen_statistical_1993,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Statistical {Models} {Based} on {Counting} {Processes}},
	isbn = {978-0-387-94519-4},
	url = {https://www.springer.com/gp/book/9780387945194},
	abstract = {Modern survival analysis and more general event history analysis may be effectively handled in the mathematical framework of counting processes, stochastic integration, martingale central limit theory and product integration. This book presents this theory, which has been the subject of an intense research activity during the past one-and-a- half decades. The exposition of the theory is integrated with careful presentation of many practical examples, almost exclusively from the authors' own experience, with detailed numerical and graphical illustrations. Statistical Models Based on Counting Processes may be viewed as a research monograph for mathematical statisticians and biostatisticians, although almost all methods are given in concrete detail to be used in practice by other mathematically oriented researchers studying event histories (demographers, econometricians, epidemiologists, actuarial mathematicians, reliabilty engineers and biologists). Much of the material has so far only been available in the journal literature (if at all), and so a wide variety of researchers will find this an invaluable survey of the subject."This book is a masterful account of the counting process approach...is certain to be the standard reference for the area, and should be on the bookshelf of anyone interested in event-history analysis." International Statistical Institute Short Book Reviews "...this impressive reference, which contains a a wealth of powerful mathematics, practical examples, and analytic insights, as well as a complete integration of historical developments and recent advances in event history analysis." Journal of the American Statistical Association},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Andersen, Per Kragh and Borgan, Ornulf and Gill, Richard D. and Keiding, Niels},
	year = {1993},
	doi = {10.1007/978-1-4612-4348-9},
}

@book{kalbfleisch_statistical_2011,
	title = {The {Statistical} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-118-03123-0},
	abstract = {* Contains additional discussion and examples on left truncation as well as material on more general censoring and truncation patterns. * Introduces the martingale and counting process formulation swil lbe in a new chapter. * Develops multivariate failure time data in a separate chapter and extends the material on Markov and semi Markov formulations. * Presents new examples and applications of data analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Kalbfleisch, John D. and Prentice, Ross L.},
	month = jan,
	year = {2011},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{nan_asymptotic_2009,
	title = {Asymptotic {Theory} for the {Semiparametric} {Accelerated} {Failure} {Time} {Model} with {Missing} {Data}},
	volume = {37},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/30243708},
	abstract = {We consider a class of doubly weighted rank-based estimating methods for the transformation (or accelerated failure time) model with missing data as arise, for example, in case-cohort studies. The weights considered may not be predictable as required in a martingale stochastic process formulation. We treat the general problem as a semiparametric estimating equation problem and provide proofs of asymptotic properties for the weighted estimators, with either true weights or estimated weights, by using empirical process theory where martingale theory may fail. Simulations show that the outcome-dependent weighted method works well for finite samples in case-cohort studies and improves efficiency compared to methods based on predictable weights. Further, it is seen that the method is even more efficient when estimated weights are used, as is commonly the case in the missing data literature. The Gehan censored data Wilcoxon weights are found to be surprisingly efficient in a wide class of problems.},
	number = {5A},
	urldate = {2021-03-26},
	journal = {The Annals of Statistics},
	author = {Nan, Bin and Kalbfleisch, John D. and Yu, Menggang},
	year = {2009},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2351--2376},
}

@article{lee_threshold_2006,
	title = {Threshold {Regression} for {Survival} {Analysis}: {Modeling} {Event} {Times} by a {Stochastic} {Process} {Reaching} a {Boundary}},
	volume = {21},
	issn = {0883-4237},
	shorttitle = {Threshold {Regression} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/0708.0346},
	doi = {10.1214/088342306000000330},
	abstract = {Many researchers have investigated ﬁrst hitting times as models for survival data. First hitting times arise naturally in many types of stochastic processes, ranging from Wiener processes to Markov chains. In a survival context, the state of the underlying process represents the strength of an item or the health of an individual. The item fails or the individual experiences a clinical endpoint when the process reaches an adverse threshold state for the ﬁrst time. The time scale can be calendar time or some other operational measure of degradation or disease progression. In many applications, the process is latent (i.e., unobservable). Threshold regression refers to ﬁrst-hitting-time models with regression structures that accommodate covariate data. The parameters of the process, threshold state and time scale may depend on the covariates. This paper reviews aspects of this topic and discusses fruitful avenues for future research.},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Statistical Science},
	author = {Lee, Mei-Ling Ting and Whitmore, G. A.},
	month = nov,
	year = {2006},
	note = {arXiv: 0708.0346},
	keywords = {Statistics - Methodology},
}

@book{ibrahim_bayesian_2001,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Bayesian {Survival} {Analysis}},
	isbn = {978-0-387-95277-2},
	url = {https://www.springer.com/gp/book/9780387952772},
	abstract = {Survival analysis arises in many fields of study including medicine, biology, engineering, public health, epidemiology, and economics. This book provides a comprehensive treatment of Bayesian survival analysis.Several topics are addressed, including parametric models, semiparametric models based on prior processes, proportional and non-proportional hazards models, frailty models, cure rate models, model selection and comparison, joint models for longitudinal and survival data, models with time varying covariates, missing covariate data, design and monitoring of clinical trials, accelerated failure time models, models for mulitivariate survival data, and special types of hierarchial survival models. Also various censoring schemes are examined including right and interval censored data. Several additional topics are discussed, including noninformative and informative prior specificiations, computing posterior qualities of interest, Bayesian hypothesis testing, variable selection, model selection with nonnested models, model checking techniques using Bayesian diagnostic methods, and Markov chain Monte Carlo (MCMC) algorithms for sampling from the posteiror and predictive distributions.The book presents a balance between theory and applications, and for each class of models discussed, detailed examples and analyses from case studies are presented whenever possible. The applications are all essentially from the health sciences, including cancer, AIDS, and the environment. The book is intended as a graduate textbook or a reference book for a one semester course at the advanced masters or Ph.D. level. This book would be most suitable for second or third year graduate students in statistics or biostatistics. It would also serve as a useful reference book for applied or theoretical researchers as well as practitioners.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Sinha, Debajyoti},
	year = {2001},
	doi = {10.1007/978-1-4757-3447-8},
}

@book{aalen_survival_2008,
	address = {New York},
	series = {Statistics for {Biology} and {Health}},
	title = {Survival and {Event} {History} {Analysis}: {A} {Process} {Point} of {View}},
	isbn = {978-0-387-20287-7},
	shorttitle = {Survival and {Event} {History} {Analysis}},
	url = {https://www.springer.com/gp/book/9780387202877},
	abstract = {Time-to-event data are ubiquitous in fields such as medicine, biology, demography, sociology, economics and reliability theory. Recently, a need to analyze more complex event histories has emerged. Examples are individuals that move among several states, frailty that makes some units fail before others, internal time-dependent covariates, and the estimation of causal effects from observational data. The aim of this book is to bridge the gap between standard textbook models and a range of models where the dynamic structure of the data manifests itself fully. The common denominator of such models is stochastic processes. The authors show how counting processes, martingales, and stochastic integrals fit very nicely with censored data. Beginning with standard analyses such as Kaplan-Meier plots and Cox regression, the presentation progresses to the additive hazard model and recurrent event data. Stochastic processes are also used as natural models for individual frailty; they allow sensible interpretations of a number of surprising artifacts seen in population data. The stochastic process framework is naturally connected to causality. The authors show how dynamic path analyses can incorporate many modern causality ideas in a framework that takes the time aspect seriously. To make the material accessible to the reader, a large number of practical examples, mainly from medicine, are developed in detail. Stochastic processes are introduced in an intuitive and non-technical manner. The book is aimed at investigators who use event history methods and want a better understanding of the statistical concepts. It is suitable as a textbook for graduate courses in statistics and biostatistics. Odd O. Aalen is professor of medical statistics at the University of Oslo, Norway. His Ph.D. from the University of California, Berkeley in 1975 introduced counting processes and martingales in event history analysis. He has also contributed to numerous other areas of event history analysis, such as additive hazards regression, frailty, and causality through dynamic modeling. Ørnulf Borgan is professor of statistics at the University of Oslo, Norway. Since his Ph.D. in 1984 he has contributed extensively to event history analysis. He is co-author of the monograph Statistical Models Based on Counting Processes, and is editor of Scandinavian Journal of Statistics. Håkon K. Gjessing is professor of medical statistics at the Norwegian Institute of Public Health and the University of Bergen, Norway. Since his Ph.D. in probability in 1995, he has worked on a broad range of theoretical and applied problems in biostatistics.},
	language = {en},
	urldate = {2021-03-26},
	publisher = {Springer-Verlag},
	author = {Aalen, Odd and Borgan, Ornulf and Gjessing, Hakon},
	year = {2008},
	doi = {10.1007/978-0-387-68560-1},
}

@article{crowley_note_1974,
	title = {A {Note} on {Some} {Recent} {Likelihoods} {Leading} to the {Log} {Rank} {Test}},
	volume = {61},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334736},
	doi = {10.2307/2334736},
	abstract = {A discussion is given of the derivation by Peto (1972) of the log rank test as the locally most powerful rank invariant test against certain Lehmann-type alternatives in the two-sample problem with censored data. The random censorship model is used to show that the test with this local optimality property may depend on the `censoring mechanism, even if censoring is applied equally to both groups. An explanation is offered of Peto's likelihood and of a special case of the conditional likelihood of Cox (1972). Reference is made to asymptotic relative efficiency results which demonstrate that the log rank test may not be fully efficient when censoring is not equal.},
	number = {3},
	urldate = {2021-03-26},
	journal = {Biometrika},
	author = {Crowley, John},
	year = {1974},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {533--538},
}

@article{breslow_covariance_1974,
	title = {Covariance analysis of censored survival data},
	volume = {30},
	issn = {0006-341X},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Breslow, N.},
	month = mar,
	year = {1974},
	pmid = {4813387},
	keywords = {Humans, Models, Biological, Prognosis, Age Factors, Child, Preschool, Dactinomycin, Leukemia, Lymphoid, Leukocyte Count, Mercaptopurine, Methotrexate, Nitrogen Mustard Compounds, Regression Analysis, Remission, Spontaneous, Statistics as Topic, Time Factors},
	pages = {89--99},
}

@article{kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2281868},
	doi = {10.2307/2281868},
	abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 ≤ t$_{\textrm{1}}$' ≤ t$_{\textrm{2}}$' ≤ ⋯ ≤ t$_{\textrm{N}}$'. Then {\textless}tex-math{\textgreater}\${\textbackslash}hat\{P\}(t) = {\textbackslash}prod\_r {\textbackslash}lbrack(N - r)/(N - r + 1){\textbackslash}rbrack\${\textless}/tex-math{\textgreater}, where r assumes those values for which t$_{\textrm{r}}$' ≤ t and for which t$_{\textrm{r}}$' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
	number = {282},
	urldate = {2021-03-26},
	journal = {Journal of the American Statistical Association},
	author = {Kaplan, E. L. and Meier, Paul},
	year = {1958},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {457--481},
}

@article{kalbfleisch_non-parametric_1978,
	title = {Non-{Parametric} {Bayesian} {Analysis} of {Survival} {Time} {Data}},
	volume = {40},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984758},
	abstract = {A Bayesian analysis of the semi-parametric regression and life model of Cox (1972) is given. The cumulative hazard function is modelled as a gamma process. Both estimation of the regression parameters and of the underlying survival distribution are considered. The results are compared to the results obtained by other approaches.},
	number = {2},
	urldate = {2021-03-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Kalbfleisch, John D.},
	year = {1978},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {214--221},
}

@article{arjas_filtering_1992,
	title = {Filtering the histories of a partially observed marked point process},
	volume = {40},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/030441499290013G},
	doi = {10.1016/0304-4149(92)90013-G},
	abstract = {We consider a situation in which the evolution of an ‘underlying’ marked point process is of interest, but where this process is not directly observable. Instead, we assume that another marked point process, which is fully determined by the underlying process, can be observed. The problem is then the estimation, at any given time t, of the underlying development so far, given the corresponding observations. The solution, in the sense of a conditional distribution of the underlying pre-t history, is shown to satisfy a recursive filter formula. Sufficient conditions for the uniqueness of the solution are given. Two non-trivial examples are considered in detail.},
	language = {en},
	number = {2},
	urldate = {2021-04-04},
	journal = {Stochastic Processes and their Applications},
	author = {Arjas, Elja and Haara, Pentti and Norros, Ikka},
	month = mar,
	year = {1992},
	keywords = {alternating renewal process, compensator, disruption problem, filtering, history set, marked point process},
	pages = {225--250},
}

@article{gelman_bayesian_2020,
	title = {Bayesian workflow},
	journal = {arXiv preprint arXiv:2011.01808},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	year = {2020},
}

@article{muhammad_epic-survival_2021,
	title = {{EPIC}-{Survival}: {End}-to-end {Part} {Inferred} {Clustering} for {Survival} {Analysis}, {Featuring} {Prognostic} {Stratification} {Boosting}},
	shorttitle = {{EPIC}-{Survival}},
	url = {http://arxiv.org/abs/2101.11085},
	abstract = {Histopathology-based survival modelling has two major hurdles. Firstly, a well-performing survival model has minimal clinical application if it does not contribute to the stratiﬁcation of a cancer patient cohort into different risk groups, preferably driven by histologic morphologies. In the clinical setting, individuals are not given speciﬁc prognostic predictions, but are rather predicted to lie within a risk group which has a general survival trend. Thus, It is imperative that a survival model produces well-stratiﬁed risk groups. Secondly, until now, survival modelling was done in a two-stage approach (encoding and aggregation). The massive amount of pixels in digitized whole slide images were never utilized to their fullest extent due to technological constraints on data processing, forcing decoupled learning. EPIC-Survival bridges encoding and aggregation into an end-to-end survival modelling approach, while introducing stratiﬁcation boosting to encourage the model to not only optimize ranking, but also to discriminate between risk groups. In this study we show that EPIC-Survival performs better than other approaches in modelling intrahepatic cholangiocarcinoma, a historically difﬁcult cancer to model. Further, we show that stratiﬁcation boosting improves further improves model performance, resulting in a concordance-index of 0.880 on a held-out test set. Finally, we were able to identify speciﬁc histologic differences, not commonly sought out in ICC, between low and high risk groups.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2101.11085 [cs]},
	author = {Muhammad, Hassan and Xie, Chensu and Sigel, Carlie S. and Doukas, Michael and Alpert, Lindsay and Fuchs, Thomas J.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.11085},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{allen_nonalcoholic_2018,
	title = {Nonalcoholic fatty liver disease incidence and impact on metabolic burden and death: a 20 year-community study},
	volume = {67},
	number = {5},
	journal = {Hepatology},
	author = {Allen, Alina M and Therneau, Terry M and Larson, Joseph J and Coward, Alexandra and Somers, Virend K and Kamath, Patrick S},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {1726--1736},
}

@article{lewandowski2009generating,
  title={Generating random correlation matrices based on vines and extended onion method},
  author={Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  journal={Journal of multivariate analysis},
  volume={100},
  number={9},
  pages={1989--2001},
  year={2009},
  publisher={Elsevier}
}

@article{mohamed_monte_2019,
  title={Monte Carlo Gradient Estimation in Machine Learning.},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={132},
  pages={1--62},
  year={2020}
}

@inproceedings{ranganath_black_2014,
	title = {Black box variational inference},
	booktitle = {Artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
	year = {2014},
	pages = {814--822},
}

@inproceedings{ranganath_deep_2016,
	title = {Deep survival analysis},
	booktitle = {Machine {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Perotte, Adler and Elhadad, Noémie and Blei, David},
	year = {2016},
	pages = {101--114},
}

@article{brilleman_bayesian_2020,
	title = {Bayesian {Survival} {Analysis} {Using} the rstanarm {R} {Package}},
	journal = {arXiv preprint arXiv:2002.09633},
	author = {Brilleman, Samuel L and Elci, Eren M and Novik, Jacqueline Buros and Wolfe, Rory},
	year = {2020},
}

@misc{sas_sas_nodate,
	title = {{SAS} {Help} {Center}: {Specifics} for {Bayesian} {Analysis}},
	url = {https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_phreg_details82.htm#statug_phreg020296},
	urldate = {2021-05-14},
	author = {SAS},
}

@misc{tay_regularized_2021,
	title = {Regularized {Cox} {Regression}},
	url = {https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf},
	language = {en},
	urldate = {2021-05-01},
	author = {Tay, Kenneth and Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob and Narasimhan, Balasubramanian},
	year = {2021},
}

@incollection{therneau_cox_2000,
	title = {The cox model},
	booktitle = {Modeling survival data: extending the {Cox} model},
	publisher = {Springer},
	author = {Therneau, Terry M and Grambsch, Patricia M},
	year = {2000},
	pages = {39--77},
}

@article{shin_scalable_2018,
	title = {Scalable {Bayesian} variable selection using nonlocal prior densities in ultrahigh-dimensional settings},
	volume = {28},
	number = {2},
	journal = {Statistica Sinica},
	author = {Shin, Minsuk and Bhattacharya, Anirban and Johnson, Valen E},
	year = {2018},
	note = {Publisher: NIH Public Access},
	pages = {1053},
}

@article{schmidt_danish_2015,
	title = {The {Danish} {National} {Patient} {Registry}: a review of content, data quality, and research potential},
	volume = {7},
	journal = {Clinical epidemiology},
	author = {Schmidt, Morten and Schmidt, Sigrun Alba Johannesdottir and Sandegaard, Jakob Lynge and Ehrenstein, Vera and Pedersen, Lars and Sørensen, Henrik Toft},
	year = {2015},
	note = {Publisher: Dove Press},
	pages = {449},
}

@article{royston_use_1999,
	title = {The use of fractional polynomials to model continuous risk variables in epidemiology.},
	volume = {28},
	number = {5},
	journal = {International journal of epidemiology},
	author = {Royston, Patrick and Ambler, Gareth and Sauerbrei, Willi},
	year = {1999},
	pages = {964--974},
}

@article{sinha_bayesian_2003,
	title = {A {Bayesian} {Justification} of {Cox}'s {Partial} {Likelihood}},
	volume = {90},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/30042071},
	abstract = {In this paper, we establish both naive and formal Bayesian justifications of Cox's (1975) partial likelihood and its various modifications. We extend the original work of Kalbfieisch (1978), who showed that the partial likelihood is a limiting marginal posterior under noninformative priors for baseline hazards. We extend the result to scenarios with time-dependent covariates and time-varying regression parameters. We establish results for continuous time as well as grouped survival data. In addition, we present a Bayesian justification of a modified partial likelihood for handling ties. We also present tools for simplification of the Gibbs sampling algorithm for implementing partial likelihood based Bayesian inference in various practical applications.},
	number = {3},
	urldate = {2021-10-25},
	journal = {Biometrika},
	author = {Sinha, Debajyoti and Ibrahim, Joseph G. and Chen, Ming-Hui},
	year = {2003},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {629--641},
}

@article{aalen_nonparametric_1978,
	title = {Nonparametric {Inference} for a {Family} of {Counting} {Processes}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-4/Nonparametric-Inference-for-a-Family-of-Counting-Processes/10.1214/aos/1176344247.full},
	doi = {10.1214/aos/1176344247},
	abstract = {Let \${\textbackslash}mathbf\{B\} = (N\_1, {\textbackslash}cdots, N\_k)\$ be a multivariate counting process and let \${\textbackslash}mathscr\{F\}\_t\$ be the collection of all events observed on the time interval \${\textbackslash}lbrack 0, t{\textbackslash}rbrack.\$ The intensity process is given by \${\textbackslash}Lambda\_i(t) = {\textbackslash}lim\_\{h {\textbackslash}downarrow 0\} {\textbackslash}frac\{1\}\{h\}E(N\_i(t + h) - N\_i(t) {\textbackslash}mid {\textbackslash}mathscr\{F\}\_t){\textbackslash}quad i = 1, {\textbackslash}cdots, k.\$ We give an application of the recently developed martingale-based approach to the study of \${\textbackslash}mathbf\{N\}\$ via \${\textbackslash}mathbf\{{\textbackslash}Lambda\}.\$ A statistical model is defined by letting \${\textbackslash}Lambda\_i(t) = {\textbackslash}alpha\_i(t)Y\_i(t), i = 1, {\textbackslash}cdots, k,\$ where \${\textbackslash}mathbf\{{\textbackslash}alpha\} = ({\textbackslash}alpha\_1, {\textbackslash}cdots, {\textbackslash}alpha\_k)\$ is an unknown nonnegative function while \${\textbackslash}mathbf\{Y\} = (Y\_1, {\textbackslash}cdots, Y\_k),\$ together with \${\textbackslash}mathbf\{N\},\$ is a process observable over a certain time interval. Special cases are time-continuous Markov chains on finite state spaces, birth and death processes and models for survival analysis with censored data. The model is termed nonparametric when \${\textbackslash}mathbf\{{\textbackslash}alpha\}\$ is allowed to vary arbitrarily except for regularity conditions. The existence of complete and sufficient statistics for this model is studied. An empirical process estimating \${\textbackslash}beta\_i(t) = {\textbackslash}int{\textasciicircum}t\_0 {\textbackslash}alpha\_i(s) ds\$ is given and studied by means of the theory of stochastic integrals. This empirical process is intended for plotting purposes and it generalizes the empirical cumulative hazard rate from survival analysis and is related to the product limit estimator. Consistency and weak convergence results are given. Tests for comparison of two counting processes, generalizing the two sample rank tests, are defined and studied. Finally, an application to a set of biological data is given.},
	number = {4},
	urldate = {2022-01-11},
	journal = {The Annals of Statistics},
	author = {Aalen, Odd},
	month = jul,
	year = {1978},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60G45, 60H05, 62G05, 62G10, 62M05, 62M99, 62N05, counting process, empirical process, Inference for stochastic processes, intensity process, Martingales, nonparametric theory, point process, stochastic integrals, Survival analysis},
	pages = {701--726},
}

@article{alvares_bayesian_2021,
	title = {Bayesian survival analysis with {BUGS}},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8933},
	doi = {10.1002/sim.8933},
	abstract = {Survival analysis is one of the most important fields of statistics in medicine and biological sciences. In addition, the computational advances in the last decades have favored the use of Bayesian methods in this context, providing a flexible and powerful alternative to the traditional frequentist approach. The objective of this article is to summarize some of the most popular Bayesian survival models, such as accelerated failure time, proportional hazards, mixture cure, competing risks, multi-state, frailty, and joint models of longitudinal and survival data. Moreover, an implementation of each presented model is provided using a BUGS syntax that can be run with JAGS from the R programming language. Reference to other Bayesian R-packages is also discussed.},
	language = {en},
	number = {12},
	urldate = {2022-01-11},
	journal = {Statistics in Medicine},
	author = {Alvares, Danilo and Lázaro, Elena and Gómez-Rubio, Virgilio and Armero, Carmen},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8933},
	keywords = {Bayesian inference, JAGS, R-packages, time-to-event analysis},
	pages = {2975--3020},
}

@article{arjas_marked_1984,
	title = {A {Marked} {Point} {Process} {Approach} to {Censored} {Failure} {Data} with {Complicated} {Covariates}},
	volume = {11},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/4615959},
	abstract = {Complicated failure time data which can involve, e.g., random covariates, censored observations and multiple failures, is here considered as a sample path of a marked point process (MPP). Our main task is to derive likelihood expressions for parametric statistical models under such general circumstances. To do this, and motivated by concrete examples, we split each marked point into two characteristic parts, called innovation and non-innovation, and then characterize this representation in terms of the statistical model. Technically the paper is based on the martingale approach to point processes.},
	number = {4},
	urldate = {2022-01-11},
	journal = {Scandinavian Journal of Statistics},
	author = {Arjas, Elja and Haara, Pentti},
	year = {1984},
	note = {Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	pages = {193--209},
}

@article{austin_review_2020,
	title = {A review of the use of time-varying covariates in the {Fine}-{Gray} subdistribution hazard competing risk regression model},
	volume = {39},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8399},
	doi = {10.1002/sim.8399},
	abstract = {In survival analysis, time-varying covariates are covariates whose value can change during follow-up. Outcomes in medical research are frequently subject to competing risks (events precluding the occurrence of the primary outcome). We review the types of time-varying covariates and highlight the effect of their inclusion in the subdistribution hazard model. External time-dependent covariates are external to the subject, can effect the failure process, but are not otherwise involved in the failure mechanism. Internal time-varying covariates are measured on the subject, can effect the failure process directly, and may also be impacted by the failure mechanism. In the absence of competing risks, a consequence of including internal time-dependent covariates in the Cox model is that one cannot estimate the survival function or the effect of covariates on the survival function. In the presence of competing risks, the inclusion of internal time-varying covariates in a subdistribution hazard model results in the loss of the ability to estimate the cumulative incidence function (CIF) or the effect of covariates on the CIF. Furthermore, the definition of the risk set for the subdistribution hazard function can make defining internal time-varying covariates difficult or impossible. We conducted a review of the use of time-varying covariates in subdistribution hazard models in articles published in the medical literature in 2015 and in the first 5 months of 2019. Seven percent of articles published included a time-varying covariate. Several inappropriately described a time-varying covariate as having an association with the risk of the outcome.},
	language = {en},
	number = {2},
	urldate = {2022-01-11},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Latouche, Aurélien and Fine, Jason P.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8399},
	keywords = {competing risks, subdistribution hazard model, survival analysis, time-varying covariate},
	pages = {103--113},
}

@inproceedings{bardenet_towards_2014,
	address = {Bejing, China},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Towards scaling up {Markov} chain {Monte} {Carlo}: an adaptive subsampling approach},
	volume = {32},
	url = {https://proceedings.mlr.press/v32/bardenet14.html},
	abstract = {Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICML’14), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bardenet, Rémi and Doucet, Arnaud and Holmes, Chris},
	editor = {Xing, Eric P. and Jebara, Tony},
	month = jun,
	year = {2014},
	note = {Issue: 1},
	pages = {405--413},
}

@inproceedings{betancourt_fundamental_2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Fundamental} {Incompatibility} of {Scalable} {Hamiltonian} {Monte} {Carlo} and {Naive} {Data} {Subsampling}},
	volume = {37},
	url = {https://proceedings.mlr.press/v37/betancourt15.html},
	abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Betancourt, Michael},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	pages = {533--540},
}

@book{who_international_2015,
	edition = {10th revision, Fifth edition, 2016},
	title = {International statistical classification of diseases and related health problems},
	publisher = {World Health Organization},
	author = {WHO, World Health Organization},
	year = {2015},
	note = {Type: Publications},
}

@article{chen_china_2011,
	title = {China {Kadoorie} {Biobank} of 0.5 million people: survey methods, baseline characteristics and long-term follow-up},
	volume = {40},
	issn = {1464-3685},
	shorttitle = {China {Kadoorie} {Biobank} of 0.5 million people},
	doi = {10.1093/ije/dyr120},
	abstract = {BACKGROUND: Large blood-based prospective studies can provide reliable assessment of the complex interplay of lifestyle, environmental and genetic factors as determinants of chronic disease.
METHODS: The baseline survey of the China Kadoorie Biobank took place during 2004-08 in 10 geographically defined regions, with collection of questionnaire data, physical measurements and blood samples. Subsequently, a re-survey of 25,000 randomly selected participants was done (80\% responded) using the same methods as in the baseline. All participants are being followed for cause-specific mortality and morbidity, and for any hospital admission through linkages with registries and health insurance (HI) databases.
RESULTS: Overall, 512,891 adults aged 30-79 years were recruited, including 41\% men, 56\% from rural areas and mean age was 52 years. The prevalence of ever-regular smoking was 74\% in men and 3\% in women. The mean blood pressure was 132/79 mmHg in men and 130/77 mmHg in women. The mean body mass index (BMI) was 23.4 kg/m(2) in men and 23.8 kg/m(2) in women, with only 4\% being obese ({\textgreater}30 kg/m(2)), and 3.2\% being diabetic. Blood collection was successful in 99.98\% and the mean delay from sample collection to processing was 10.6 h. For each of the main baseline variables, there is good reproducibility but large heterogeneity by age, sex and study area. By 1 January 2011, over 10,000 deaths had been recorded, with 91\% of surviving participants already linked to HI databases.
CONCLUSION: This established large biobank will be a rich and powerful resource for investigating genetic and non-genetic causes of many common chronic diseases in the Chinese population.},
	language = {eng},
	number = {6},
	journal = {International Journal of Epidemiology},
	author = {Chen, Zhengming and Chen, Junshi and Collins, Rory and Guo, Yu and Peto, Richard and Wu, Fan and Li, Liming and {China Kadoorie Biobank (CKB) collaborative group}},
	month = dec,
	year = {2011},
	pmid = {22158673},
	pmcid = {PMC3235021},
	keywords = {Adult, Age Factors, Aged, Blood Pressure, Body Mass Index, Body Weights and Measures, China, Chronic Disease, Data Collection, Databases, Factual, Databases, Genetic, Environment, Female, Follow-Up Studies, Genetic Predisposition to Disease, Genetic Testing, Health Behavior, Humans, Life Style, Male, Middle Aged, Prospective Studies, Residence Characteristics, Rural Population, Sex Factors, Smoking, Socioeconomic Factors, Surveys and Questionnaires},
	pages = {1652--1666},
}

@article{carey_prevention_2018,
	title = {Prevention, {Detection}, {Evaluation}, and {Management} of {High} {Blood} {Pressure} in {Adults}: {Synopsis} of the 2017 {American} {College} of {Cardiology}/{American} {Heart} {Association} {Hypertension} {Guideline}},
	volume = {168},
	issn = {1539-3704},
	shorttitle = {Prevention, {Detection}, {Evaluation}, and {Management} of {High} {Blood} {Pressure} in {Adults}},
	doi = {10.7326/M17-3203},
	abstract = {Description: In November 2017, the American College of Cardiology (ACC) and the American Heart Association (AHA) released a clinical practice guideline for the prevention, detection, evaluation, and treatment of high blood pressure (BP) in adults. This article summarizes the major recommendations.
Methods: In 2014, the ACC and the AHA appointed a multidisciplinary committee to update previous reports of the Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure. The committee reviewed literature and commissioned systematic reviews and meta-analyses on out-of-office BP monitoring, the optimal target for BP lowering, the comparative benefits and harms of different classes of antihypertensive agents, and the comparative benefits and harms of initiating therapy with a single antihypertensive agent or a combination of 2 agents.
Recommendations: This article summarizes key recommendations in the following areas: BP classification, BP measurement, screening for secondary hypertension, nonpharmacologic therapy, BP thresholds and cardiac risk estimation to guide drug treatment, treatment goals (general and for patients with diabetes mellitus, chronic kidney disease, and advanced age), choice of initial drug therapy, resistant hypertension, and strategies to improve hypertension control.},
	language = {eng},
	number = {5},
	journal = {Annals of Internal Medicine},
	author = {Carey, Robert M. and Whelton, Paul K. and {2017 ACC/AHA Hypertension Guideline Writing Committee}},
	month = mar,
	year = {2018},
	pmid = {29357392},
	keywords = {Adult, Antihypertensive Agents, Blood Pressure Determination, Comorbidity, Humans, Hypertension, Mass Screening, Secondary Prevention},
	pages = {351--358},
}

@article{clift_living_2020,
	title = {Living risk prediction algorithm ({QCOVID}) for risk of hospital admission and mortality from coronavirus 19 in adults: national derivation and validation cohort study},
	volume = {371},
	issn = {1756-1833},
	shorttitle = {Living risk prediction algorithm ({QCOVID}) for risk of hospital admission and mortality from coronavirus 19 in adults},
	doi = {10.1136/bmj.m3731},
	abstract = {OBJECTIVE: To derive and validate a risk prediction algorithm to estimate hospital admission and mortality outcomes from coronavirus disease 2019 (covid-19) in adults.
DESIGN: Population based cohort study.
SETTING AND PARTICIPANTS: QResearch database, comprising 1205 general practices in England with linkage to covid-19 test results, Hospital Episode Statistics, and death registry data. 6.08 million adults aged 19-100 years were included in the derivation dataset and 2.17 million in the validation dataset. The derivation and first validation cohort period was 24 January 2020 to 30 April 2020. The second temporal validation cohort covered the period 1 May 2020 to 30 June 2020.
MAIN OUTCOME MEASURES: The primary outcome was time to death from covid-19, defined as death due to confirmed or suspected covid-19 as per the death certification or death occurring in a person with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in the period 24 January to 30 April 2020. The secondary outcome was time to hospital admission with confirmed SARS-CoV-2 infection. Models were fitted in the derivation cohort to derive risk equations using a range of predictor variables. Performance, including measures of discrimination and calibration, was evaluated in each validation time period.
RESULTS: 4384 deaths from covid-19 occurred in the derivation cohort during follow-up and 1722 in the first validation cohort period and 621 in the second validation cohort period. The final risk algorithms included age, ethnicity, deprivation, body mass index, and a range of comorbidities. The algorithm had good calibration in the first validation cohort. For deaths from covid-19 in men, it explained 73.1\% (95\% confidence interval 71.9\% to 74.3\%) of the variation in time to death (R2); the D statistic was 3.37 (95\% confidence interval 3.27 to 3.47), and Harrell's C was 0.928 (0.919 to 0.938). Similar results were obtained for women, for both outcomes, and in both time periods. In the top 5\% of patients with the highest predicted risks of death, the sensitivity for identifying deaths within 97 days was 75.7\%. People in the top 20\% of predicted risk of death accounted for 94\% of all deaths from covid-19.
CONCLUSION: The QCOVID population based risk algorithm performed well, showing very high levels of discrimination for deaths and hospital admissions due to covid-19. The absolute risks presented, however, will change over time in line with the prevailing SARS-C0V-2 infection rate and the extent of social distancing measures in place, so they should be interpreted with caution. The model can be recalibrated for different time periods, however, and has the potential to be dynamically updated as the pandemic evolves.},
	language = {eng},
	journal = {BMJ (Clinical research ed.)},
	author = {Clift, Ash K. and Coupland, Carol A. C. and Keogh, Ruth H. and Diaz-Ordaz, Karla and Williamson, Elizabeth and Harrison, Ewen M. and Hayward, Andrew and Hemingway, Harry and Horby, Peter and Mehta, Nisha and Benger, Jonathan and Khunti, Kamlesh and Spiegelhalter, David and Sheikh, Aziz and Valabhji, Jonathan and Lyons, Ronan A. and Robson, John and Semple, Malcolm G. and Kee, Frank and Johnson, Peter and Jebb, Susan and Williams, Tony and Hippisley-Cox, Julia},
	month = oct,
	year = {2020},
	pmid = {33082154},
	pmcid = {PMC7574532},
	keywords = {Adult, Aged, 80 and over, Algorithms, Betacoronavirus, Clinical Decision Rules, Cohort Studies, Coronavirus Infections, COVID-19, Databases, Factual, England, Female, Hospitalization, Humans, Male, Mortality, Pandemics, Pneumonia, Viral, Prognosis, Reproducibility of Results, Risk Assessment, SARS-CoV-2},
	pages = {m3731},
}

@article{cox_general_1968,
	title = {A {General} {Definition} of {Residuals}},
	volume = {30},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984505},
	abstract = {Residuals are usually defined in connection with linear models. Here a more general definition is given and some asymptotic properties found. Some illustrative examples are discussed, including a regression problem involving exponentially distributed errors and some problems concerning Poisson and binomially distributed observations.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R. and Snell, E. J.},
	year = {1968},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {248--275},
}

@article{crowley_covariance_1977,
	title = {Covariance {Analysis} of {Heart} {Transplant} {Survival} {Data}},
	volume = {72},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286902},
	doi = {10.2307/2286902},
	abstract = {This paper presents a number of analyses to assess the effects of various covariates on the survival of patients in the Stanford Heart Transplantation Program. The data have been updated from previously published versions and include some additional covariates, such as measures of tissue typing. The methods used allow for simultaneous investigation of several covariates and provide estimates of the relative risk of transplantation as well as significance tests.},
	number = {357},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Crowley, John and Hu, Marie},
	year = {1977},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {27--36},
}

@article{davison_deviance_1989,
	title = {Deviance residuals and normal scores plots},
	volume = {76},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/76.2.211},
	doi = {10.1093/biomet/76.2.211},
	abstract = {We discuss the use of normal order statistics plots, based on deviance residuals, to check distributional assumptions in regression models. Continuous and discrete error distributions are considered, as are censored data. Misspecified error distributions and discrimination between competing models are discussed, with an example.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Davison, A. C. and Gigli, A.},
	month = jun,
	year = {1989},
	pages = {211--221},
}

@article{dawber_epidemiological_1951,
	title = {Epidemiological approaches to heart disease: the {Framingham} {Study}},
	volume = {41},
	issn = {0002-9572},
	shorttitle = {Epidemiological approaches to heart disease},
	doi = {10.2105/ajph.41.3.279},
	language = {eng},
	number = {3},
	journal = {American Journal of Public Health and the Nation's Health},
	author = {Dawber, T. R. and Meadors, G. F. and Moore, F. E.},
	month = mar,
	year = {1951},
	pmid = {14819398},
	pmcid = {PMC1525365},
	keywords = {HEART DISEASE, Heart Diseases, Humans},
	pages = {279--281},
}

@article{egeberg_assessment_2016,
	title = {Assessment of the risk of cardiovascular disease in patients with rosacea},
	volume = {75},
	issn = {1097-6787},
	doi = {10.1016/j.jaad.2016.02.1158},
	abstract = {BACKGROUND: Recent studies have shown a higher prevalence of cardiovascular (CV) risk factors in patients with rosacea. However, it remains unknown whether rosacea represents an independent CV risk factor.
OBJECTIVE: We evaluated the risk of myocardial infarction, stroke, CV death, major adverse CV events, and all-cause mortality, respectively.
METHODS: Between January 1, 1997, and December 31, 2012, a total of 4948 patients with rosacea were identified and matched with 23,823 control subjects. We used Poisson regression to calculate incidence rate ratios.
RESULTS: Adjusted incidence rate ratios were 0.75 (95\% confidence intervals [CI] 0.57-1.00) for myocardial infarction, 1.08 (95\% CI 0.86-1.35) for ischemic stroke, 1.01 (95\% CI 0.61-1.67) for hemorrhagic stroke, 0.99 (95\% CI 0.80-1.24) for CV death, 0.99 (95\% CI 0.86-1.15) for major adverse CV events, and 0.95 (95\% CI 0.85-1.06) for all-cause mortality.
LIMITATIONS: We were unable to distinguish between the different subtypes and severities of rosacea.
CONCLUSIONS: In this population-based study, rosacea was not associated with increased risk of adverse CV outcomes or death.},
	language = {eng},
	number = {2},
	journal = {Journal of the American Academy of Dermatology},
	author = {Egeberg, Alexander and Hansen, Peter R. and Gislason, Gunnar H. and Thyssen, Jacob P.},
	month = aug,
	year = {2016},
	pmid = {27444070},
	keywords = {cardiovascular disease, Cardiovascular Diseases, Case-Control Studies, Cause of Death, Comorbidity, epidemiology, Female, Humans, Incidence, Male, Middle Aged, Poisson Distribution, risk factors, Risk Factors, rosacea, Rosacea},
	pages = {336--339},
}

@article{fine_proportional_1999,
	title = {A {Proportional} {Hazards} {Model} for the {Subdistribution} of a {Competing} {Risk}},
	volume = {94},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10474144},
	doi = {10.1080/01621459.1999.10474144},
	abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.},
	number = {446},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Fine, Jason P. and Gray, Robert J.},
	month = jun,
	year = {1999},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10474144},
	keywords = {Hazard of subdistribution, Martingale, Partial likelihood, Transformation model},
	pages = {496--509},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	language = {eng},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {1467-985X},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Place: United Kingdom
Publisher: Wiley-Blackwell Publishing Ltd.},
	keywords = {Bayesian Analysis, Statistical Data, Statistical Norms},
	pages = {389--402},
}

@article{halabi_score_2020,
	title = {Score and deviance residuals based on the full likelihood approach in survival analysis},
	volume = {19},
	issn = {1539-1612},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2047},
	doi = {10.1002/pst.2047},
	abstract = {Assuming the proportional hazards model and non-informative censoring, the full likelihood approach is used to obtain two new residuals. The first residual is based on the ideas used in obtaining score-type residuals similar to the partial likelihood approach. The second type of residual is based on the concept of deviance residuals. Extensive simulations are conducted to compare the performance of the residuals from the full likelihood-based approach with those of the partial likelihood method. We demonstrate through simulation studies that the full likelihood-based residuals are more efficient than their partial likelihood counterpart in identifying potential outliers when the censoring proportion is high. The graphical techniques are used to illustrate the applications of these residuals using some examples.},
	language = {en},
	number = {6},
	urldate = {2022-01-11},
	journal = {Pharmaceutical Statistics},
	author = {Halabi, Susan and Dutta, Sandipan and Wu, Yuan and Liu, Aiyi},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.2047},
	keywords = {deviance residuals, full likelihood, non-informative censoring, partial likelihood, proportional hazards, score-type residuals},
	pages = {940--954},
}

@article{hippisley-cox_predicting_2021,
	title = {Predicting the risk of prostate cancer in asymptomatic men: a cohort study to develop and validate a novel algorithm},
	volume = {71},
	issn = {1478-5242},
	shorttitle = {Predicting the risk of prostate cancer in asymptomatic men},
	doi = {10.3399/bjgp20X714137},
	abstract = {BACKGROUND: Diagnosis of prostate cancer at an early stage can potentially identify tumours when intervention may improve treatment options and survival.
AIM: To develop and validate an equation to predict absolute risk of prostate cancer in asymptomatic men with prostate specific antigen (PSA) tests in primary care.
DESIGN AND SETTING: Cohort study using data from English general practices, held in the QResearch database.
METHOD: Routine data were collected from 1098 QResearch English general practices linked to mortality, hospital, and cancer records for model development. Two separate sets of practices were used for validation. In total, there were 844 455 men aged 25-84 years with PSA tests recorded who were free of prostate cancer at baseline in the derivation cohort; the validation cohorts comprised 292 084 and 316 583 men. The primary outcome was incident prostate cancer. Cox proportional hazards models were used to derive 10-year risk equations. Measures of performance were determined in both validation cohorts.
RESULTS: There were 40 821 incident cases of prostate cancer in the derivation cohort. The risk equation included PSA level, age, deprivation, ethnicity, smoking status, serious mental illness, diabetes, BMI, and family history of prostate cancer. The risk equation explained 70.4\% (95\% CI = 69.2 to 71.6) of the variation in time to diagnosis of prostate cancer (R 2) (D statistic 3.15, 95\% CI = 3.06 to 3.25; Harrell's C-index 0.917, 95\% CI = 0.915 to 0.919). Two-step approach had higher sensitivity than a fixed PSA threshold at identifying prostate cancer cases (identifying 68.2\% versus 43.9\% of cases), high-grade cancers (49.2\% versus 40.3\%), and deaths (67.0\% versus 31.5\%).
CONCLUSION: The risk equation provided valid measures of absolute risk and had higher sensitivity for incident prostate cancer, high-grade cancers, and prostate cancer mortality than a simple approach based on age and PSA threshold.},
	language = {eng},
	number = {706},
	journal = {The British Journal of General Practice: The Journal of the Royal College of General Practitioners},
	author = {Hippisley-Cox, Julia and Coupland, Carol},
	month = may,
	year = {2021},
	pmid = {33875417},
	pmcid = {PMC8087311},
	keywords = {Algorithms, cohort studies, Cohort Studies, Humans, Male, primary health care, Prospective Studies, prostate cancer, prostate-specific antigen, Prostate-Specific Antigen, Prostatic Neoplasms, Risk Assessment, Risk Factors, risk prediction},
	pages = {e364--e371},
}

@article{hippisley-cox_development_2017,
	title = {Development and validation of {QRISK3} risk prediction algorithms to estimate future risk of cardiovascular disease: prospective cohort study},
	volume = {357},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions 				.  					This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {1756-1833},
	shorttitle = {Development and validation of {QRISK3} risk prediction algorithms to estimate future risk of cardiovascular disease},
	url = {https://www.bmj.com/content/357/bmj.j2099},
	doi = {10.1136/bmj.j2099},
	abstract = {Objectives To develop and validate updated QRISK3 prediction algorithms to estimate the 10 year risk of cardiovascular disease in women and men accounting for potential new risk factors.
Design Prospective open cohort study.
Setting General practices in England providing data for the QResearch database.
Participants 1309 QResearch general practices in England: 981 practices were used to develop the scores and a separate set of 328 practices were used to validate the scores. 7.89 million patients aged 25-84 years were in the derivation cohort and 2.67 million patients in the validation cohort. Patients were free of cardiovascular disease and not prescribed statins at baseline.
Methods Cox proportional hazards models in the derivation cohort to derive separate risk equations in men and women for evaluation at 10 years. Risk factors considered included those already in QRISK2 (age, ethnicity, deprivation, systolic blood pressure, body mass index, total cholesterol: high density lipoprotein cholesterol ratio, smoking, family history of coronary heart disease in a first degree relative aged less than 60 years, type 1 diabetes, type 2 diabetes, treated hypertension, rheumatoid arthritis, atrial fibrillation, chronic kidney disease (stage 4 or 5)) and new risk factors (chronic kidney disease (stage 3, 4, or 5), a measure of systolic blood pressure variability (standard deviation of repeated measures), migraine, corticosteroids, systemic lupus erythematosus (SLE), atypical antipsychotics, severe mental illness, and HIV/AIDs). We also considered erectile dysfunction diagnosis or treatment in men. Measures of calibration and discrimination were determined in the validation cohort for men and women separately and for individual subgroups by age group, ethnicity, and baseline disease status.
Main outcome measures Incident cardiovascular disease recorded on any of the following three linked data sources: general practice, mortality, or hospital admission records.
Results 363 565 incident cases of cardiovascular disease were identified in the derivation cohort during follow-up arising from 50.8 million person years of observation. All new risk factors considered met the model inclusion criteria except for HIV/AIDS, which was not statistically significant. The models had good calibration and high levels of explained variation and discrimination. In women, the algorithm explained 59.6\% of the variation in time to diagnosis of cardiovascular disease (R2, with higher values indicating more variation), and the D statistic was 2.48 and Harrell’s C statistic was 0.88 (both measures of discrimination, with higher values indicating better discrimination). The corresponding values for men were 54.8\%, 2.26, and 0.86. Overall performance of the updated QRISK3 algorithms was similar to the QRISK2 algorithms.
Conclusion Updated QRISK3 risk prediction models were developed and validated. The inclusion of additional clinical variables in QRISK3 (chronic kidney disease, a measure of systolic blood pressure variability (standard deviation of repeated measures), migraine, corticosteroids, SLE, atypical antipsychotics, severe mental illness, and erectile dysfunction) can help enable doctors to identify those at most risk of heart disease and stroke.},
	language = {en},
	urldate = {2022-01-11},
	journal = {BMJ},
	author = {Hippisley-Cox, Julia and Coupland, Carol and Brindle, Peter},
	month = may,
	year = {2017},
	pmid = {28536104},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	pages = {j2099},
}

@article{ibrahim_bayesian_1999,
	title = {Bayesian {Variable} {Selection} for {Proportional} {Hazards} {Models}},
	volume = {27},
	issn = {0319-5724},
	url = {https://www.jstor.org/stable/3316126},
	doi = {10.2307/3316126},
	abstract = {The authors consider the problem of Bayesian variable selection for proportional hazards regression models with right censored data. They propose a semi-parametric approach in which a nonparametric prior is specified for the baseline hazard rate and a fully parametric prior is specified for the regression coefficients. For the baseline hazard, they use a discrete gamma process prior, and for the regression coefficients and the model space, they propose a semi-automatic parametric informative prior specification that focuses on the observables rather than the parameters. To implement the methodology, they propose a Markov chain Monte Carlo method to compute the posterior model probabilities. Examples using simulated and real data are given to demonstrate the methodology. /// Les auteurs abordent d'un point de vue bayésien le problème de la sélection de variables dans les modèles de régression des risques proportionnels en présence de censure à droite. Ils proposent une approche semi-paramétrique dans laquelle la loi a priori du taux de base est non paramétrique, mais celle des coefficients de régression est complètement paramétrique. L'information concernant le taux de base est représentée par la loi a priori issue d'un processus gamma discret; quant à la loi a priori des paramètres du modèle de régression, elle est choisie dans une classe de lois paramétriques au moyen d'une procédure semi-automatique centrée sur les données plutôt que sur les paramètres. La mise à jour de l'information se fait au moyen d'un algorithme de Monte-Carlo à chaîne de Markov. Des données réelles et simulées permettent d'illustrer la méthode.},
	number = {4},
	urldate = {2022-01-11},
	journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and MacEachern, Steven N.},
	year = {1999},
	note = {Publisher: [Statistical Society of Canada, Wiley]},
	pages = {701--717},
}

@article{kannel_lessons_1976,
	title = {Some lessons in cardiovascular epidemiology from {Framingham}},
	volume = {37},
	issn = {0002-9149},
	doi = {10.1016/0002-9149(76)90323-4},
	abstract = {Epidemiologic investigations have provided a portrait of the potential candidate for coronary heart disease. This is important because studies of the evolution of coronary disease in the general population reveal that it is a common disease that frequently attacks without warning, can be silent in its most dangerous form and can present with sudden death as the first symptom. Progress in identifyin- persons in jeopardy and the factors needing correction makes it theoretically possible to interrupt the chain of factors that eventuate in this disease. Coronary disease does not really begin with crushing chest pain, pulmonary edema, shock, angina or ventricular fibrillation, but rather with more subtle signs like a poor coronary risk profile. The risk factors can be treated quantitatively as ingredients of a cardiovascular risk profile and their joint effect estimated. An efficient practicable set of variables for this purpose is a casual blood test for cholesterol and sugar, a blood pressure determination, an electrocardiogram and a cigarette smoking history. With this set of variables the risk of coronary heart diseases can be estimated over a 30-fold range and 10 percent of the asymptomatic population identified in whom 25 percent of the coronary disease, 40 percent of the occlusive peripheral arterial disease and 50 percent of the strokes and congestive heart failure will evolve. The periodic use of the electrocardiogram at rest and after exercise in persons with a poor risk profile can demonstrate persons with asymptomatic ischemic cardiomyopathy due to advanced coronary artery disease. Most cases of angina pectoris or myocardial infarction represent medical failures; the conditions should have been detected years earlier for preventive management. About 30 percent of patients with infraction will shortly experience new angina, have an annual death rate of 4 percent and a fourfold increased risk of sudden death. Reinfarction will occur at an annual rate of 6 percent, and half the recurrences will be fatal. Congestive heart failure must be expected at 10 times and strokes at 5 times the rate found in the general population. Although no major innovations are required to identify candidates for coronary disease and to estimate their risk, we have much to learn about motivating changes in behavior to control risk factors. Approaches to prevention of coronary heart disease include public health measures to alter the ecology in favor of cardiovascular health, preventive medicine directed at highly vulnerable candidates and hygienic measures initiated by an informed public in its own behalf.},
	language = {eng},
	number = {2},
	journal = {The American Journal of Cardiology},
	author = {Kannel, W. B.},
	month = feb,
	year = {1976},
	pmid = {1246956},
	keywords = {Adult, Age Factors, Aged, Cardiovascular Diseases, Coronary Disease, Electrocardiography, Environment, Female, Humans, Hypertension, Lipids, Male, Massachusetts, Methods, Middle Aged, Population Surveillance, Smoking},
	pages = {269--282},
}

@article{laurie_surgical_1989,
	title = {Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. {The} {North} {Central} {Cancer} {Treatment} {Group} and the {Mayo} {Clinic}},
	volume = {7},
	issn = {0732-183X},
	shorttitle = {Surgical adjuvant therapy of large-bowel carcinoma},
	doi = {10.1200/JCO.1989.7.10.1447},
	abstract = {A total of 401 eligible patients with resected stages B and C colorectal carcinoma were randomly assigned to no-further therapy or to adjuvant treatment with either levamisole alone, 150 mg/d for 3 days every 2 weeks for 1 year, or levamisole plus fluorouracil (5-FU), 450 mg/m2/d intravenously (IV) for 5 days and beginning at 28 days, 450 mg/m2 weekly for 1 year. Levamisole plus 5-FU, and to a lesser extent levamisole alone, reduced cancer recurrence in comparison with no adjuvant therapy. These differences, after correction for imbalances in prognostic variables, were only suggestive for levamisole alone (P = .05) but quite significant for levamisole plus 5-FU (P = .003). Whereas both treatment regimens were associated with overall improvements in survival, these improvements reached borderline significance only for stage C patients treated with levamisole plus 5-FU (P = .03). Therapy was clinically tolerable with either regimen and severe toxicity was uncommon. These promising results have led to a large national intergroup confirmatory trial currently in progress.},
	language = {eng},
	number = {10},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Laurie, J. A. and Moertel, C. G. and Fleming, T. R. and Wieand, H. S. and Leigh, J. E. and Rubin, J. and McCormack, G. W. and Gerstner, J. B. and Krook, J. E. and Malliard, J.},
	month = oct,
	year = {1989},
	pmid = {2778478},
	keywords = {Adult, Aged, Aged, 80 and over, Colorectal Neoplasms, Female, Fluorouracil, Humans, Levamisole, Lymphatic Metastasis, Male, Middle Aged, Neoplasm Recurrence, Local, Neoplasm Staging, Neoplasms, Multiple Primary, Patient Compliance, Random Allocation},
	pages = {1447--1456},
}

@article{loprinzi_prospective_1994,
	title = {Prospective evaluation of prognostic variables from patient-completed questionnaires. {North} {Central} {Cancer} {Treatment} {Group}},
	volume = {12},
	issn = {0732-183X},
	doi = {10.1200/JCO.1994.12.3.601},
	abstract = {PURPOSE: This study was developed to determine whether descriptive information from a patient-completed questionnaire could provide prognostic information that was independent from that already obtained by the patient's physician.
PATIENTS AND METHODS: An initial detailed questionnaire was administered to approximately 150 patients with advanced cancer. This questionnaire was subsequently revised and given to a total of 1,115 patients with advanced colorectal or lung cancer. Univariate and multivariate analyses were performed to evaluate the data from these questionnaires.
RESULTS: A total of 36 variables showed statistically significant prognostic information for survival in univariate analyses, even though many of these variables were associated with only a minimal increase in risk. A multivariate analysis demonstrated that there was a high correlation between many variables. Three major groups of variables became apparent as providing strong prognostic information. These included the following: (1) a physician's assessment of performance status (PS); (2) a patient's assessment of their own PS; and (3) a nutritional factor such as appetite, caloric intake, or overall food intake.
CONCLUSION: Data generated by a patient-completed questionnaire can provide important prognostic information independent from that obtained by other physician-determined prognostic factors.},
	language = {eng},
	number = {3},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Loprinzi, C. L. and Laurie, J. A. and Wieand, H. S. and Krook, J. E. and Novotny, P. J. and Kugler, J. W. and Bartel, J. and Law, M. and Bateman, M. and Klatt, N. E.},
	month = mar,
	year = {1994},
	pmid = {8120560},
	keywords = {Analysis of Variance, Colorectal Neoplasms, Humans, Karnofsky Performance Status, Lung Neoplasms, Neoplasms, Prognosis, Proportional Hazards Models, Prospective Studies, Severity of Illness Index, Surveys and Questionnaires},
	pages = {601--607},
}

@article{mahmood_framingham_2014,
	title = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease: a historical perspective},
	volume = {383},
	issn = {1474-547X},
	shorttitle = {The {Framingham} {Heart} {Study} and the epidemiology of cardiovascular disease},
	doi = {10.1016/S0140-6736(13)61752-3},
	abstract = {On Sept 29, 2013, the Framingham Heart Study will celebrate 65 years since the examination of the first volunteer in 1948. During this period, the study has provided substantial insight into the epidemiology and risk factors of cardiovascular disease. The origins of the study are closely linked to the cardiovascular health of President Franklin D Roosevelt and his premature death from hypertensive heart disease and stroke in 1945. In this Review we describe the events leading to the foundation of the Framingham Heart Study, and provide a brief historical overview of selected contributions from the study.},
	language = {eng},
	number = {9921},
	journal = {Lancet (London, England)},
	author = {Mahmood, Syed S. and Levy, Daniel and Vasan, Ramachandran S. and Wang, Thomas J.},
	month = mar,
	year = {2014},
	pmid = {24084292},
	pmcid = {PMC4159698},
	keywords = {Cardiovascular Diseases, Epidemiology, History, 20th Century, History, 21st Century, Humans, Longitudinal Studies, Massachusetts, Research Support as Topic, Risk Factors},
	pages = {999--1008},
}

@article{nelson_theory_1972,
	title = {Theory and {Applications} of {Hazard} {Plotting} for {Censored} {Failure} {Data}},
	volume = {14},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1267144},
	doi = {10.2307/1267144},
	abstract = {This paper presents theory and applications of a simple graphical method, called hazard plotting, for the analysis of multiply censored life data consisting of failure times of failed units intermixed with running times on unfailed units. Applications of the method are given for multiply censored data on service life of equipment, for strength data on an item with different failure modes, and for biological data multiply censored on both sides from paired comparisons. Theory for the hazard plotting method, which is based on the hazard function of a distribution, is developed from the properties of order statistics from Type II multiply censored samples.},
	number = {4},
	urldate = {2022-01-11},
	journal = {Technometrics},
	author = {Nelson, Wayne},
	year = {1972},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {945--966},
}

@article{nikooienejad_bayesian_2020,
	title = {Bayesian variable selection for survival data using inverse moment priors},
	volume = {14},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-14/issue-2/Bayesian-variable-selection-for-survival-data-using-inverse-moment-priors/10.1214/20-AOAS1325.full},
	doi = {10.1214/20-AOAS1325},
	abstract = {Efficient variable selection in high-dimensional cancer genomic studies is critical for discovering genes associated with specific cancer types and for predicting response to treatment. Censored survival data is prevalent in such studies. In this article we introduce a Bayesian variable selection procedure that uses a mixture prior composed of a point mass at zero and an inverse moment prior in conjunction with the partial likelihood defined by the Cox proportional hazard model. The procedure is implemented in the R package BVSNLP, which supports parallel computing and uses a stochastic search method to explore the model space. Bayesian model averaging is used for prediction. The proposed algorithm provides better performance than other variable selection procedures in simulation studies and appears to provide more consistent variable selection when applied to actual genomic datasets.},
	number = {2},
	urldate = {2022-01-11},
	journal = {The Annals of Applied Statistics},
	author = {Nikooienejad, Amir and Wang, Wenyi and Johnson, Valen E.},
	month = jun,
	year = {2020},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayesian variable selection, Cancer genomics, Cox proportional hazard model, High-dimensional data, nonlocal prior, survival data analysis},
	pages = {809--828},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full},
	doi = {10.1214/17-EJS1337SI},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	month = jan,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62F15, Bayesian inference, horseshoe prior, shrinkage priors, Sparse estimation},
	pages = {5018--5051},
}

@article{qian_fast_2020,
	title = {A fast and scalable framework for large-scale and ultrahigh-dimensional sparse regression with application to the {UK} {Biobank}},
	volume = {16},
	issn = {1553-7404},
	url = {https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1009141},
	doi = {10.1371/journal.pgen.1009141},
	abstract = {The UK Biobank is a very large, prospective population-based cohort study across the United Kingdom. It provides unprecedented opportunities for researchers to investigate the relationship between genotypic information and phenotypes of interest. Multiple regression methods, compared with genome-wide association studies (GWAS), have already been showed to greatly improve the prediction performance for a variety of phenotypes. In the high-dimensional settings, the lasso, since its first proposal in statistics, has been proved to be an effective method for simultaneous variable selection and estimation. However, the large-scale and ultrahigh dimension seen in the UK Biobank pose new challenges for applying the lasso method, as many existing algorithms and their implementations are not scalable to large applications. In this paper, we propose a computational framework called batch screening iterative lasso (BASIL) that can take advantage of any existing lasso solver and easily build a scalable solution for very large data, including those that are larger than the memory size. We introduce snpnet, an R package that implements the proposed algorithm on top of glmnet and optimizes for single nucleotide polymorphism (SNP) datasets. It currently supports ℓ1-penalized linear model, logistic regression, Cox model, and also extends to the elastic net with ℓ1/ℓ2 penalty. We demonstrate results on the UK Biobank dataset, where we achieve competitive predictive performance for all four phenotypes considered (height, body mass index, asthma, high cholesterol) using only a small fraction of the variants compared with other established polygenic risk score methods.},
	language = {en},
	number = {10},
	urldate = {2022-01-11},
	journal = {PLOS Genetics},
	author = {Qian, Junyang and Tanigawa, Yosuke and Du, Wenfei and Aguirre, Matthew and Chang, Chris and Tibshirani, Robert and Rivas, Manuel A. and Hastie, Trevor},
	month = oct,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Asthma, Body mass index, Genetics, Genome-wide association studies, Heredity, Hypercholesterolemia, Single nucleotide polymorphisms},
	pages = {e1009141},
}

@article{schoenfeld_partial_1982,
	title = {Partial {Residuals} for {The} {Proportional} {Hazards} {Regression} {Model}},
	volume = {69},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335876},
	doi = {10.2307/2335876},
	abstract = {Residuals are defined for the proportional hazards regression model introduced by Cox (1972). These residuals can be plotted against time to test the proportional hazards assumption. Histograms of these residuals can be used to examine fit and detect outlying covariate values.},
	number = {1},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Schoenfeld, David},
	year = {1982},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {239--241},
}

@article{segall_heart_2014,
	title = {Heart failure in patients with chronic kidney disease: a systematic integrative review},
	volume = {2014},
	issn = {2314-6141},
	shorttitle = {Heart failure in patients with chronic kidney disease},
	doi = {10.1155/2014/937398},
	abstract = {INTRODUCTION: Heart failure (HF) is highly prevalent in patients with chronic kidney disease (CKD) and end-stage renal disease (ESRD) and is strongly associated with mortality in these patients. However, the treatment of HF in this population is largely unclear.
STUDY DESIGN: We conducted a systematic integrative review of the literature to assess the current evidence of HF treatment in CKD patients, searching electronic databases in April 2014. Synthesis used narrative methods.
SETTING AND POPULATION: We focused on adults with a primary diagnosis of CKD and HF.
SELECTION CRITERIA FOR STUDIES: We included studies of any design, quantitative or qualitative.
INTERVENTIONS: HF treatment was defined as any formal means taken to improve the symptoms of HF and/or the heart structure and function abnormalities.
OUTCOMES: Measures of all kinds were considered of interest.
RESULTS: Of 1,439 results returned by database searches, 79 articles met inclusion criteria. A further 23 relevant articles were identified by hand searching.
CONCLUSIONS: Control of fluid overload, the use of beta-blockers and angiotensin-converting enzyme inhibitors or angiotensin receptor blockers, and optimization of dialysis appear to be the most important methods to treat HF in CKD and ESRD patients. Aldosterone antagonists and digitalis glycosides may additionally be considered; however, their use is associated with significant risks. The role of anemia correction, control of CKD-mineral and bone disorder, and cardiac resynchronization therapy are also discussed.},
	language = {eng},
	journal = {BioMed Research International},
	author = {Segall, Liviu and Nistor, Ionut and Covic, Adrian},
	year = {2014},
	pmid = {24959595},
	pmcid = {PMC4052068},
	keywords = {Angiotensin Receptor Antagonists, Angiotensin-Converting Enzyme Inhibitors, Glomerular Filtration Rate, Heart Failure, Humans, Kidney Failure, Chronic, Renal Insufficiency, Chronic, Risk Factors},
	pages = {937398},
}

@article{shin_scalable_2018-1,
	title = {{SCALABLE} {BAYESIAN} {VARIABLE} {SELECTION} {USING} {NONLOCAL} {PRIOR} {DENSITIES} {IN} {ULTRAHIGH}-{DIMENSIONAL} {SETTINGS}},
	volume = {28},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/44841937},
	abstract = {Bayesian model selection procedures based on nonlocal alternative prior densities are extended to ultrahigh dimensional settings and compared to other variable selection procedures using precision-recall curves. Variable selection procedures included in these comparisons include methods based on g-priors, reciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria. The use of precision-recall curves eliminates the sensitivity of our conclusions to the choice of tuning parameters. We find that Bayesian variable selection procedures based on nonlocal priors are competitive to all other procedures in a range of simulation scenarios, and we subsequently explain this favorable performance through a theoretical examination of their consistency properties. When certain regularity conditions apply, we demonstrate that the nonlocal procedures are consistent for linear models even when the number of covariates increases sub-exponentially with the sample size n. A model selection procedure based on Zellner's g-prior is also found to be competitive with penalized likelihood methods in identifying the true model, but the posterior distribution on the model space induced by this method is much more dispersed than the posterior distribution induced on the model space by the nonlocal prior methods. We investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and show that it attains a unique term that cannot be derived from the other Bayesian model selection procedures. We also propose a scalable and efficient algorithm called Simplified Shotgun Stochastic Search with Screening (S5) to explore the enormous model space, and we show that S5 dramatically reduces the computing time without losing the capacity to search the interesting region in the model space, at least in the simulation settings considered. The S5 algorithm is available in an Rpackage BayesS5 on CRAN.},
	number = {2},
	urldate = {2022-01-11},
	journal = {Statistica Sinica},
	author = {Shin, Minsuk and Bhattacharya, Anirban and Johnson, Valen E.},
	year = {2018},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {1053--1078},
}

@article{sleeper_regression_1990,
	title = {Regression {Splines} in the {Cox} {Model} with {Application} to {Covariate} {Effects} in {Liver} {Disease}},
	volume = {85},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474965},
	doi = {10.1080/01621459.1990.10474965},
	abstract = {The Cox proportional hazards model restricts the log hazard ratio to be linear in the covariates. A smooth nonlinear covariate effect may go undetected in this model but can be well approximated by a spline function. A survival model based on data from a clinical trial of primary biliary cirrhosis is developed using regression splines, and the resulting log hazard ratio estimates are compared with those from nonparametric methods. We remove the linear restriction on the log hazard ratio by transforming a continuous covariate into a vector of fixed knot basis splines (B-splines). B-splines are known to produce better-conditioned systems of equations than the truncated power basis when used as interpolants, and show similar behavior when fitting proportional hazards models. We describe the procedures for, and the issues arising in, the estimation and the testing of the B-spline coefficients. Although inference is not well developed for some nonparametric methods that estimate covariate effects, the asymptotic theory for Cox model B-spline estimates is relatively straightforward. An S function for fitting B-splines in Cox regression models is available.},
	number = {412},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Sleeper, Lynn A. and Harrington, David P.},
	month = dec,
	year = {1990},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1990.10474965},
	keywords = {B-spline, Proportional hazards, Relative risk, Spline approximation, Survival analysis},
	pages = {941--949},
}

@article{sudlow_uk_2015,
	title = {{UK} biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age},
	volume = {12},
	issn = {1549-1676},
	shorttitle = {{UK} biobank},
	doi = {10.1371/journal.pmed.1001779},
	abstract = {Cathie Sudlow and colleagues describe the UK Biobank, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
	language = {eng},
	number = {3},
	journal = {PLoS medicine},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins, Rory},
	month = mar,
	year = {2015},
	pmid = {25826379},
	pmcid = {PMC4380465},
	keywords = {Access to Information, Adult, Aged, Aging, Biological Specimen Banks, Databases, Factual, Genotype, Humans, Middle Aged, Neoplasms, Phenotype, Prospective Studies, Research, United Kingdom},
	pages = {e1001779},
}

@article{therneau_martingale-based_1990,
	title = {Martingale-{Based} {Residuals} for {Survival} {Models}},
	volume = {77},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2336057},
	doi = {10.2307/2336057},
	abstract = {Graphical methods based on the analysis of residuals are considered for the setting of the highly-used Cox (1972) regression model and for the Andersen-Gill (1982) generalization of that model. We start with a class of martingale-based residuals as proposed by Barlow \& Prentice (1988). These residuals and/or their transforms are useful for investigating the functional form of a covariate, the proportional hazards assumption, the leverage of each subject upon the estimates of β, and the lack of model fit to a given subject.},
	number = {1},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Therneau, Terry M. and Grambsch, Patricia M. and Fleming, Thomas R.},
	year = {1990},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {147--160},
}

@book{therneau_package_2021,
	title = {A {Package} for {Survival} {Analysis} in {R}},
	url = {https://CRAN.R-project.org/package=survival},
	author = {Therneau, Terry M.},
	year = {2021},
}

@article{virani_heart_2021,
	title = {Heart {Disease} and {Stroke} {Statistics}-2021 {Update}: {A} {Report} {From} the {American} {Heart} {Association}},
	volume = {143},
	issn = {1524-4539},
	shorttitle = {Heart {Disease} and {Stroke} {Statistics}-2021 {Update}},
	doi = {10.1161/CIR.0000000000000950},
	abstract = {BACKGROUND: The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs).
METHODS: The American Heart Association, through its Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update. The 2021 Statistical Update is the product of a full year's worth of effort by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. This year's edition includes data on the monitoring and benefits of cardiovascular health in the population, an enhanced focus on social determinants of health, adverse pregnancy outcomes, vascular contributions to brain health, the global burden of cardiovascular disease, and further evidence-based approaches to changing behaviors related to cardiovascular disease.
RESULTS: Each of the 27 chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics.
CONCLUSIONS: The Statistical Update represents a critical resource for the lay public, policy makers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.},
	language = {eng},
	number = {8},
	journal = {Circulation},
	author = {Virani, Salim S. and Alonso, Alvaro and Aparicio, Hugo J. and Benjamin, Emelia J. and Bittencourt, Marcio S. and Callaway, Clifton W. and Carson, April P. and Chamberlain, Alanna M. and Cheng, Susan and Delling, Francesca N. and Elkind, Mitchell S. V. and Evenson, Kelly R. and Ferguson, Jane F. and Gupta, Deepak K. and Khan, Sadiya S. and Kissela, Brett M. and Knutson, Kristen L. and Lee, Chong D. and Lewis, Tené T. and Liu, Junxiu and Loop, Matthew Shane and Lutsey, Pamela L. and Ma, Jun and Mackey, Jason and Martin, Seth S. and Matchar, David B. and Mussolino, Michael E. and Navaneethan, Sankar D. and Perak, Amanda Marma and Roth, Gregory A. and Samad, Zainab and Satou, Gary M. and Schroeder, Emily B. and Shah, Svati H. and Shay, Christina M. and Stokes, Andrew and VanWagner, Lisa B. and Wang, Nae-Yuh and Tsao, Connie W. and {American Heart Association Council on Epidemiology and Prevention Statistics Committee and Stroke Statistics Subcommittee}},
	month = feb,
	year = {2021},
	pmid = {33501848},
	keywords = {AHA Scientific Statements, American Heart Association, Blood Pressure, cardiovascular diseases, Cholesterol, Diabetes Mellitus, Diet, Healthy, epidemiology, Exercise, Global Burden of Disease, Health Behavior, Heart Diseases, Hospitalization, Humans, Obesity, Prevalence, risk factors, Risk Factors, Smoking, statistics, stroke, Stroke, United States},
	pages = {e254--e743},
}

@inproceedings{yao_yes_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Yes, but {Did} {It} {Work}?: {Evaluating} {Variational} {Inference}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/yao18a.html},
	abstract = {While it’s always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation. We propose two diagnostic algorithms to alleviate this problem. The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational simulation-based calibration (VSBC) assesses the average performance of point estimates.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {5581--5590},
}

@article{breheny_coordinate_2011,
	title = {Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection},
	volume = {5},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-1/Coordinate-descent-algorithms-for-nonconvex-penalized-regression-with-applications-to/10.1214/10-AOAS388.full},
	doi = {10.1214/10-AOAS388},
	abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
	number = {1},
	urldate = {2022-01-11},
	journal = {The Annals of Applied Statistics},
	author = {Breheny, Patrick and Huang, Jian},
	month = mar,
	year = {2011},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Coordinate descent, Lasso, MCP, optimization, penalized regression, SCAD},
	pages = {232--253},
}

@article{zou_adaptive_2006,
	title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000735},
	doi = {10.1198/016214506000000735},
	abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ℓ1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
	number = {476},
	urldate = {2022-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Zou, Hui},
	month = dec,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000000735},
	keywords = {Asymptotic normality, Lasso, Minimax, Oracle inequality, Oracle procedure, Variable selection},
	pages = {1418--1429},
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s proportional hazards model},
	volume = {94},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asm037},
	doi = {10.1093/biomet/asm037},
	abstract = {We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L1 penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively.},
	number = {3},
	urldate = {2022-01-11},
	journal = {Biometrika},
	author = {Zhang, Hao Helen and Lu, Wenbin},
	month = aug,
	year = {2007},
	pages = {691--703},
}

@article{yang_cocktail_2013,
  title={A cocktail algorithm for solving the elastic net penalized Cox’s regression in high dimensions},
  author={Yang, Yi and Zou, Hui},
  journal={Statistics and its Interface},
  volume={6},
  number={2},
  pages={167--173},
  year={2013},
  publisher={International Press of Boston}
}

@article{park2008bayesian,
  title={The bayesian lasso},
  author={Park, Trevor and Casella, George},
  journal={Journal of the American Statistical Association},
  volume={103},
  number={482},
  pages={681--686},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{hans2009bayesian,
  title={Bayesian lasso regression},
  author={Hans, Chris},
  journal={Biometrika},
  volume={96},
  number={4},
  pages={835--845},
  year={2009},
  publisher={Oxford University Press}
}

@article{wang2021fast,
  title={A fast divide-and-conquer sparse Cox regression},
  author={Wang, Yan and Hong, Chuan and Palmer, Nathan and Di, Qian and Schwartz, Joel and Kohane, Isaac and Cai, Tianxi},
  journal={Biostatistics},
  volume={22},
  number={2},
  pages={381--401},
  year={2021},
  publisher={Oxford University Press}
}
